{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alim98/MPI/blob/main/SynapseSegmentationDataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daa58lj6b7qc"
      },
      "source": [
        "# Essential downloads"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T7SY--kjukgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lxdYaS94LBSI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47df249a-f4c4-40d8-bb8c-8b6702b8cde3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJzj48_y4iya",
        "outputId": "f0d97e07-2109-47f7-e4f5-38d87c020d04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping ploty as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting plotly==5.3.1\n",
            "  Downloading plotly-5.3.1-py2.py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly==5.3.1) (9.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from plotly==5.3.1) (1.17.0)\n",
            "Downloading plotly-5.3.1-py2.py3-none-any.whl (23.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.9/23.9 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: plotly\n",
            "  Attempting uninstall: plotly\n",
            "    Found existing installation: plotly 5.24.1\n",
            "    Uninstalling plotly-5.24.1:\n",
            "      Successfully uninstalled plotly-5.24.1\n",
            "Successfully installed plotly-5.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall ploty\n",
        "!pip install plotly==5.3.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIIIxarwTVT0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2baec991-dd9f-430b-adea-170a60fac019"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-18 10:18:18--  https://drive.usercontent.google.com/download?id=1iHPBdBOPEagvPTHZmrN__LD49emXwReY&export=download&authuser=0&confirm=t&uuid=631d60dd-569c-4bb1-a9e8-d681f0ed3d43&at=APvzH3r4me8x_LwP3n8O7lgPo8oK%3A1733988188000\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.130.132, 2404:6800:4003:c01::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.130.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1264688649 (1.2G) [application/octet-stream]\n",
            "Saving to: ‘downloaded_file.zip’\n",
            "\n",
            "downloaded_file.zip 100%[===================>]   1.18G  60.1MB/s    in 19s     \n",
            "\n",
            "2025-02-18 10:18:40 (64.8 MB/s) - ‘downloaded_file.zip’ saved [1264688649/1264688649]\n",
            "\n",
            "--2025-02-18 10:18:40--  https://drive.usercontent.google.com/download?id=1qRibZL3kr7MQJQRgDFRquHMQlIGCN4XP&export=download&authuser=0&confirm=t&uuid=631d60dd-569c-4bb1-a9e8-d681f0ed3d43&at=APvzH3r4me8x_LwP3n8O7lgPo8oK%3A1733988188000\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.130.132, 2404:6800:4003:c01::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.130.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14246564 (14M) [application/octet-stream]\n",
            "Saving to: ‘vesicle_cloud__syn_interface__mitochondria_annotation.zip’\n",
            "\n",
            "vesicle_cloud__syn_ 100%[===================>]  13.59M  34.3MB/s    in 0.4s    \n",
            "\n",
            "2025-02-18 10:19:12 (34.3 MB/s) - ‘vesicle_cloud__syn_interface__mitochondria_annotation.zip’ saved [14246564/14246564]\n",
            "\n",
            "Collecting git+https://github.com/funkelab/funlib.learn.torch.git\n",
            "  Cloning https://github.com/funkelab/funlib.learn.torch.git to /tmp/pip-req-build-ppw8cyc7\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/funkelab/funlib.learn.torch.git /tmp/pip-req-build-ppw8cyc7\n",
            "  Resolved https://github.com/funkelab/funlib.learn.torch.git to commit 049729151c7a2c0320a446dc9d3244ac830f7ea8\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Collecting umap-learn\n",
            "  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.61.0)\n",
            "Collecting pynndescent>=0.5 (from umap-learn)\n",
            "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn) (0.44.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m116.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: funlib.learn.torch\n",
            "  Building wheel for funlib.learn.torch (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for funlib.learn.torch: filename=funlib.learn.torch-0.1.0-py3-none-any.whl size=13995 sha256=5ace576f697beeb5704caee2b47612ab0a4601577be84f308796cec8329c5e2f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-51voe1cd/wheels/ae/7f/7b/ecbd355ccdfbd2bb0ab4f76ea45f60a02a572c88c1f4761e8d\n",
            "Successfully built funlib.learn.torch\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, pynndescent, nvidia-cusolver-cu12, umap-learn, funlib.learn.torch\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed funlib.learn.torch-0.1.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pynndescent-0.5.13 umap-learn-0.5.7\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n"
          ]
        }
      ],
      "source": [
        "!wget -O downloaded_file.zip \"https://drive.usercontent.google.com/download?id=1iHPBdBOPEagvPTHZmrN__LD49emXwReY&export=download&authuser=0&confirm=t&uuid=631d60dd-569c-4bb1-a9e8-d681f0ed3d43&at=APvzH3r4me8x_LwP3n8O7lgPo8oK%3A1733988188000\"\n",
        "!wget -O vesicle_cloud__syn_interface__mitochondria_annotation.zip \"https://drive.usercontent.google.com/download?id=1qRibZL3kr7MQJQRgDFRquHMQlIGCN4XP&export=download&authuser=0&confirm=t&uuid=631d60dd-569c-4bb1-a9e8-d681f0ed3d43&at=APvzH3r4me8x_LwP3n8O7lgPo8oK%3A1733988188000\"\n",
        "\n",
        "!unzip -q downloaded_file.zip\n",
        "!unzip -q vesicle_cloud__syn_interface__mitochondria_annotation.zip\n",
        "\n",
        "!pip install transformers scikit-learn matplotlib seaborn torch torchvision umap-learn git+https://github.com/funkelab/funlib.learn.torch.git\n",
        "!pip install openpyxl\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-iwYc6iryNA"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "# Synapse Dataset Processing\n",
        "\n",
        "This repository provides a tool for processing 3D volume data of synapse structures for segmentation, visualization, and analysis. The tool allows users to load, segment, and process raw and segmented image data and generate 3D visualizations of synapse structures.\n",
        "\n",
        "The code processes multiple bounding boxes, each containing raw, segmentation, and additional mask data. Users can customize the segmentation overlay and generate visualizations in the form of segmented 3D cubes.\n",
        "\n",
        "## Features\n",
        "\n",
        "- Load raw, segmentation, and additional mask data from directories.\n",
        "- Customize segmentation overlays for different synapse structures (e.g., vesicles, clefts, mitochondria).\n",
        "- Process multiple bounding boxes in parallel.\n",
        "- Generate 3D cubes with customizable segmentation.\n",
        "- Alpha blending for better visualization of the data.\n",
        "- Ability to save generated GIFs for visual inspection.\n",
        "- Efficient processing pipelines for handling large synapse datasets.\n",
        "\n",
        "## Requirements\n",
        "\n",
        "Before using the tool, make sure you have the following Python packages installed:\n",
        "\n",
        "```bash\n",
        "pip install numpy pandas imageio tqdm torch torchvision scipy\n",
        "```\n",
        "\n",
        "## Arguments Overview\n",
        "\n",
        "To run the script, use the following arguments to configure the dataset processing:\n",
        "\n",
        "### `--raw_base_dir` (Required)\n",
        "- **Description**: Directory containing the raw image data files (e.g., `.tif` slices).\n",
        "- **Type**: `str`\n",
        "- **Default**: `'raw'`\n",
        "- **Example**:\n",
        "    ```bash\n",
        "    --raw_base_dir /path/to/raw/data\n",
        "    ```\n",
        "\n",
        "### `--seg_base_dir` (Required)\n",
        "- **Description**: Directory containing segmentation data files for pre and post-synaptic structures (e.g., `.tif` slices).\n",
        "- **Type**: `str`\n",
        "- **Default**: `'seg'`\n",
        "- **Example**:\n",
        "    ```bash\n",
        "    --seg_base_dir /path/to/segmentation/data\n",
        "    ```\n",
        "\n",
        "### `--add_mask_base_dir` (Optional)\n",
        "- **Description**: Directory containing additional mask files for vesicles, clefts, and mitochondria (e.g., `.tif` slices).\n",
        "- **Type**: `str`\n",
        "- **Default**: `''` (empty string, optional)\n",
        "- **Example**:\n",
        "    ```bash\n",
        "    --add_mask_base_dir /path/to/additional/masks\n",
        "    ```\n",
        "\n",
        "### `--bbox_name` (Required)\n",
        "- **Description**: List of bounding box names to process. Each bounding box corresponds to a set of data files (raw, segmentation, and masks).\n",
        "- **Type**: `list[str]`\n",
        "- **Default**: `['bbox1']`\n",
        "- **Example**:\n",
        "    ```bash\n",
        "    --bbox_name bbox1 bbox2 bbox3\n",
        "    ```\n",
        "\n",
        "### `--excel_file` (Required)\n",
        "- **Description**: Path to the directory containing Excel files with synapse information. The data from these Excel files will be used for synapse annotations.\n",
        "- **Type**: `str`\n",
        "- **Default**: `''` (required path to a directory)\n",
        "- **Example**:\n",
        "    ```bash\n",
        "    --excel_file /path/to/excel/files\n",
        "    ```\n",
        "\n",
        "### `--csv_output_dir` (Optional)\n",
        "- **Description**: Directory to save CSV outputs, such as processed data summaries.\n",
        "- **Type**: `str`\n",
        "- **Default**: `'csv_outputs'`\n",
        "- **Example**:\n",
        "    ```bash\n",
        "    --csv_output_dir /path/to/csv/outputs\n",
        "    ```\n",
        "\n",
        "### `--size` (Optional)\n",
        "- **Description**: Target size for the frames. This will resize the frames to this size before processing.\n",
        "- **Type**: `tuple[int, int]`\n",
        "- **Default**: `(80, 80)`\n",
        "- **Example**:\n",
        "    ```bash\n",
        "    --size 128 128\n",
        "    ```\n",
        "\n",
        "### `--subvol_size` (Optional)\n",
        "- **Description**: Subvolume size for extracting regions from the full volume. This size determines the 3D crop of the data.\n",
        "- **Type**: `int`\n",
        "- **Default**: `80`\n",
        "- **Example**:\n",
        "    ```bash\n",
        "    --subvol_size 128\n",
        "    ```\n",
        "\n",
        "### `--num_frames` (Optional)\n",
        "- **Description**: Number of frames to extract from the data.\n",
        "- **Type**: `int`\n",
        "- **Default**: `80`\n",
        "- **Example**:\n",
        "    ```bash\n",
        "    --num_frames 16\n",
        "    ```\n",
        "\n",
        "### `--save_gifs_dir` (Optional)\n",
        "- **Description**: Directory to save generated GIFs for each segmentation type.\n",
        "- **Type**: `str`\n",
        "- **Default**: `'gifs'`\n",
        "- **Example**:\n",
        "    ```bash\n",
        "    --save_gifs_dir /path/to/save/gifs\n",
        "    ```\n",
        "\n",
        "### `--alpha` (Optional)\n",
        "- **Description**: Alpha blending factor for combining the raw image and the mask. This controls how much the unmasked areas are blended with a black overlay.\n",
        "- **Type**: `float`\n",
        "- **Default**: `0.5`\n",
        "- **Example**:\n",
        "    ```bash\n",
        "    --alpha 0.7\n",
        "    ```\n",
        "\n",
        "### `--segmentation_type` (Required)\n",
        "- **Description**: Defines which type of segmentation overlay to apply to the raw data. This option determines which mask type will be used for overlaying the raw image.\n",
        "- **Type**: `int`\n",
        "- **Choices**:\n",
        "    - `0`: Raw image only (no overlay).\n",
        "    - `1`: Presynapse region.\n",
        "    - `2`: Postsynapse region.\n",
        "    - `3`: Both presynapse and postsynapse.\n",
        "    - `4`: Vesicles + Cleft (closest only).\n",
        "    - `5`: All structures (vesicles, clefts, mitochondria, and both synaptic sides).\n",
        "    - `6`: Vesicle cloud (closest).\n",
        "    - `7`: Cleft regions only.\n",
        "    - `8`: Mitochondria regions only.\n",
        "    - `9`: Vesicle + Cleft combined (closest).\n",
        "- **Default**: `6`\n",
        "- **Example**:\n",
        "    ```bash\n",
        "    --segmentation_type 3\n",
        "    ```\n",
        "\n",
        "## Example Usage\n",
        "\n",
        "Here is an example of how to run the script with the necessary arguments:\n",
        "\n",
        "```bash\n",
        "python data_loader.py \\\n",
        "    --raw_base_dir /path/to/raw/data \\\n",
        "    --seg_base_dir /path/to/segmentation/data \\\n",
        "    --add_mask_base_dir /path/to/additional/masks \\\n",
        "    --bbox_name bbox1 bbox2 \\\n",
        "    --excel_file /path/to/excel/files \\\n",
        "    --csv_output_dir /path/to/csv/outputs \\\n",
        "    --save_gifs_dir /path/to/save/gifs \\\n",
        "    --segmentation_type 2 \\\n",
        "    --alpha 0.5\n",
        "```\n",
        "\n",
        "## Segmentation Type Handling\n",
        "\n",
        "The segmentation logic is based on the `segmentation_type` argument. It determines how to combine masks and create the desired visualization.\n",
        "\n",
        "### Segmentation Logic:\n",
        "\n",
        "- **`0`**: Raw image (no overlay)\n",
        "- **`1`**: Presynapse region (based on overlap of vesicles with side1 or side2).\n",
        "- **`2`**: Postsynapse region (based on overlap of vesicles with side1 or side2).\n",
        "- **`3`**: Both presynapssde and postsynapse regions (overlay of both).\n",
        "- **`4`**: Vesicles and clefts (closest components).\n",
        "- **`5`**: All structures (vesicles, clefts, and both synaptic sides).\n",
        "- **`6`**: Vesicle cloud (closest to target).\n",
        "- **`7`**: Cleft regions (closest to target).\n",
        "- **`8`**: Mitochondria regions (closest to target).\n",
        "- **`9`**: Combined vesicle + cleft (closest to target).\n",
        "- **`10`**: presynapse + cleft (closest to target).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gq_5sVGQQiyo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import io\n",
        "import argparse\n",
        "import multiprocessing\n",
        "from typing import List, Tuple\n",
        "import imageio\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import imageio.v3 as iio\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision import transforms\n",
        "from scipy.ndimage import label, center_of_mass\n",
        "from tqdm import tqdm\n",
        "class Synapse3DProcessor:\n",
        "    def __init__(self, size=(80, 80), mean=(0.485,), std=(0.229,)):\n",
        "        # Use Grayscale without converting to 3-channel RGB\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize(size),\n",
        "            transforms.Grayscale(num_output_channels=1),  # Changed to 1 channel (grayscale)\n",
        "            transforms.ToTensor(),\n",
        "            # transforms.Normalize(mean=mean, std=std),\n",
        "        ])\n",
        "\n",
        "\n",
        "    def __call__(self, frames, return_tensors=None):\n",
        "        processed_frames = [self.transform(frame) for frame in frames]\n",
        "        pixel_values = torch.stack(processed_frames)\n",
        "        if return_tensors == \"pt\":\n",
        "            return {\"pixel_values\": pixel_values}\n",
        "        else:\n",
        "            return pixel_values\n",
        "\n",
        "def load_volumes(bbox_name: str, raw_base_dir: str, seg_base_dir: str, add_mask_base_dir: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    raw_dir = os.path.join(raw_base_dir, bbox_name)\n",
        "    seg_dir = os.path.join(seg_base_dir, bbox_name)\n",
        "    if bbox_name.startswith(\"bbox\"):\n",
        "        bbox_num = bbox_name.replace(\"bbox\", \"\")\n",
        "        add_mask_dir = os.path.join(add_mask_base_dir, f\"bbox_{bbox_num}\")\n",
        "    else:\n",
        "        add_mask_dir = os.path.join(add_mask_base_dir, bbox_name)\n",
        "    raw_tif_files = sorted(glob.glob(os.path.join(raw_dir, 'slice_*.tif')))\n",
        "    seg_tif_files = sorted(glob.glob(os.path.join(seg_dir, 'slice_*.tif')))\n",
        "    add_mask_tif_files = sorted(glob.glob(os.path.join(add_mask_dir, 'slice_*.tif')))\n",
        "    if not (len(raw_tif_files) == len(seg_tif_files) == len(add_mask_tif_files)):\n",
        "        return None, None, None\n",
        "    try:\n",
        "        raw_vol = np.stack([iio.imread(f) for f in raw_tif_files], axis=0)\n",
        "        seg_vol = np.stack([iio.imread(f).astype(np.uint32) for f in seg_tif_files], axis=0)\n",
        "        add_mask_vol = np.stack([iio.imread(f).astype(np.uint32) for f in add_mask_tif_files], axis=0)\n",
        "        return raw_vol, seg_vol, add_mask_vol\n",
        "    except Exception as e:\n",
        "        return None, None, None\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"Synapse Dataset\")\n",
        "    parser.add_argument('--raw_base_dir', type=str, default='raw')\n",
        "    parser.add_argument('--seg_base_dir', type=str, default='seg')\n",
        "    parser.add_argument('--add_mask_base_dir', type=str, default='')\n",
        "    parser.add_argument('--bbox_name', type=str, default=['bbox1'], nargs='+')\n",
        "    parser.add_argument('--excel_file', type=str, default='')\n",
        "    parser.add_argument('--csv_output_dir', type=str, default='csv_outputs')\n",
        "    parser.add_argument('--size', type=tuple, default=(80, 80))\n",
        "    parser.add_argument('--subvol_size', type=int, default=80)\n",
        "    parser.add_argument('--num_frames', type=int, default=80)\n",
        "    parser.add_argument('--save_gifs_dir', type=str, default='gifs')\n",
        "    parser.add_argument('--alpha', type=float, default=0.5)\n",
        "    parser.add_argument('--segmentation_type', type=int, default=6, choices=range(0, 13),\n",
        "                        help='Type of segmentation overlay')\n",
        "    args, _ = parser.parse_known_args()\n",
        "    return args\n",
        "def get_closest_component_mask(full_mask, z_start, z_end, y_start, y_end, x_start, x_end, target_coord):\n",
        "    sub_mask = full_mask[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "    labeled_sub_mask, num_features = label(sub_mask)\n",
        "    if num_features == 0:\n",
        "        return np.zeros_like(full_mask, dtype=bool)\n",
        "    else:\n",
        "        # For each label (vesicle cloud), find the nearest pixel to the target_coord\n",
        "        cx, cy, cz = target_coord\n",
        "        min_distance = float('inf')  # Initialize minimum distance as infinity\n",
        "        closest_label = None\n",
        "\n",
        "        for label_num in range(1, num_features + 1):  # labels are 1-based, not 0\n",
        "            # Get the coordinates of all pixels that belong to this label (vesicle cloud)\n",
        "            vesicle_coords = np.column_stack(np.where(labeled_sub_mask == label_num))\n",
        "\n",
        "            # Compute the distance of each pixel in the vesicle cloud to the target coordinate\n",
        "            distances = np.sqrt(\n",
        "                (vesicle_coords[:, 0] + z_start - cz) ** 2 +\n",
        "                (vesicle_coords[:, 1] + y_start - cy) ** 2 +\n",
        "                (vesicle_coords[:, 2] + x_start - cx) ** 2\n",
        "            )\n",
        "\n",
        "            # Find the pixel with the minimum distance\n",
        "            min_dist_for_vesicle = np.min(distances)\n",
        "            if min_dist_for_vesicle < min_distance:\n",
        "                min_distance = min_dist_for_vesicle\n",
        "                closest_label = label_num\n",
        "\n",
        "        # Now, create a mask for the closest vesicle cloud\n",
        "        if closest_label is not None:\n",
        "            filtered_sub_mask = (labeled_sub_mask == closest_label)\n",
        "            combined_mask = np.zeros_like(full_mask, dtype=bool)\n",
        "            combined_mask[z_start:z_end, y_start:y_end, x_start:x_end] = filtered_sub_mask\n",
        "            return combined_mask\n",
        "        else:\n",
        "            return np.zeros_like(full_mask, dtype=bool)\n",
        "\n",
        "def create_segmented_cube(\n",
        "    raw_vol: np.ndarray,\n",
        "    seg_vol: np.ndarray,\n",
        "    add_mask_vol: np.ndarray,\n",
        "    central_coord: Tuple[int, int, int],\n",
        "    side1_coord: Tuple[int, int, int],\n",
        "    side2_coord: Tuple[int, int, int],\n",
        "    segmentation_type: int,\n",
        "    subvolume_size: int = 80,\n",
        "    alpha: float = 0.3,\n",
        "    bbox_name: str = \"\",\n",
        ") -> np.ndarray:\n",
        "    bbox_num = bbox_name.replace(\"bbox\", \"\").strip()\n",
        "    if bbox_num in {'2', '5',}:\n",
        "        mito_label = 1\n",
        "        vesicle_label = 3\n",
        "        cleft_label2 = 4\n",
        "        cleft_label = 2\n",
        "    elif bbox_num == '7':\n",
        "        mito_label = 1\n",
        "        vesicle_label = 2\n",
        "        cleft_label2 = 3\n",
        "        cleft_label = 4\n",
        "    elif bbox_num == '4':\n",
        "        mito_label = 3\n",
        "        vesicle_label = 2\n",
        "        cleft_label2 = 4\n",
        "        cleft_label = 1\n",
        "    elif bbox_num == '3':\n",
        "        # print(\"bbox_num3\")\n",
        "        mito_label = 6\n",
        "        vesicle_label = 7\n",
        "        cleft_label2 = 8\n",
        "        cleft_label = 9\n",
        "    else:  # For bbox1, 3, 6, etc.\n",
        "        mito_label = 5\n",
        "        vesicle_label = 6\n",
        "        cleft_label = 7\n",
        "        cleft_label2 = 7\n",
        "\n",
        "    # --- Always calculate subvolume bounds FIRST ---\n",
        "    half_size = subvolume_size // 2\n",
        "    cx, cy, cz = central_coord\n",
        "    x_start = max(cx - half_size, 0)\n",
        "    x_end = min(cx + half_size, raw_vol.shape[2])\n",
        "    y_start = max(cy - half_size, 0)\n",
        "    y_end = min(cy + half_size, raw_vol.shape[1])\n",
        "    z_start = max(cz - half_size, 0)\n",
        "    z_end = min(cz + half_size, raw_vol.shape[0])\n",
        "\n",
        "    # --- Vesicle filtering (critical for presynapse determination) ---\n",
        "    vesicle_full_mask = (add_mask_vol == vesicle_label)\n",
        "    vesicle_mask = get_closest_component_mask(\n",
        "        vesicle_full_mask,\n",
        "        z_start, z_end,\n",
        "        y_start, y_end,\n",
        "        x_start, x_end,\n",
        "        (cx, cy, cz)\n",
        "    )\n",
        "\n",
        "    # --- Side masks ---\n",
        "    def create_segment_masks(segmentation_volume, s1_coord, s2_coord):\n",
        "        x1, y1, z1 = s1_coord\n",
        "        x2, y2, z2 = s2_coord\n",
        "        seg_id_1 = segmentation_volume[z1, y1, x1]\n",
        "        seg_id_2 = segmentation_volume[z2, y2, x2]\n",
        "        mask_1 = (segmentation_volume == seg_id_1) if seg_id_1 != 0 else np.zeros_like(segmentation_volume, dtype=bool)\n",
        "        mask_2 = (segmentation_volume == seg_id_2) if seg_id_2 != 0 else np.zeros_like(segmentation_volume, dtype=bool)\n",
        "        return mask_1, mask_2\n",
        "\n",
        "    mask_1_full, mask_2_full = create_segment_masks(seg_vol, side1_coord, side2_coord)\n",
        "\n",
        "    # --- Determine pre-synapse side using filtered vesicles ---\n",
        "    overlap_side1 = np.sum(np.logical_and(mask_1_full, vesicle_mask))\n",
        "    overlap_side2 = np.sum(np.logical_and(mask_2_full, vesicle_mask))\n",
        "    presynapse_side = 1 if overlap_side1 > overlap_side2 else 2\n",
        "    if segmentation_type == 0: # Raw data\n",
        "        combined_mask_full = np.ones_like(add_mask_vol, dtype=bool)\n",
        "    elif segmentation_type == 1:  # Presynapse\n",
        "        combined_mask_full = mask_1_full if presynapse_side == 1 else mask_2_full\n",
        "    elif segmentation_type == 2:  # Postsynapse\n",
        "        combined_mask_full = mask_2_full if presynapse_side == 1 else mask_1_full\n",
        "    elif segmentation_type == 3:  # Both sides\n",
        "        combined_mask_full = np.logical_or(mask_1_full, mask_2_full)\n",
        "    elif segmentation_type == 4:  # Vesicles + Cleft (closest only)\n",
        "        vesicle_closest = get_closest_component_mask(\n",
        "            (add_mask_vol == vesicle_label), z_start, z_end, y_start, y_end, x_start, x_end, (cx, cy, cz)\n",
        "        )\n",
        "        cleft_closest = get_closest_component_mask(\n",
        "            ((add_mask_vol == cleft_label)), z_start, z_end, y_start, y_end, x_start, x_end, (cx, cy, cz)\n",
        "        )\n",
        "        cleft_closest2 = get_closest_component_mask(\n",
        "            ((add_mask_vol == cleft_label2)), z_start, z_end, y_start, y_end, x_start, x_end, (cx, cy, cz)\n",
        "        )\n",
        "        combined_mask_full = np.logical_or(vesicle_closest, np.logical_or(cleft_closest,cleft_closest2))\n",
        "    elif segmentation_type == 5:  # (closest vesicles/cleft + sides)\n",
        "        vesicle_closest = get_closest_component_mask(\n",
        "            (add_mask_vol == vesicle_label), z_start, z_end, y_start, y_end, x_start, x_end, (cx, cy, cz)\n",
        "        )\n",
        "        cleft_closest = get_closest_component_mask(\n",
        "            (add_mask_vol == cleft_label), z_start, z_end, y_start, y_end, x_start, x_end, (cx, cy, cz)\n",
        "        )\n",
        "        combined_mask_extra = np.logical_or(vesicle_closest, cleft_closest)\n",
        "        combined_mask_full = np.logical_or(mask_1_full, np.logical_or(mask_2_full, combined_mask_extra))\n",
        "    elif segmentation_type == 6:  # Vesicle cloud (closest)\n",
        "        combined_mask_full = get_closest_component_mask(\n",
        "            (add_mask_vol == vesicle_label), z_start, z_end, y_start, y_end, x_start, x_end, (cx, cy, cz)\n",
        "        )\n",
        "    elif segmentation_type == 7:  # Cleft (closest)\n",
        "        cleft_closest = get_closest_component_mask(\n",
        "            ((add_mask_vol == cleft_label)), z_start, z_end, y_start, y_end, x_start, x_end, (cx, cy, cz)\n",
        "        )\n",
        "        cleft_closest2 = get_closest_component_mask(\n",
        "            ((add_mask_vol == cleft_label2)), z_start, z_end, y_start, y_end, x_start, x_end, (cx, cy, cz)\n",
        "        )\n",
        "        combined_mask_full =  np.logical_or(cleft_closest,cleft_closest2)\n",
        "    elif segmentation_type == 8:  # Mitochondria (closest)\n",
        "        combined_mask_full = get_closest_component_mask(\n",
        "            (add_mask_vol == mito_label), z_start, z_end, y_start, y_end, x_start, x_end, (cx, cy, cz)\n",
        "        )\n",
        "    elif segmentation_type == 10:  #  +Cleft +pre\n",
        "        cleft_closest = get_closest_component_mask(\n",
        "            (add_mask_vol == cleft_label), z_start, z_end, y_start, y_end, x_start, x_end, (cx, cy, cz)\n",
        "        )\n",
        "        pre_mask_full = mask_1_full if presynapse_side == 1 else mask_2_full\n",
        "\n",
        "        combined_mask_full = np.logical_or(cleft_closest,pre_mask_full)\n",
        "\n",
        "    elif segmentation_type == 9:  # cleft+vesicle\n",
        "        vesicle_closest = get_closest_component_mask(\n",
        "            (add_mask_vol == vesicle_label), z_start, z_end, y_start, y_end, x_start, x_end, (cx, cy, cz)\n",
        "        )\n",
        "        cleft_closest = get_closest_component_mask(\n",
        "            (add_mask_vol == cleft_label), z_start, z_end, y_start, y_end, x_start, x_end, (cx, cy, cz)\n",
        "        )\n",
        "\n",
        "        combined_mask_full = np.logical_or(cleft_closest,vesicle_closest)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported segmentation type: {segmentation_type}\")\n",
        "\n",
        "    # --- Subvolume extraction and processing ---\n",
        "    sub_raw = raw_vol[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "    sub_combined_mask = combined_mask_full[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "\n",
        "    # Padding if needed\n",
        "    pad_z = subvolume_size - sub_raw.shape[0]\n",
        "    pad_y = subvolume_size - sub_raw.shape[1]\n",
        "    pad_x = subvolume_size - sub_raw.shape[2]\n",
        "    if pad_z > 0 or pad_y > 0 or pad_x > 0:\n",
        "        sub_raw = np.pad(sub_raw, ((0, pad_z), (0, pad_y), (0, pad_x)), mode='constant', constant_values=0)\n",
        "        sub_combined_mask = np.pad(sub_combined_mask, ((0, pad_z), (0, pad_y), (0, pad_x)), mode='constant', constant_values=False)\n",
        "\n",
        "    sub_raw = sub_raw[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "    sub_combined_mask = sub_combined_mask[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "\n",
        "    # Vectorized normalization\n",
        "    sub_raw = sub_raw.astype(np.float32)\n",
        "    mins = np.min(sub_raw, axis=(1, 2), keepdims=True)\n",
        "    maxs = np.max(sub_raw, axis=(1, 2), keepdims=True)\n",
        "    ranges = np.where(maxs > mins, maxs - mins, 1.0)\n",
        "    normalized = (sub_raw - mins) / ranges\n",
        "\n",
        "    # Define gray color (0.5 for grayscale)\n",
        "    gray_color = 0.6  # For grayscale\n",
        "\n",
        "    # Vectorized blending with gray color\n",
        "    raw_rgb = np.repeat(normalized[..., np.newaxis], 3, axis=-1)  # Convert to RGB\n",
        "    mask_factor = sub_combined_mask[..., np.newaxis]  # Adding an extra dimension to make it (80, 80, 80, 1)\n",
        "\n",
        "    if alpha < 1:\n",
        "        blended_part = alpha * gray_color + (1 - alpha) * raw_rgb  # Blend with gray\n",
        "    else:\n",
        "        # When alpha is 1, apply gray only to unmasked areas (grayscale), keep raw_rgb in masked areas\n",
        "        blended_part = gray_color * (1 - mask_factor) + raw_rgb * mask_factor\n",
        "\n",
        "    # Now, overlaid_image will be computed as follows\n",
        "    overlaid_image = raw_rgb * mask_factor + (1 - mask_factor) * blended_part\n",
        "\n",
        "    # Convert to uint8 and transpose dimensions\n",
        "    overlaid_cube = np.transpose(overlaid_image, (1, 2, 3, 0))  # Keep it grayscale\n",
        "\n",
        "    return overlaid_cube\n",
        "\n",
        "class SynapseDataset(Dataset):\n",
        "    def __init__(self, vol_data_dict: dict, synapse_df: pd.DataFrame, processor,\n",
        "                 segmentation_type: int, subvol_size: int = 80, num_frames: int = 16,\n",
        "                 alpha: float = 0.3):\n",
        "        self.vol_data_dict = vol_data_dict\n",
        "        self.synapse_df = synapse_df.reset_index(drop=True)\n",
        "        self.processor = processor\n",
        "        self.segmentation_type = segmentation_type\n",
        "        self.subvol_size = subvol_size\n",
        "        self.num_frames = num_frames\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.synapse_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        syn_info = self.synapse_df.iloc[idx]\n",
        "        bbox_name = syn_info['bbox_name']\n",
        "        raw_vol, seg_vol, add_mask_vol = self.vol_data_dict.get(bbox_name, (None, None, None))\n",
        "        if raw_vol is None:\n",
        "            return torch.zeros((self.num_frames, 1, self.subvol_size, self.subvol_size), dtype=torch.float32), syn_info, bbox_name\n",
        "\n",
        "        central_coord = (int(syn_info['central_coord_1']), int(syn_info['central_coord_2']), int(syn_info['central_coord_3']))\n",
        "        side1_coord = (int(syn_info['side_1_coord_1']), int(syn_info['side_1_coord_2']), int(syn_info['side_1_coord_3']))\n",
        "        side2_coord = (int(syn_info['side_2_coord_1']), int(syn_info['side_2_coord_2']), int(syn_info['side_2_coord_3']))\n",
        "\n",
        "        overlaid_cube = create_segmented_cube(\n",
        "            raw_vol=raw_vol,\n",
        "            seg_vol=seg_vol,\n",
        "            add_mask_vol=add_mask_vol,\n",
        "            central_coord=central_coord,\n",
        "            side1_coord=side1_coord,\n",
        "            side2_coord=side2_coord,\n",
        "            segmentation_type=self.segmentation_type,\n",
        "            subvolume_size=self.subvol_size,\n",
        "            alpha=self.alpha,\n",
        "            bbox_name=bbox_name,  # Pass bbox_name here\n",
        "        )\n",
        "        frames = [overlaid_cube[..., z] for z in range(overlaid_cube.shape[3])]\n",
        "        if len(frames) < self.num_frames:\n",
        "            frames += [frames[-1]] * (self.num_frames - len(frames))\n",
        "        elif len(frames) > self.num_frames:\n",
        "            indices = np.linspace(0, len(frames)-1, self.num_frames, dtype=int)\n",
        "            frames = [frames[i] for i in indices]\n",
        "\n",
        "        inputs = self.processor(frames, return_tensors=\"pt\")\n",
        "        return inputs[\"pixel_values\"].squeeze(0).float(), syn_info, bbox_name\n",
        "\n",
        "# Add unique IDs to fixed_samples\n",
        "fixed_samples = [\n",
        "    {\"id\": 1, \"bbox_name\": \"bbox1\", \"Var1\": \"non_spine_synapse_004\", \"slice_number\": 25},\n",
        "    {\"id\": 2, \"bbox_name\": \"bbox1\", \"Var1\": \"non_spine_synapse_006\", \"slice_number\": 40},\n",
        "    {\"id\": 4, \"bbox_name\": \"bbox2\", \"Var1\": \"explorative_2024-08-28_Cora_Wolter_031\", \"slice_number\": 43},\n",
        "    {\"id\": 3, \"bbox_name\": \"bbox2\", \"Var1\": \"explorative_2024-08-28_Cora_Wolter_051\", \"slice_number\": 28},\n",
        "    {\"id\": 5, \"bbox_name\": \"bbox3\", \"Var1\": \"non_spine_synapse_036\", \"slice_number\": 41},\n",
        "    {\"id\": 6, \"bbox_name\": \"bbox3\", \"Var1\": \"non_spine_synapse_018\", \"slice_number\": 41},\n",
        "    {\"id\": 7, \"bbox_name\": \"bbox4\", \"Var1\": \"explorative_2024-08-03_Ali_Karimi_023\", \"slice_number\": 28},\n",
        "    {\"id\": 8, \"bbox_name\": \"bbox5\", \"Var1\": \"non_spine_synapse_033\", \"slice_number\": 48},\n",
        "    {\"id\": 9, \"bbox_name\": \"bbox5\", \"Var1\": \"non_spine_synapse_045\", \"slice_number\": 40},\n",
        "    {\"id\": 10, \"bbox_name\": \"bbox6\", \"Var1\": \"spine_synapse_070\", \"slice_number\": 37},\n",
        "    {\"id\": 11, \"bbox_name\": \"bbox6\", \"Var1\": \"spine_synapse_021\", \"slice_number\": 30},\n",
        "    {\"id\": 12, \"bbox_name\": \"bbox7\", \"Var1\": \"non_spine_synapse_013\", \"slice_number\": 25},\n",
        "]\n",
        "\n",
        "\n",
        "args = parse_args()\n",
        "args.bbox_name=['bbox1','bbox2','bbox3','bbox4','bbox5','bbox6','bbox7',]\n",
        "# args.bbox_name=['bbox4']\n",
        "\n",
        "# Load volumes\n",
        "vol_data_dict = {}\n",
        "for bbox_name in args.bbox_name:\n",
        "    raw_vol, seg_vol, add_mask_vol = load_volumes(\n",
        "        bbox_name=bbox_name,\n",
        "        raw_base_dir=args.raw_base_dir,\n",
        "        seg_base_dir=args.seg_base_dir,\n",
        "        add_mask_base_dir=args.add_mask_base_dir\n",
        "    )\n",
        "    if raw_vol is not None:\n",
        "        vol_data_dict[bbox_name] = (raw_vol, seg_vol, add_mask_vol)\n",
        "\n",
        "# Load synapse data\n",
        "syn_df = pd.concat([\n",
        "    pd.read_excel(os.path.join(args.excel_file, f\"{bbox}.xlsx\")).assign(bbox_name=bbox)\n",
        "    for bbox in args.bbox_name if os.path.exists(os.path.join(args.excel_file, f\"{bbox}.xlsx\"))\n",
        "])\n",
        "\n",
        "# Initialize model\n",
        "processor = Synapse3DProcessor(size=args.size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import imageio\n",
        "bbox_name=['bbox1','bbox2','bbox3','bbox4','bbox5','bbox6','bbox7',]\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"Synapse Dataset\")\n",
        "    parser.add_argument('--raw_base_dir', type=str, default='raw')\n",
        "    parser.add_argument('--seg_base_dir', type=str, default='seg')\n",
        "    parser.add_argument('--add_mask_base_dir', type=str, default='')\n",
        "    parser.add_argument('--bbox_name', type=str, default=['bbox4'], nargs='+')\n",
        "    parser.add_argument('--excel_file', type=str, default='')\n",
        "    parser.add_argument('--csv_output_dir', type=str, default='csv_outputs')\n",
        "    parser.add_argument('--size', type=tuple, default=(80, 80))\n",
        "    parser.add_argument('--subvol_size', type=int, default=80)\n",
        "    parser.add_argument('--num_frames', type=int, default=80)\n",
        "    parser.add_argument('--save_gifs_dir', type=str, default='gifs')\n",
        "    parser.add_argument('--alpha', type=float, default=0.7)\n",
        "    parser.add_argument('--segmentation_type', type=int, default=4, choices=range(0, 13),\n",
        "                        help='Type of segmentation overlay')\n",
        "    args, _ = parser.parse_known_args()\n",
        "    return args\n",
        "args = parse_args()\n",
        "args.bbox_name=bbox_name\n",
        "args.segmentation_type=1\n",
        "vol_data_dict = {}\n",
        "for bbox_name in args.bbox_name:\n",
        "    raw_vol, seg_vol, add_mask_vol = load_volumes(\n",
        "        bbox_name=bbox_name,\n",
        "        raw_base_dir=args.raw_base_dir,\n",
        "        seg_base_dir=args.seg_base_dir,\n",
        "        add_mask_base_dir=args.add_mask_base_dir\n",
        "    )\n",
        "    if raw_vol is not None:\n",
        "        vol_data_dict[bbox_name] = (raw_vol, seg_vol, add_mask_vol)\n",
        "\n",
        "syn_df = pd.concat([\n",
        "    pd.read_excel(os.path.join(args.excel_file, f\"{bbox}.xlsx\")).assign(bbox_name=bbox)\n",
        "    for bbox in args.bbox_name if os.path.exists(os.path.join(args.excel_file, f\"{bbox}.xlsx\"))\n",
        "])\n",
        "\n",
        "processor = Synapse3DProcessor(size=args.size)\n",
        "\n",
        "\n",
        "dataset = SynapseDataset(\n",
        "    vol_data_dict=vol_data_dict,\n",
        "    synapse_df=syn_df,\n",
        "    processor=processor,\n",
        "    segmentation_type=args.segmentation_type,\n",
        "    subvol_size=args.subvol_size,\n",
        "    num_frames=args.num_frames,\n",
        "    alpha=0.5\n",
        ")\n"
      ],
      "metadata": {
        "id": "c5LgD5ff1KY7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "daa58lj6b7qc"
      ],
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}