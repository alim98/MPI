{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "daa58lj6b7qc"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPPQvdJzWAuyrWUh0QKC1ks",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alim98/MPI/blob/main/temp/Final_DL_VGG_MPItemp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Essential downloads"
      ],
      "metadata": {
        "id": "daa58lj6b7qc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O downloaded_file.zip \"https://drive.usercontent.google.com/download?id=1iHPBdBOPEagvPTHZmrN__LD49emXwReY&export=download&authuser=0&confirm=t&uuid=631d60dd-569c-4bb1-a9e8-d681f0ed3d43&at=APvzH3r4me8x_LwP3n8O7lgPo8oK%3A1733988188000\"\n",
        "!wget -O vesicle_cloud__syn_interface__mitochondria_annotation.zip \"https://drive.usercontent.google.com/download?id=1qRibZL3kr7MQJQRgDFRquHMQlIGCN4XP&export=download&authuser=0&confirm=t&uuid=631d60dd-569c-4bb1-a9e8-d681f0ed3d43&at=APvzH3r4me8x_LwP3n8O7lgPo8oK%3A1733988188000\"\n",
        "\n",
        "!unzip -q downloaded_file.zip\n",
        "!unzip -q vesicle_cloud__syn_interface__mitochondria_annotation.zip\n",
        "\n",
        "!pip install transformers scikit-learn matplotlib seaborn torch torchvision umap-learn git+https://github.com/funkelab/funlib.learn.torch.git\n",
        "!pip install openpyxl\n"
      ],
      "metadata": {
        "id": "AIIIxarwTVT0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "158151cc-86ec-4b02-864c-ee56cedfdd58"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-31 08:04:41--  https://drive.usercontent.google.com/download?id=1iHPBdBOPEagvPTHZmrN__LD49emXwReY&export=download&authuser=0&confirm=t&uuid=631d60dd-569c-4bb1-a9e8-d681f0ed3d43&at=APvzH3r4me8x_LwP3n8O7lgPo8oK%3A1733988188000\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 64.233.189.132, 2404:6800:4008:c07::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|64.233.189.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1264688649 (1.2G) [application/octet-stream]\n",
            "Saving to: ‘downloaded_file.zip’\n",
            "\n",
            "downloaded_file.zip 100%[===================>]   1.18G  54.2MB/s    in 21s     \n",
            "\n",
            "2025-01-31 08:05:05 (56.1 MB/s) - ‘downloaded_file.zip’ saved [1264688649/1264688649]\n",
            "\n",
            "--2025-01-31 08:05:05--  https://drive.usercontent.google.com/download?id=1qRibZL3kr7MQJQRgDFRquHMQlIGCN4XP&export=download&authuser=0&confirm=t&uuid=631d60dd-569c-4bb1-a9e8-d681f0ed3d43&at=APvzH3r4me8x_LwP3n8O7lgPo8oK%3A1733988188000\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 64.233.189.132, 2404:6800:4008:c07::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|64.233.189.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14246564 (14M) [application/octet-stream]\n",
            "Saving to: ‘vesicle_cloud__syn_interface__mitochondria_annotation.zip’\n",
            "\n",
            "vesicle_cloud__syn_ 100%[===================>]  13.59M  29.7MB/s    in 0.5s    \n",
            "\n",
            "2025-01-31 08:05:37 (29.7 MB/s) - ‘vesicle_cloud__syn_interface__mitochondria_annotation.zip’ saved [14246564/14246564]\n",
            "\n",
            "Collecting git+https://github.com/funkelab/funlib.learn.torch.git\n",
            "  Cloning https://github.com/funkelab/funlib.learn.torch.git to /tmp/pip-req-build-swamrf0a\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/funkelab/funlib.learn.torch.git /tmp/pip-req-build-swamrf0a\n",
            "  Resolved https://github.com/funkelab/funlib.learn.torch.git to commit 049729151c7a2c0320a446dc9d3244ac830f7ea8\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Collecting umap-learn\n",
            "  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.60.0)\n",
            "Collecting pynndescent>=0.5 (from umap-learn)\n",
            "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: funlib.learn.torch\n",
            "  Building wheel for funlib.learn.torch (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for funlib.learn.torch: filename=funlib.learn.torch-0.1.0-py3-none-any.whl size=13995 sha256=b1a0fb30fd37b84f194ed5b0a41af72257ed21a03215447c61c003f8fd60a230\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-sg0lnuqm/wheels/ae/7f/7b/ecbd355ccdfbd2bb0ab4f76ea45f60a02a572c88c1f4761e8d\n",
            "Successfully built funlib.learn.torch\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, pynndescent, nvidia-cusolver-cu12, umap-learn, funlib.learn.torch\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed funlib.learn.torch-0.1.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pynndescent-0.5.13 umap-learn-0.5.7\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Version1 , black overlay on unmasked sections of image"
      ],
      "metadata": {
        "id": "_vXAE8wVmlWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Alpha Blending (Unmasked Areas):\n",
        "# Instead of blending the raw image with the mask and using a color, for unmasked regions we now blend the raw_rgb image with a black overlay. This keeps the masked regions unchanged but gives unmasked regions a 50% black overlay.\n",
        "# Unmasked Regions:\n",
        "# The raw image is modified so that the unmasked areas blend with 50% black (0.5 * raw_rgb + 0.5 * black_overlay), where black_overlay is a black (zeroed) array of the same shape as the raw_rgb.\n",
        "import os\n",
        "import glob\n",
        "import io\n",
        "import argparse\n",
        "import multiprocessing\n",
        "from typing import List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import imageio.v3 as iio\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision import transforms\n",
        "class Synapse3DProcessor:\n",
        "    def __init__(self, size=(80, 80), mean=(0.485,),  # Use a single channel for grayscale\n",
        "                 std=(0.229,)):\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize(size),\n",
        "            transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=mean, std=std),\n",
        "        ])\n",
        "\n",
        "    def __call__(self, frames, return_tensors=None):\n",
        "        processed_frames = [self.transform(frame) for frame in frames]\n",
        "        pixel_values = torch.stack(processed_frames)\n",
        "        if return_tensors == \"pt\":\n",
        "            return {\"pixel_values\": pixel_values}\n",
        "        else:\n",
        "            return pixel_values\n",
        "\n",
        "\n",
        "def load_volumes(bbox_name: str, raw_base_dir: str, seg_base_dir: str, add_mask_base_dir: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    raw_dir = os.path.join(raw_base_dir, bbox_name)\n",
        "    seg_dir = os.path.join(seg_base_dir, bbox_name)\n",
        "\n",
        "    if bbox_name.startswith(\"bbox\"):\n",
        "        bbox_num = bbox_name.replace(\"bbox\", \"\")\n",
        "        add_mask_dir = os.path.join(add_mask_base_dir, f\"bbox_{bbox_num}\")\n",
        "    else:\n",
        "        add_mask_dir = os.path.join(add_mask_base_dir, bbox_name)\n",
        "\n",
        "    raw_tif_files = sorted(glob.glob(os.path.join(raw_dir, 'slice_*.tif')))\n",
        "    seg_tif_files = sorted(glob.glob(os.path.join(seg_dir, 'slice_*.tif')))\n",
        "    add_mask_tif_files = sorted(glob.glob(os.path.join(add_mask_dir, 'slice_*.tif')))\n",
        "\n",
        "    if not (len(raw_tif_files) == len(seg_tif_files) == len(add_mask_tif_files)):\n",
        "        return None, None, None\n",
        "\n",
        "    try:\n",
        "        raw_vol = np.stack([iio.imread(f) for f in raw_tif_files], axis=0)\n",
        "        seg_vol = np.stack([iio.imread(f).astype(np.uint32) for f in seg_tif_files], axis=0)\n",
        "        add_mask_vol = np.stack([iio.imread(f).astype(np.uint32) for f in add_mask_tif_files], axis=0)\n",
        "        return raw_vol, seg_vol, add_mask_vol\n",
        "    except Exception as e:\n",
        "        return None, None, None\n",
        "\n",
        "\n",
        "def create_segmented_cube(\n",
        "    raw_vol: np.ndarray,\n",
        "    seg_vol: np.ndarray,\n",
        "    add_mask_vol: np.ndarray,\n",
        "    central_coord: Tuple[int, int, int],\n",
        "    side1_coord: Tuple[int, int, int],\n",
        "    side2_coord: Tuple[int, int, int],\n",
        "    segmentation_type: int,\n",
        "    subvolume_size: int = 80,\n",
        "    alpha: float = 0.3,\n",
        "    input_mask: bool = False  # New parameter\n",
        ") -> np.ndarray:\n",
        "    def create_segment_masks(segmentation_volume, s1_coord, s2_coord):\n",
        "        x1, y1, z1 = s1_coord\n",
        "        x2, y2, z2 = s2_coord\n",
        "        seg_id_1 = segmentation_volume[z1, y1, x1]\n",
        "        seg_id_2 = segmentation_volume[z2, y2, x2]\n",
        "\n",
        "        mask_1 = (segmentation_volume == seg_id_1) if seg_id_1 != 0 else np.zeros_like(segmentation_volume, dtype=bool)\n",
        "        mask_2 = (segmentation_volume == seg_id_2) if seg_id_2 != 0 else np.zeros_like(segmentation_volume, dtype=bool)\n",
        "        return mask_1, mask_2\n",
        "\n",
        "    mask_1_full, mask_2_full = create_segment_masks(seg_vol, side1_coord, side2_coord)\n",
        "    mask_3_full = (add_mask_vol > 0)\n",
        "\n",
        "    # Combine all the masks using OR\n",
        "    combined_mask_full = np.logical_or(mask_1_full, np.logical_or(mask_2_full, mask_3_full))\n",
        "\n",
        "    half_size = subvolume_size // 2\n",
        "    cx, cy, cz = central_coord\n",
        "    x_start, x_end = max(cx - half_size, 0), min(cx + half_size, raw_vol.shape[2])\n",
        "    y_start, y_end = max(cy - half_size, 0), min(cy + half_size, raw_vol.shape[1])\n",
        "    z_start, z_end = max(cz - half_size, 0), min(cz + half_size, raw_vol.shape[0])\n",
        "\n",
        "    sub_raw = raw_vol[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "    sub_combined_mask = combined_mask_full[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "\n",
        "    pad_z = subvolume_size - sub_raw.shape[0]\n",
        "    pad_y = subvolume_size - sub_raw.shape[1]\n",
        "    pad_x = subvolume_size - sub_raw.shape[2]\n",
        "\n",
        "    if pad_z > 0 or pad_y > 0 or pad_x > 0:\n",
        "        sub_raw = np.pad(sub_raw, ((0, pad_z), (0, pad_y), (0, pad_x)), mode='constant', constant_values=0)\n",
        "        sub_combined_mask = np.pad(sub_combined_mask, ((0, pad_z), (0, pad_y), (0, pad_x)), mode='constant', constant_values=False)\n",
        "\n",
        "    sub_raw = sub_raw[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "    sub_combined_mask = sub_combined_mask[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "\n",
        "    overlaid_cube = np.zeros((subvolume_size, subvolume_size, 3, subvolume_size), dtype=np.uint8)\n",
        "\n",
        "    alpha_black = 0.7  # Adjust this value to control the darkness of unmasked regions\n",
        "\n",
        "    for z in range(subvolume_size):\n",
        "        raw_slice = sub_raw[z].astype(np.float32)\n",
        "        mn, mx = raw_slice.min(), raw_slice.max()\n",
        "        if mx > mn:\n",
        "            raw_slice = (raw_slice - mn) / (mx - mn)\n",
        "        else:\n",
        "            raw_slice = raw_slice - mn\n",
        "\n",
        "        # Apply blending based on the combined mask\n",
        "        raw_rgb = np.stack([raw_slice] * 3, axis=-1)\n",
        "\n",
        "        # Create mask factor\n",
        "        mask_factor = sub_combined_mask[z, :, :, np.newaxis]\n",
        "\n",
        "        # Blend: mask regions remain original, unmasked are scaled by (1 - alpha_black)\n",
        "        raw_rgb = raw_rgb * (mask_factor + (1 - mask_factor) * (1 - alpha_black))\n",
        "\n",
        "        overlaid_image = (np.clip(raw_rgb, 0, 1) * 255).astype(np.uint8)\n",
        "\n",
        "        overlaid_cube[:, :, :, z] = overlaid_image\n",
        "\n",
        "    return overlaid_cube\n",
        "\n",
        "\n",
        "class SynapseDataset(Dataset):\n",
        "    def __init__(self, vol_data_dict: dict, synapse_df: pd.DataFrame, processor,\n",
        "                 segmentation_type: int, subvol_size: int = 80, num_frames: int = 16,\n",
        "                 alpha: float = 0.3, input_mask: bool = False):  # Added input_mask\n",
        "        self.vol_data_dict = vol_data_dict\n",
        "        self.synapse_df = synapse_df.reset_index(drop=True)\n",
        "        self.processor = processor\n",
        "        self.segmentation_type = segmentation_type\n",
        "        self.subvol_size = subvol_size\n",
        "        self.num_frames = num_frames\n",
        "        self.alpha = alpha\n",
        "        self.input_mask = input_mask  # Store input_mask flag\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.synapse_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        syn_info = self.synapse_df.iloc[idx]\n",
        "        bbox_name = syn_info['bbox_name']\n",
        "        raw_vol, seg_vol, add_mask_vol = self.vol_data_dict.get(bbox_name, (None, None, None))\n",
        "\n",
        "        if raw_vol is None:\n",
        "            return torch.zeros((self.num_frames, 3, self.subvol_size, self.subvol_size), dtype=torch.float32), syn_info, bbox_name\n",
        "\n",
        "        central_coord = (int(syn_info['central_coord_1']), int(syn_info['central_coord_2']), int(syn_info['central_coord_3']))\n",
        "        side1_coord = (int(syn_info['side_1_coord_1']), int(syn_info['side_1_coord_2']), int(syn_info['side_1_coord_3']))\n",
        "        side2_coord = (int(syn_info['side_2_coord_1']), int(syn_info['side_2_coord_2']), int(syn_info['side_2_coord_3']))\n",
        "\n",
        "        overlaid_cube = create_segmented_cube(\n",
        "            raw_vol=raw_vol,\n",
        "            seg_vol=seg_vol,\n",
        "            add_mask_vol=add_mask_vol,\n",
        "            central_coord=central_coord,\n",
        "            side1_coord=side1_coord,\n",
        "            side2_coord=side2_coord,\n",
        "            segmentation_type=self.segmentation_type,\n",
        "            subvolume_size=self.subvol_size,\n",
        "            alpha=self.alpha,\n",
        "            input_mask=self.input_mask  # Pass the flag\n",
        "        )\n",
        "\n",
        "        frames = [overlaid_cube[..., z] for z in range(overlaid_cube.shape[3])]\n",
        "        if len(frames) < self.num_frames:\n",
        "            frames += [frames[-1]] * (self.num_frames - len(frames))\n",
        "        elif len(frames) > self.num_frames:\n",
        "            indices = np.linspace(0, len(frames)-1, self.num_frames, dtype=int)\n",
        "            frames = [frames[i] for i in indices]\n",
        "\n",
        "        inputs = self.processor(frames, return_tensors=\"pt\")\n",
        "        return inputs[\"pixel_values\"].squeeze(0).float(), syn_info, bbox_name\n",
        "# ,'bbox2','bbox3','bbox4','bbox5','bbox6',\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"VideoMAE Pre-training Script with Segmented Videos and Additional Masks\")\n",
        "    parser.add_argument('--raw_base_dir', type=str, default='raw')\n",
        "    parser.add_argument('--seg_base_dir', type=str, default='seg')\n",
        "    parser.add_argument('--add_mask_base_dir', type=str, default='')\n",
        "    parser.add_argument('--bbox_name', type=str, default=['bbox1' ], nargs='+')\n",
        "    parser.add_argument('--excel_file', type=str, default='')\n",
        "    parser.add_argument('--csv_output_dir', type=str, default='csv_outputs')\n",
        "    parser.add_argument('--checkpoint_dir', type=str, default='checkpoints')\n",
        "    parser.add_argument('--log_dir', type=str, default='logs')\n",
        "    parser.add_argument('--size', type=tuple, default=(80,80))\n",
        "    parser.add_argument('--batch_size', type=int, default=2)\n",
        "    parser.add_argument('--num_epochs', type=int, default=5)\n",
        "    parser.add_argument('--learning_rate', type=float, default=1e-4)\n",
        "    parser.add_argument('--weight_decay', type=float, default=1e-2)\n",
        "    parser.add_argument('--subvol_size', type=int, default=80)\n",
        "    parser.add_argument('--num_frames', type=int, default=80)\n",
        "    parser.add_argument('--mask_ratio', type=float, default=0.75)\n",
        "    parser.add_argument('--patience', type=int, default=3)\n",
        "    parser.add_argument('--resume_checkpoint', type=str, default=None)\n",
        "    parser.add_argument('--save_gifs_dir', type=str, default='gifs')\n",
        "    parser.add_argument('--num_gifs', type=int, default=10)\n",
        "    parser.add_argument('--alpha', type=float, default=0.9)\n",
        "    parser.add_argument('--segmentation_type', type=int, default=5, choices=range(0, 6))\n",
        "    parser.add_argument('--input_mask', action='store_true', # Added argument\n",
        "                       help='Mask input image using segmentation_type regions')\n",
        "    args, _ = parser.parse_known_args()\n",
        "\n",
        "    return args\n",
        "\n",
        "def main(args):\n",
        "    processor = Synapse3DProcessor(size=(80, 80))\n",
        "    vol_data_dict = {}\n",
        "\n",
        "    for bbox_name in args.bbox_name:\n",
        "        raw_vol, seg_vol, add_mask_vol = load_volumes(\n",
        "            bbox_name=bbox_name,\n",
        "            raw_base_dir=args.raw_base_dir,\n",
        "            seg_base_dir=args.seg_base_dir,\n",
        "            add_mask_base_dir=args.add_mask_base_dir\n",
        "        )\n",
        "        if raw_vol is not None:\n",
        "            vol_data_dict[bbox_name] = (raw_vol, seg_vol, add_mask_vol)\n",
        "\n",
        "    synapse_dfs = []\n",
        "    for bbox_name in args.bbox_name:\n",
        "        excel_path = os.path.join(args.excel_file, f\"{bbox_name}.xlsx\")\n",
        "        if os.path.exists(excel_path):\n",
        "            df = pd.read_excel(excel_path)\n",
        "            df['bbox_name'] = bbox_name\n",
        "            synapse_dfs.append(df)\n",
        "\n",
        "    syn_df = pd.concat(synapse_dfs, ignore_index=True)\n",
        "    dataset = SynapseDataset(\n",
        "        vol_data_dict=vol_data_dict,\n",
        "        synapse_df=syn_df,\n",
        "        processor=processor,\n",
        "        segmentation_type=args.segmentation_type,\n",
        "        subvol_size=args.subvol_size,\n",
        "        num_frames=args.num_frames,\n",
        "        alpha=args.alpha,\n",
        "        input_mask=args.input_mask  # Pass the flag\n",
        "    )\n",
        "\n",
        "    cubes = []\n",
        "    syn_info_list = []\n",
        "    for idx in range(len(dataset)):\n",
        "        pixel_values, syn_info, _ = dataset[idx]\n",
        "        cubes.append(pixel_values)\n",
        "        syn_info_list.append(syn_info)\n",
        "\n",
        "    print(f\"Processed {len(cubes)} cubes successfully.\")\n",
        "    return cubes, pd.DataFrame(syn_info_list)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "args = parse_args()\n",
        "\n",
        "args.input_mask=False\n",
        "cubes, sys_inf = main(args)\n",
        "print(f\"Final output: {len(cubes)} cubes\")\n",
        "\n",
        "# Extract the data tensor (cube) from the tuple returned by dataset[0]\n",
        "cube = cubes[0]\n",
        "\n",
        "mean = torch.tensor([0.485]).view(1, 1, 1, 1)\n",
        "std = torch.tensor([0.229]).view(1, 1, 1, 1)\n",
        "\n",
        "denormalized_cube = cube * std + mean\n",
        "denormalized_cube = torch.clamp(denormalized_cube, 0, 1)\n",
        "\n",
        "# Convert to RGB if needed\n",
        "frames = denormalized_cube.squeeze(1).numpy()  # Remove channel dimension\n",
        "frames = (frames * 255).astype(np.uint8)\n",
        "\n",
        "# Stack to create RGB\n",
        "frames = np.stack([frames, frames, frames], axis=-1)\n",
        "\n",
        "Gif_Name = f\"VGG_Gif_segmentation_type_{args.segmentation_type}_input_mask_{args.input_mask}\"\n",
        "# Gif_Name = f\"VGG_Gif_segmentation_type_{args.segmentation_type}_input_mask_True\"\n",
        "output_gif_path = os.path.join(\"\", f\"{Gif_Name}.gif\")\n",
        "# Save the frames as a gif using imageio\n",
        "imageio.mimsave(output_gif_path, frames, fps=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXYwmxeAZ0LS",
        "outputId": "2ac15bf3-6a6e-4fb4-f194-05261547b044"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 58 cubes successfully.\n",
            "Final output: 58 cubes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Version 2 - avg color overlay"
      ],
      "metadata": {
        "id": "Ym15nAitmrXw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xlFnsC9HmzBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Alpha Blending (Unmasked Areas):\n",
        "# Instead of blending the raw image with the mask and using a color, for unmasked regions we now blend the raw_rgb image with a black overlay. This keeps the masked regions unchanged but gives unmasked regions a 50% black overlay.\n",
        "# Unmasked Regions:\n",
        "# The raw image is modified so that the unmasked areas blend with 50% black (0.5 * raw_rgb + 0.5 * black_overlay), where black_overlay is a black (zeroed) array of the same shape as the raw_rgb.\n",
        "import os\n",
        "import glob\n",
        "import io\n",
        "import argparse\n",
        "import multiprocessing\n",
        "from typing import List, Tuple\n",
        "import imageio\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import imageio.v3 as iio\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision import transforms\n",
        "class Synapse3DProcessor:\n",
        "    def __init__(self, size=(80, 80), mean=(0.485,),  # Use a single channel for grayscale\n",
        "                 std=(0.229,)):\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize(size),\n",
        "            transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=mean, std=std),\n",
        "        ])\n",
        "\n",
        "    def __call__(self, frames, return_tensors=None):\n",
        "        processed_frames = [self.transform(frame) for frame in frames]\n",
        "        pixel_values = torch.stack(processed_frames)\n",
        "        if return_tensors == \"pt\":\n",
        "            return {\"pixel_values\": pixel_values}\n",
        "        else:\n",
        "            return pixel_values\n",
        "\n",
        "\n",
        "def load_volumes(bbox_name: str, raw_base_dir: str, seg_base_dir: str, add_mask_base_dir: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    raw_dir = os.path.join(raw_base_dir, bbox_name)\n",
        "    seg_dir = os.path.join(seg_base_dir, bbox_name)\n",
        "\n",
        "    if bbox_name.startswith(\"bbox\"):\n",
        "        bbox_num = bbox_name.replace(\"bbox\", \"\")\n",
        "        add_mask_dir = os.path.join(add_mask_base_dir, f\"bbox_{bbox_num}\")\n",
        "    else:\n",
        "        add_mask_dir = os.path.join(add_mask_base_dir, bbox_name)\n",
        "\n",
        "    raw_tif_files = sorted(glob.glob(os.path.join(raw_dir, 'slice_*.tif')))\n",
        "    seg_tif_files = sorted(glob.glob(os.path.join(seg_dir, 'slice_*.tif')))\n",
        "    add_mask_tif_files = sorted(glob.glob(os.path.join(add_mask_dir, 'slice_*.tif')))\n",
        "\n",
        "    if not (len(raw_tif_files) == len(seg_tif_files) == len(add_mask_tif_files)):\n",
        "        return None, None, None\n",
        "\n",
        "    try:\n",
        "        raw_vol = np.stack([iio.imread(f) for f in raw_tif_files], axis=0)\n",
        "        seg_vol = np.stack([iio.imread(f).astype(np.uint32) for f in seg_tif_files], axis=0)\n",
        "        add_mask_vol = np.stack([iio.imread(f).astype(np.uint32) for f in add_mask_tif_files], axis=0)\n",
        "        return raw_vol, seg_vol, add_mask_vol\n",
        "    except Exception as e:\n",
        "        return None, None, None\n",
        "\n",
        "def create_segmented_cube(\n",
        "    raw_vol: np.ndarray,\n",
        "    seg_vol: np.ndarray,\n",
        "    add_mask_vol: np.ndarray,\n",
        "    central_coord: Tuple[int, int, int],\n",
        "    side1_coord: Tuple[int, int, int],\n",
        "    side2_coord: Tuple[int, int, int],\n",
        "    segmentation_type: int,\n",
        "    subvolume_size: int = 80,\n",
        "    alpha: float = 0.3,\n",
        "    input_mask: bool = False,\n",
        ") -> np.ndarray:\n",
        "    def create_segment_masks(segmentation_volume, s1_coord, s2_coord):\n",
        "        x1, y1, z1 = s1_coord\n",
        "        x2, y2, z2 = s2_coord\n",
        "        seg_id_1 = segmentation_volume[z1, y1, x1]\n",
        "        seg_id_2 = segmentation_volume[z2, y2, x2]\n",
        "\n",
        "        mask_1 = (segmentation_volume == seg_id_1) if seg_id_1 != 0 else np.zeros_like(segmentation_volume, dtype=bool)\n",
        "        mask_2 = (segmentation_volume == seg_id_2) if seg_id_2 != 0 else np.zeros_like(segmentation_volume, dtype=bool)\n",
        "        return mask_1, mask_2\n",
        "\n",
        "    mask_1_full, mask_2_full = create_segment_masks(seg_vol, side1_coord, side2_coord)\n",
        "    mask_3_full = (add_mask_vol > 0)\n",
        "\n",
        "    combined_mask_full = np.logical_or(mask_1_full, np.logical_or(mask_2_full, mask_3_full))\n",
        "\n",
        "    half_size = subvolume_size // 2\n",
        "    cx, cy, cz = central_coord\n",
        "    x_start, x_end = max(cx - half_size, 0), min(cx + half_size, raw_vol.shape[2])\n",
        "    y_start, y_end = max(cy - half_size, 0), min(cy + half_size, raw_vol.shape[1])\n",
        "    z_start, z_end = max(cz - half_size, 0), min(cz + half_size, raw_vol.shape[0])\n",
        "\n",
        "    sub_raw = raw_vol[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "    sub_combined_mask = combined_mask_full[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "\n",
        "    pad_z = subvolume_size - sub_raw.shape[0]\n",
        "    pad_y = subvolume_size - sub_raw.shape[1]\n",
        "    pad_x = subvolume_size - sub_raw.shape[2]\n",
        "\n",
        "    if pad_z > 0 or pad_y > 0 or pad_x > 0:\n",
        "        sub_raw = np.pad(sub_raw, ((0, pad_z), (0, pad_y), (0, pad_x)), mode='constant', constant_values=0)\n",
        "        sub_combined_mask = np.pad(sub_combined_mask, ((0, pad_z), (0, pad_y), (0, pad_x)), mode='constant', constant_values=False)\n",
        "\n",
        "    sub_raw = sub_raw[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "    sub_combined_mask = sub_combined_mask[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "\n",
        "    overlaid_cube = np.zeros((subvolume_size, subvolume_size, 3, subvolume_size), dtype=np.uint8)\n",
        "\n",
        "    for z in range(subvolume_size):\n",
        "        raw_slice = sub_raw[z].astype(np.float32)\n",
        "        mn, mx = raw_slice.min(), raw_slice.max()\n",
        "        if mx > mn:\n",
        "            raw_slice = (raw_slice - mn) / (mx - mn)\n",
        "        else:\n",
        "            raw_slice = raw_slice - mn\n",
        "\n",
        "        raw_rgb = np.stack([raw_slice] * 3, axis=-1)\n",
        "\n",
        "        # Create 2D mask for unmasked regions\n",
        "        mask_factor = sub_combined_mask[z]  # This is now 2D\n",
        "\n",
        "        # Calculate average color of unmasked regions\n",
        "        unmasked_pixels = raw_slice[~mask_factor]  # Get unmasked pixel values\n",
        "        if len(unmasked_pixels) > 0:\n",
        "            avg_color = unmasked_pixels.mean()\n",
        "        else:\n",
        "            avg_color = 0  # In case there are no unmasked pixels\n",
        "\n",
        "        # Blend: apply transparency control (alpha_black)\n",
        "        raw_rgb = raw_rgb * mask_factor[:, :, np.newaxis] + (1 - mask_factor[:, :, np.newaxis]) * (alpha * avg_color + (1 - alpha) * raw_rgb)\n",
        "\n",
        "        overlaid_image = (np.clip(raw_rgb, 0, 1) * 255).astype(np.uint8)\n",
        "\n",
        "        overlaid_cube[:, :, :, z] = overlaid_image\n",
        "\n",
        "    return overlaid_cube\n",
        "\n",
        "\n",
        "class SynapseDataset(Dataset):\n",
        "    def __init__(self, vol_data_dict: dict, synapse_df: pd.DataFrame, processor,\n",
        "                 segmentation_type: int, subvol_size: int = 80, num_frames: int = 16,\n",
        "                 alpha: float = 0.3, input_mask: bool = False):  # Added input_mask\n",
        "        self.vol_data_dict = vol_data_dict\n",
        "        self.synapse_df = synapse_df.reset_index(drop=True)\n",
        "        self.processor = processor\n",
        "        self.segmentation_type = segmentation_type\n",
        "        self.subvol_size = subvol_size\n",
        "        self.num_frames = num_frames\n",
        "        self.alpha = alpha\n",
        "        self.input_mask = input_mask  # Store input_mask flag\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.synapse_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        syn_info = self.synapse_df.iloc[idx]\n",
        "        bbox_name = syn_info['bbox_name']\n",
        "        raw_vol, seg_vol, add_mask_vol = self.vol_data_dict.get(bbox_name, (None, None, None))\n",
        "\n",
        "        if raw_vol is None:\n",
        "            return torch.zeros((self.num_frames, 3, self.subvol_size, self.subvol_size), dtype=torch.float32), syn_info, bbox_name\n",
        "\n",
        "        central_coord = (int(syn_info['central_coord_1']), int(syn_info['central_coord_2']), int(syn_info['central_coord_3']))\n",
        "        side1_coord = (int(syn_info['side_1_coord_1']), int(syn_info['side_1_coord_2']), int(syn_info['side_1_coord_3']))\n",
        "        side2_coord = (int(syn_info['side_2_coord_1']), int(syn_info['side_2_coord_2']), int(syn_info['side_2_coord_3']))\n",
        "\n",
        "        overlaid_cube = create_segmented_cube(\n",
        "            raw_vol=raw_vol,\n",
        "            seg_vol=seg_vol,\n",
        "            add_mask_vol=add_mask_vol,\n",
        "            central_coord=central_coord,\n",
        "            side1_coord=side1_coord,\n",
        "            side2_coord=side2_coord,\n",
        "            segmentation_type=self.segmentation_type,\n",
        "            subvolume_size=self.subvol_size,\n",
        "            alpha=self.alpha,\n",
        "            input_mask=self.input_mask  # Pass the flag\n",
        "        )\n",
        "\n",
        "        frames = [overlaid_cube[..., z] for z in range(overlaid_cube.shape[3])]\n",
        "        if len(frames) < self.num_frames:\n",
        "            frames += [frames[-1]] * (self.num_frames - len(frames))\n",
        "        elif len(frames) > self.num_frames:\n",
        "            indices = np.linspace(0, len(frames)-1, self.num_frames, dtype=int)\n",
        "            frames = [frames[i] for i in indices]\n",
        "\n",
        "        inputs = self.processor(frames, return_tensors=\"pt\")\n",
        "        return inputs[\"pixel_values\"].squeeze(0).float(), syn_info, bbox_name\n",
        "# ,'bbox2','bbox3','bbox4','bbox5','bbox6',\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"VideoMAE Pre-training Script with Segmented Videos and Additional Masks\")\n",
        "    parser.add_argument('--raw_base_dir', type=str, default='raw')\n",
        "    parser.add_argument('--seg_base_dir', type=str, default='seg')\n",
        "    parser.add_argument('--add_mask_base_dir', type=str, default='')\n",
        "    parser.add_argument('--bbox_name', type=str, default=['bbox1' ], nargs='+')\n",
        "    parser.add_argument('--excel_file', type=str, default='')\n",
        "    parser.add_argument('--csv_output_dir', type=str, default='csv_outputs')\n",
        "    parser.add_argument('--checkpoint_dir', type=str, default='checkpoints')\n",
        "    parser.add_argument('--log_dir', type=str, default='logs')\n",
        "    parser.add_argument('--size', type=tuple, default=(80,80))\n",
        "    parser.add_argument('--batch_size', type=int, default=2)\n",
        "    parser.add_argument('--num_epochs', type=int, default=5)\n",
        "    parser.add_argument('--learning_rate', type=float, default=1e-4)\n",
        "    parser.add_argument('--weight_decay', type=float, default=1e-2)\n",
        "    parser.add_argument('--subvol_size', type=int, default=80)\n",
        "    parser.add_argument('--num_frames', type=int, default=80)\n",
        "    parser.add_argument('--mask_ratio', type=float, default=0.75)\n",
        "    parser.add_argument('--patience', type=int, default=3)\n",
        "    parser.add_argument('--resume_checkpoint', type=str, default=None)\n",
        "    parser.add_argument('--save_gifs_dir', type=str, default='gifs')\n",
        "    parser.add_argument('--num_gifs', type=int, default=10)\n",
        "    parser.add_argument('--alpha', type=float, default=0.9)\n",
        "    parser.add_argument('--segmentation_type', type=int, default=5, choices=range(0, 6))\n",
        "    parser.add_argument('--input_mask', action='store_true', # Added argument\n",
        "                       help='Mask input image using segmentation_type regions')\n",
        "    args, _ = parser.parse_known_args()\n",
        "\n",
        "    return args\n",
        "\n",
        "def main(args):\n",
        "    processor = Synapse3DProcessor(size=(80, 80))\n",
        "    vol_data_dict = {}\n",
        "\n",
        "    for bbox_name in args.bbox_name:\n",
        "        raw_vol, seg_vol, add_mask_vol = load_volumes(\n",
        "            bbox_name=bbox_name,\n",
        "            raw_base_dir=args.raw_base_dir,\n",
        "            seg_base_dir=args.seg_base_dir,\n",
        "            add_mask_base_dir=args.add_mask_base_dir\n",
        "        )\n",
        "        if raw_vol is not None:\n",
        "            vol_data_dict[bbox_name] = (raw_vol, seg_vol, add_mask_vol)\n",
        "\n",
        "    synapse_dfs = []\n",
        "    for bbox_name in args.bbox_name:\n",
        "        excel_path = os.path.join(args.excel_file, f\"{bbox_name}.xlsx\")\n",
        "        if os.path.exists(excel_path):\n",
        "            df = pd.read_excel(excel_path)\n",
        "            df['bbox_name'] = bbox_name\n",
        "            synapse_dfs.append(df)\n",
        "\n",
        "    syn_df = pd.concat(synapse_dfs, ignore_index=True)\n",
        "    dataset = SynapseDataset(\n",
        "        vol_data_dict=vol_data_dict,\n",
        "        synapse_df=syn_df,\n",
        "        processor=processor,\n",
        "        segmentation_type=args.segmentation_type,\n",
        "        subvol_size=args.subvol_size,\n",
        "        num_frames=args.num_frames,\n",
        "        alpha=args.alpha,\n",
        "        input_mask=args.input_mask  # Pass the flag\n",
        "    )\n",
        "\n",
        "    cubes = []\n",
        "    syn_info_list = []\n",
        "    for idx in range(len(dataset)):\n",
        "        pixel_values, syn_info, _ = dataset[idx]\n",
        "        cubes.append(pixel_values)\n",
        "        syn_info_list.append(syn_info)\n",
        "\n",
        "    print(f\"Processed {len(cubes)} cubes successfully.\")\n",
        "    return cubes, pd.DataFrame(syn_info_list)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "args = parse_args()\n",
        "\n",
        "args.input_mask=False\n",
        "cubes, sys_inf = main(args)\n",
        "print(f\"Final output: {len(cubes)} cubes\")\n",
        "\n",
        "# Extract the data tensor (cube) from the tuple returned by dataset[0]\n",
        "cube = cubes[0]\n",
        "\n",
        "mean = torch.tensor([0.485]).view(1, 1, 1, 1)\n",
        "std = torch.tensor([0.229]).view(1, 1, 1, 1)\n",
        "\n",
        "denormalized_cube = cube * std + mean\n",
        "denormalized_cube = torch.clamp(denormalized_cube, 0, 1)\n",
        "\n",
        "# Convert to RGB if needed\n",
        "frames = denormalized_cube.squeeze(1).numpy()  # Remove channel dimension\n",
        "frames = (frames * 255).astype(np.uint8)\n",
        "\n",
        "# Stack to create RGB\n",
        "frames = np.stack([frames, frames, frames], axis=-1)\n",
        "\n",
        "Gif_Name = f\"VGG_Gif_segmentation_type_{args.segmentation_type}_input_mask_{args.input_mask}\"\n",
        "# Gif_Name = f\"VGG_Gif_segmentation_type_{args.segmentation_type}_input_mask_True\"\n",
        "output_gif_path = os.path.join(\"\", f\"{Gif_Name}.gif\")\n",
        "# Save the frames as a gif using imageio\n",
        "imageio.mimsave(output_gif_path, frames, fps=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVYs4BPVm0SK",
        "outputId": "2735fcc7-864e-4bc7-9aaa-aa654eefb6d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 58 cubes successfully.\n",
            "Final output: 58 cubes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove input-mask and just adjust it using transparency"
      ],
      "metadata": {
        "id": "8koDg3Mcqf3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Alpha Blending (Unmasked Areas):\n",
        "# Instead of blending the raw image with the mask and using a color, for unmasked regions we now blend the raw_rgb image with a black overlay. This keeps the masked regions unchanged but gives unmasked regions a 50% black overlay.\n",
        "# Unmasked Regions:\n",
        "# The raw image is modified so that the unmasked areas blend with 50% black (0.5 * raw_rgb + 0.5 * black_overlay), where black_overlay is a black (zeroed) array of the same shape as the raw_rgb.\n",
        "import os\n",
        "import glob\n",
        "import io\n",
        "import argparse\n",
        "import multiprocessing\n",
        "from typing import List, Tuple\n",
        "import imageio\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import imageio.v3 as iio\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision import transforms\n",
        "class Synapse3DProcessor:\n",
        "    def __init__(self, size=(80, 80), mean=(0.485,),  # Use a single channel for grayscale\n",
        "                 std=(0.229,)):\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize(size),\n",
        "            transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=mean, std=std),\n",
        "        ])\n",
        "\n",
        "    def __call__(self, frames, return_tensors=None):\n",
        "        processed_frames = [self.transform(frame) for frame in frames]\n",
        "        pixel_values = torch.stack(processed_frames)\n",
        "        if return_tensors == \"pt\":\n",
        "            return {\"pixel_values\": pixel_values}\n",
        "        else:\n",
        "            return pixel_values\n",
        "\n",
        "\n",
        "def load_volumes(bbox_name: str, raw_base_dir: str, seg_base_dir: str, add_mask_base_dir: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    raw_dir = os.path.join(raw_base_dir, bbox_name)\n",
        "    seg_dir = os.path.join(seg_base_dir, bbox_name)\n",
        "\n",
        "    if bbox_name.startswith(\"bbox\"):\n",
        "        bbox_num = bbox_name.replace(\"bbox\", \"\")\n",
        "        add_mask_dir = os.path.join(add_mask_base_dir, f\"bbox_{bbox_num}\")\n",
        "    else:\n",
        "        add_mask_dir = os.path.join(add_mask_base_dir, bbox_name)\n",
        "\n",
        "    raw_tif_files = sorted(glob.glob(os.path.join(raw_dir, 'slice_*.tif')))\n",
        "    seg_tif_files = sorted(glob.glob(os.path.join(seg_dir, 'slice_*.tif')))\n",
        "    add_mask_tif_files = sorted(glob.glob(os.path.join(add_mask_dir, 'slice_*.tif')))\n",
        "\n",
        "    if not (len(raw_tif_files) == len(seg_tif_files) == len(add_mask_tif_files)):\n",
        "        return None, None, None\n",
        "\n",
        "    try:\n",
        "        raw_vol = np.stack([iio.imread(f) for f in raw_tif_files], axis=0)\n",
        "        seg_vol = np.stack([iio.imread(f).astype(np.uint32) for f in seg_tif_files], axis=0)\n",
        "        add_mask_vol = np.stack([iio.imread(f).astype(np.uint32) for f in add_mask_tif_files], axis=0)\n",
        "        return raw_vol, seg_vol, add_mask_vol\n",
        "    except Exception as e:\n",
        "        return None, None, None\n",
        "\n",
        "def create_segmented_cube(\n",
        "    raw_vol: np.ndarray,\n",
        "    seg_vol: np.ndarray,\n",
        "    add_mask_vol: np.ndarray,\n",
        "    central_coord: Tuple[int, int, int],\n",
        "    side1_coord: Tuple[int, int, int],\n",
        "    side2_coord: Tuple[int, int, int],\n",
        "    segmentation_type: int,\n",
        "    subvolume_size: int = 80,\n",
        "    alpha: float = 0.3,\n",
        ") -> np.ndarray:\n",
        "    def create_segment_masks(segmentation_volume, s1_coord, s2_coord):\n",
        "        x1, y1, z1 = s1_coord\n",
        "        x2, y2, z2 = s2_coord\n",
        "        seg_id_1 = segmentation_volume[z1, y1, x1]\n",
        "        seg_id_2 = segmentation_volume[z2, y2, x2]\n",
        "\n",
        "        mask_1 = (segmentation_volume == seg_id_1) if seg_id_1 != 0 else np.zeros_like(segmentation_volume, dtype=bool)\n",
        "        mask_2 = (segmentation_volume == seg_id_2) if seg_id_2 != 0 else np.zeros_like(segmentation_volume, dtype=bool)\n",
        "        return mask_1, mask_2\n",
        "\n",
        "    mask_1_full, mask_2_full = create_segment_masks(seg_vol, side1_coord, side2_coord)\n",
        "    mask_3_full = (add_mask_vol > 0)\n",
        "\n",
        "    combined_mask_full = np.logical_or(mask_1_full, np.logical_or(mask_2_full, mask_3_full))\n",
        "\n",
        "    half_size = subvolume_size // 2\n",
        "    cx, cy, cz = central_coord\n",
        "    x_start, x_end = max(cx - half_size, 0), min(cx + half_size, raw_vol.shape[2])\n",
        "    y_start, y_end = max(cy - half_size, 0), min(cy + half_size, raw_vol.shape[1])\n",
        "    z_start, z_end = max(cz - half_size, 0), min(cz + half_size, raw_vol.shape[0])\n",
        "\n",
        "    sub_raw = raw_vol[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "    sub_combined_mask = combined_mask_full[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "\n",
        "    pad_z = subvolume_size - sub_raw.shape[0]\n",
        "    pad_y = subvolume_size - sub_raw.shape[1]\n",
        "    pad_x = subvolume_size - sub_raw.shape[2]\n",
        "\n",
        "    if pad_z > 0 or pad_y > 0 or pad_x > 0:\n",
        "        sub_raw = np.pad(sub_raw, ((0, pad_z), (0, pad_y), (0, pad_x)), mode='constant', constant_values=0)\n",
        "        sub_combined_mask = np.pad(sub_combined_mask, ((0, pad_z), (0, pad_y), (0, pad_x)), mode='constant', constant_values=False)\n",
        "\n",
        "    sub_raw = sub_raw[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "    sub_combined_mask = sub_combined_mask[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "\n",
        "    overlaid_cube = np.zeros((subvolume_size, subvolume_size, 3, subvolume_size), dtype=np.uint8)\n",
        "\n",
        "    for z in range(subvolume_size):\n",
        "        raw_slice = sub_raw[z].astype(np.float32)\n",
        "        mn, mx = raw_slice.min(), raw_slice.max()\n",
        "        if mx > mn:\n",
        "            raw_slice = (raw_slice - mn) / (mx - mn)\n",
        "        else:\n",
        "            raw_slice = raw_slice - mn\n",
        "\n",
        "        raw_rgb = np.stack([raw_slice] * 3, axis=-1)\n",
        "\n",
        "        # Create 2D mask for unmasked regions\n",
        "        mask_factor = sub_combined_mask[z]  # This is now 2D\n",
        "\n",
        "        # Calculate average color of unmasked regions\n",
        "        unmasked_pixels = raw_slice[~mask_factor]  # Get unmasked pixel values\n",
        "        if len(unmasked_pixels) > 0:\n",
        "            avg_color = unmasked_pixels.mean()\n",
        "        else:\n",
        "            avg_color = 0  # In case there are no unmasked pixels\n",
        "\n",
        "        # Blend: apply transparency control (alpha_black)\n",
        "        raw_rgb = raw_rgb * mask_factor[:, :, np.newaxis] + (1 - mask_factor[:, :, np.newaxis]) * (alpha * avg_color + (1 - alpha) * raw_rgb)\n",
        "\n",
        "        overlaid_image = (np.clip(raw_rgb, 0, 1) * 255).astype(np.uint8)\n",
        "\n",
        "        overlaid_cube[:, :, :, z] = overlaid_image\n",
        "\n",
        "    return overlaid_cube\n",
        "\n",
        "\n",
        "class SynapseDataset(Dataset):\n",
        "    def __init__(self, vol_data_dict: dict, synapse_df: pd.DataFrame, processor,\n",
        "                 segmentation_type: int, subvol_size: int = 80, num_frames: int = 16,\n",
        "                 alpha: float = 0.3):\n",
        "        self.vol_data_dict = vol_data_dict\n",
        "        self.synapse_df = synapse_df.reset_index(drop=True)\n",
        "        self.processor = processor\n",
        "        self.segmentation_type = segmentation_type\n",
        "        self.subvol_size = subvol_size\n",
        "        self.num_frames = num_frames\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.synapse_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        syn_info = self.synapse_df.iloc[idx]\n",
        "        bbox_name = syn_info['bbox_name']\n",
        "        raw_vol, seg_vol, add_mask_vol = self.vol_data_dict.get(bbox_name, (None, None, None))\n",
        "\n",
        "        if raw_vol is None:\n",
        "            return torch.zeros((self.num_frames, 3, self.subvol_size, self.subvol_size), dtype=torch.float32), syn_info, bbox_name\n",
        "\n",
        "        central_coord = (int(syn_info['central_coord_1']), int(syn_info['central_coord_2']), int(syn_info['central_coord_3']))\n",
        "        side1_coord = (int(syn_info['side_1_coord_1']), int(syn_info['side_1_coord_2']), int(syn_info['side_1_coord_3']))\n",
        "        side2_coord = (int(syn_info['side_2_coord_1']), int(syn_info['side_2_coord_2']), int(syn_info['side_2_coord_3']))\n",
        "\n",
        "        overlaid_cube = create_segmented_cube(\n",
        "            raw_vol=raw_vol,\n",
        "            seg_vol=seg_vol,\n",
        "            add_mask_vol=add_mask_vol,\n",
        "            central_coord=central_coord,\n",
        "            side1_coord=side1_coord,\n",
        "            side2_coord=side2_coord,\n",
        "            segmentation_type=self.segmentation_type,\n",
        "            subvolume_size=self.subvol_size,\n",
        "            alpha=self.alpha,\n",
        "        )\n",
        "\n",
        "        frames = [overlaid_cube[..., z] for z in range(overlaid_cube.shape[3])]\n",
        "        if len(frames) < self.num_frames:\n",
        "            frames += [frames[-1]] * (self.num_frames - len(frames))\n",
        "        elif len(frames) > self.num_frames:\n",
        "            indices = np.linspace(0, len(frames)-1, self.num_frames, dtype=int)\n",
        "            frames = [frames[i] for i in indices]\n",
        "\n",
        "        inputs = self.processor(frames, return_tensors=\"pt\")\n",
        "        return inputs[\"pixel_values\"].squeeze(0).float(), syn_info, bbox_name\n",
        "# ,'bbox2','bbox3','bbox4','bbox5','bbox6',\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"VideoMAE Pre-training Script with Segmented Videos and Additional Masks\")\n",
        "    parser.add_argument('--raw_base_dir', type=str, default='raw')\n",
        "    parser.add_argument('--seg_base_dir', type=str, default='seg')\n",
        "    parser.add_argument('--add_mask_base_dir', type=str, default='')\n",
        "    parser.add_argument('--bbox_name', type=str, default=['bbox1' ], nargs='+')\n",
        "    parser.add_argument('--excel_file', type=str, default='')\n",
        "    parser.add_argument('--csv_output_dir', type=str, default='csv_outputs')\n",
        "    parser.add_argument('--size', type=tuple, default=(80,80))\n",
        "    parser.add_argument('--subvol_size', type=int, default=80)\n",
        "    parser.add_argument('--num_frames', type=int, default=80)\n",
        "    parser.add_argument('--save_gifs_dir', type=str, default='gifs')\n",
        "    parser.add_argument('--alpha', type=float, default=0.5)\n",
        "    parser.add_argument('--segmentation_type', type=int, default=5, choices=range(0, 6))\n",
        "    args, _ = parser.parse_known_args()\n",
        "\n",
        "    return args\n",
        "\n",
        "def main(args):\n",
        "    processor = Synapse3DProcessor(size=(80, 80))\n",
        "    vol_data_dict = {}\n",
        "\n",
        "    for bbox_name in args.bbox_name:\n",
        "        raw_vol, seg_vol, add_mask_vol = load_volumes(\n",
        "            bbox_name=bbox_name,\n",
        "            raw_base_dir=args.raw_base_dir,\n",
        "            seg_base_dir=args.seg_base_dir,\n",
        "            add_mask_base_dir=args.add_mask_base_dir\n",
        "        )\n",
        "        if raw_vol is not None:\n",
        "            vol_data_dict[bbox_name] = (raw_vol, seg_vol, add_mask_vol)\n",
        "\n",
        "    synapse_dfs = []\n",
        "    for bbox_name in args.bbox_name:\n",
        "        excel_path = os.path.join(args.excel_file, f\"{bbox_name}.xlsx\")\n",
        "        if os.path.exists(excel_path):\n",
        "            df = pd.read_excel(excel_path)\n",
        "            df['bbox_name'] = bbox_name\n",
        "            synapse_dfs.append(df)\n",
        "\n",
        "    syn_df = pd.concat(synapse_dfs, ignore_index=True)\n",
        "    dataset = SynapseDataset(\n",
        "        vol_data_dict=vol_data_dict,\n",
        "        synapse_df=syn_df,\n",
        "        processor=processor,\n",
        "        segmentation_type=args.segmentation_type,\n",
        "        subvol_size=args.subvol_size,\n",
        "        num_frames=args.num_frames,\n",
        "        alpha=args.alpha,\n",
        "    )\n",
        "\n",
        "    cubes = []\n",
        "    syn_info_list = []\n",
        "    for idx in range(len(dataset)):\n",
        "        pixel_values, syn_info, _ = dataset[idx]\n",
        "        cubes.append(pixel_values)\n",
        "        syn_info_list.append(syn_info)\n",
        "\n",
        "    print(f\"Processed {len(cubes)} cubes successfully.\")\n",
        "    return cubes, pd.DataFrame(syn_info_list)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "args = parse_args()\n",
        "\n",
        "cubes, sys_inf = main(args)\n",
        "print(f\"Final output: {len(cubes)} cubes\")\n",
        "\n",
        "# Extract the data tensor (cube) from the tuple returned by dataset[0]\n",
        "cube = cubes[0]\n",
        "\n",
        "mean = torch.tensor([0.485]).view(1, 1, 1, 1)\n",
        "std = torch.tensor([0.229]).view(1, 1, 1, 1)\n",
        "\n",
        "denormalized_cube = cube * std + mean\n",
        "denormalized_cube = torch.clamp(denormalized_cube, 0, 1)\n",
        "\n",
        "# Convert to RGB if needed\n",
        "frames = denormalized_cube.squeeze(1).numpy()\n",
        "frames = (frames * 255).astype(np.uint8)\n",
        "\n",
        "# Stack to create RGB\n",
        "frames = np.stack([frames, frames, frames], axis=-1)\n",
        "\n",
        "Gif_Name = f\"VGG_Gif_segmentation_type_{args.segmentation_type}_alpha_{args.alpha}\"\n",
        "output_gif_path = os.path.join(\"\", f\"{Gif_Name}.gif\")\n",
        "# Save the frames as a gif using imageio\n",
        "imageio.mimsave(output_gif_path, frames, fps=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "853682Peo93j",
        "outputId": "9baaf277-6433-4393-e226-fefb7cee007f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 58 cubes successfully.\n",
            "Final output: 58 cubes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Seperate Additinal masks and turn them to options for loader"
      ],
      "metadata": {
        "id": "RyvoGKfQqug0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def analyze_mask_types(add_mask_vol: np.ndarray):\n",
        "    \"\"\"\n",
        "    Analyzes the unique mask types based on color values in the add_mask_vol.\n",
        "    \"\"\"\n",
        "    # Get the unique values representing different mask types\n",
        "    unique_mask_values = np.unique(add_mask_vol)\n",
        "\n",
        "    # Remove zero (which could represent background or unmasked regions)\n",
        "    unique_mask_values = unique_mask_values[unique_mask_values > 0]\n",
        "\n",
        "    # Dictionary to store the mask type and its occurrence count\n",
        "    mask_analysis = {}\n",
        "\n",
        "    for value in unique_mask_values:\n",
        "        # Create a binary mask for each unique value\n",
        "        binary_mask = (add_mask_vol == value)\n",
        "\n",
        "        # Count the number of pixels in the mask (non-zero regions)\n",
        "        mask_size = np.sum(binary_mask)\n",
        "\n",
        "        mask_analysis[value] = {\n",
        "            'mask_size': mask_size,\n",
        "            'percentage_of_total': (mask_size / add_mask_vol.size) * 100\n",
        "        }\n",
        "\n",
        "    # Print the analysis result\n",
        "    for value, analysis in mask_analysis.items():\n",
        "        print(f\"Mask Type (Color Value {value}):\")\n",
        "        print(f\"  Size: {analysis['mask_size']} pixels\")\n",
        "        print(f\"  Percentage of Total Volume: {analysis['percentage_of_total']:.2f}%\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    # Optional: Visualize the mask types\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    for idx, value in enumerate(mask_analysis.keys(), 1):\n",
        "        plt.subplot(1, len(mask_analysis), idx)\n",
        "        plt.imshow(add_mask_vol[0] == value, cmap='gray')  # Visualize the first slice of the mask\n",
        "        plt.title(f\"Mask {value}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    return mask_analysis\n",
        "\n",
        "# Example usage (assuming `add_mask_vol` is already loaded from `load_volumes`):\n",
        "raw_vol, seg_vol, add_mask_vol = load_volumes(\n",
        "    bbox_name='bbox1',  # Example bbox_name\n",
        "    raw_base_dir='raw',\n",
        "    seg_base_dir='seg',\n",
        "    add_mask_base_dir=''\n",
        ")\n",
        "\n",
        "if add_mask_vol is not None:\n",
        "    mask_analysis = analyze_mask_types(add_mask_vol)\n",
        "else:\n",
        "    print(\"No additional mask volume found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "apFFNCflrxTS",
        "outputId": "11752beb-8df2-4aa0-e260-5236e978bda1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mask Type (Color Value 5):\n",
            "  Size: 7106332 pixels\n",
            "  Percentage of Total Volume: 3.74%\n",
            "--------------------------------------------------\n",
            "Mask Type (Color Value 6):\n",
            "  Size: 837270 pixels\n",
            "  Percentage of Total Volume: 0.44%\n",
            "--------------------------------------------------\n",
            "Mask Type (Color Value 7):\n",
            "  Size: 133793 pixels\n",
            "  Percentage of Total Volume: 0.07%\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loader Options\n",
        "\n",
        " This is a structured overview of the available arguments for the synapse dataset data loader.\n",
        "\n",
        "## Arguments\n",
        "\n",
        "### `--raw_base_dir` (Required)\n",
        "- **Description**: Directory containing the raw data files (e.g., `.tif` slices).\n",
        "- **Type**: `str`\n",
        "- **Default**: `'raw'`\n",
        "- **Example**:\n",
        "    ```\n",
        "    --raw_base_dir /path/to/raw/data\n",
        "    ```\n",
        "\n",
        "### `--seg_base_dir` (Required)\n",
        "- **Description**: Directory containing the segmentation data files (e.g., `.tif` slices).\n",
        "- **Type**: `str`\n",
        "- **Default**: `'seg'`\n",
        "- **Example**:\n",
        "    ```\n",
        "    --seg_base_dir /path/to/segmentation/data\n",
        "    ```\n",
        "\n",
        "### `--add_mask_base_dir` (Optional)\n",
        "- **Description**: Directory containing additional mask files (e.g., `.tif` slices).\n",
        "- **Type**: `str`\n",
        "- **Default**: `''` (empty string, optional)\n",
        "- **Example**:\n",
        "    ```\n",
        "    --add_mask_base_dir /path/to/additional/masks\n",
        "    ```\n",
        "\n",
        "### `--bbox_name` (Required)\n",
        "- **Description**: List of bounding box names to process. Each bounding box corresponds to a set of data (raw, segmentation, and mask files).\n",
        "- **Type**: `list[str]`\n",
        "- **Default**: `['bbox1']`\n",
        "- **Example**:\n",
        "    ```\n",
        "    --bbox_name bbox1 bbox2 bbox3\n",
        "    ```\n",
        "\n",
        "### `--excel_file` (Required)\n",
        "- **Description**: Path to the directory containing Excel files with synapse information. The data from these Excel files will be used for synapse annotations.\n",
        "- **Type**: `str`\n",
        "- **Default**: `''` (empty string, required path to a directory)\n",
        "- **Example**:\n",
        "    ```\n",
        "    --excel_file /path/to/excel/files\n",
        "    ```\n",
        "\n",
        "### `--csv_output_dir` (Optional)\n",
        "- **Description**: Directory to save CSV outputs, such as processed data summaries.\n",
        "- **Type**: `str`\n",
        "- **Default**: `'csv_outputs'`\n",
        "- **Example**:\n",
        "    ```\n",
        "    --csv_output_dir /path/to/csv/outputs\n",
        "    ```\n",
        "\n",
        "### `--size` (Optional)\n",
        "- **Description**: Target size for the frames. This will resize the frames to this size before processing.\n",
        "- **Type**: `tuple[int, int]`\n",
        "- **Default**: `(80, 80)`\n",
        "- **Example**:\n",
        "    ```\n",
        "    --size 128 128\n",
        "    ```\n",
        "\n",
        "### `--subvol_size` (Optional)\n",
        "- **Description**: Subvolume size for extracting regions from the full volume. This size determines the 3D crop of the data.\n",
        "- **Type**: `int`\n",
        "- **Default**: `80`\n",
        "- **Example**:\n",
        "    ```\n",
        "    --subvol_size 128\n",
        "    ```\n",
        "\n",
        "### `--num_frames` (Optional)\n",
        "- **Description**: Number of frames to extract from the data.\n",
        "- **Type**: `int`\n",
        "- **Default**: `80`\n",
        "- **Example**:\n",
        "    ```\n",
        "    --num_frames 16\n",
        "    ```\n",
        "\n",
        "### `--save_gifs_dir` (Optional)\n",
        "- **Description**: Directory to save generated GIFs for each segmentation type.\n",
        "- **Type**: `str`\n",
        "- **Default**: `'gifs'`\n",
        "- **Example**:\n",
        "    ```\n",
        "    --save_gifs_dir /path/to/save/gifs\n",
        "    ```\n",
        "\n",
        "### `--alpha` (Optional)\n",
        "- **Description**: Alpha blending factor for combining raw image and mask. This controls how much the unmasked areas are blended with a black overlay.\n",
        "- **Type**: `float`\n",
        "- **Default**: `0.5`\n",
        "- **Example**:\n",
        "    ```\n",
        "    --alpha 0.7\n",
        "    ```\n",
        "\n",
        "### `--segmentation_type` (Required)\n",
        "- **Description**: Type of segmentation overlay to apply to the raw data. This option defines which mask type is used for overlaying the raw data.\n",
        "- **Type**: `int`\n",
        "- **Choices**:\n",
        "    - `0 = Raw image` – No overlay, raw image only.\n",
        "    - `1 = Raw + Side1` – Raw image overlaid with Side1.\n",
        "    - `2 = Raw + Side2` – Raw image overlaid with Side2.\n",
        "    - `3 = Raw + Side1 + Side2` – Raw image overlaid with both Side1 and Side2.\n",
        "    - `4 = Raw + Vesicles` – Raw image overlaid with vesicle regions.\n",
        "    - `5 = Raw + Side1 + Side2 + Vesicles` – Raw image overlaid with Side1, Side2, and vesicles.\n",
        "    - `6 = Density` – Density regions only.\n",
        "    - `7 = Cleft` – Cleft regions only.\n",
        "    - `8 = Mitochondria` – Mitochondria regions only.\n",
        "    - `9 = Cleft + Density` – Cleft + Density regions combined.\n",
        "- **Default**: `6`\n",
        "- **Example**:\n",
        "    ```\n",
        "    --segmentation_type 3\n",
        "    ```\n",
        "\n",
        "## Example Usage\n",
        "\n",
        "Below is an example of how to use the data loader script with specified options:\n",
        "\n",
        "```bash\n",
        "python data_loader.py \\\n",
        "    --raw_base_dir /path/to/raw/data \\\n",
        "    --seg_base_dir /path/to/segmentation/data \\\n",
        "    --add_mask_base_dir /path/to/additional/masks \\\n",
        "    --bbox_name bbox1 bbox2 \\\n",
        "    --excel_file /path/to/excel/files \\\n",
        "    --csv_output_dir /path/to/csv/outputs \\\n",
        "    --save_gifs_dir /path/to/save/gifs \\\n",
        "    --segmentation_type 2 \\\n",
        "    --alpha 0.5\n"
      ],
      "metadata": {
        "id": "5-iwYc6iryNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import io\n",
        "import argparse\n",
        "import multiprocessing\n",
        "from typing import List, Tuple\n",
        "import imageio\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import imageio.v3 as iio\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision import transforms\n",
        "class Synapse3DProcessor:\n",
        "    def __init__(self, size=(80, 80), mean=(0.485,),  # Use a single channel for grayscale\n",
        "                 std=(0.229,)):\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize(size),\n",
        "            transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=mean, std=std),\n",
        "        ])\n",
        "\n",
        "    def __call__(self, frames, return_tensors=None):\n",
        "        processed_frames = [self.transform(frame) for frame in frames]\n",
        "        pixel_values = torch.stack(processed_frames)\n",
        "        if return_tensors == \"pt\":\n",
        "            return {\"pixel_values\": pixel_values}\n",
        "        else:\n",
        "            return pixel_values\n",
        "\n",
        "\n",
        "def load_volumes(bbox_name: str, raw_base_dir: str, seg_base_dir: str, add_mask_base_dir: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    raw_dir = os.path.join(raw_base_dir, bbox_name)\n",
        "    seg_dir = os.path.join(seg_base_dir, bbox_name)\n",
        "\n",
        "    if bbox_name.startswith(\"bbox\"):\n",
        "        bbox_num = bbox_name.replace(\"bbox\", \"\")\n",
        "        add_mask_dir = os.path.join(add_mask_base_dir, f\"bbox_{bbox_num}\")\n",
        "    else:\n",
        "        add_mask_dir = os.path.join(add_mask_base_dir, bbox_name)\n",
        "\n",
        "    raw_tif_files = sorted(glob.glob(os.path.join(raw_dir, 'slice_*.tif')))\n",
        "    seg_tif_files = sorted(glob.glob(os.path.join(seg_dir, 'slice_*.tif')))\n",
        "    add_mask_tif_files = sorted(glob.glob(os.path.join(add_mask_dir, 'slice_*.tif')))\n",
        "\n",
        "    if not (len(raw_tif_files) == len(seg_tif_files) == len(add_mask_tif_files)):\n",
        "        return None, None, None\n",
        "\n",
        "    try:\n",
        "        raw_vol = np.stack([iio.imread(f) for f in raw_tif_files], axis=0)\n",
        "        seg_vol = np.stack([iio.imread(f).astype(np.uint32) for f in seg_tif_files], axis=0)\n",
        "        add_mask_vol = np.stack([iio.imread(f).astype(np.uint32) for f in add_mask_tif_files], axis=0)\n",
        "        return raw_vol, seg_vol, add_mask_vol\n",
        "    except Exception as e:\n",
        "        return None, None, None\n",
        "\n",
        "class SynapseDataset(Dataset):\n",
        "    def __init__(self, vol_data_dict: dict, synapse_df: pd.DataFrame, processor,\n",
        "                 segmentation_type: int, subvol_size: int = 80, num_frames: int = 16,\n",
        "                 alpha: float = 0.3):\n",
        "        self.vol_data_dict = vol_data_dict\n",
        "        self.synapse_df = synapse_df.reset_index(drop=True)\n",
        "        self.processor = processor\n",
        "        self.segmentation_type = segmentation_type\n",
        "        self.subvol_size = subvol_size\n",
        "        self.num_frames = num_frames\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.synapse_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        syn_info = self.synapse_df.iloc[idx]\n",
        "        bbox_name = syn_info['bbox_name']\n",
        "        raw_vol, seg_vol, add_mask_vol = self.vol_data_dict.get(bbox_name, (None, None, None))\n",
        "\n",
        "        if raw_vol is None:\n",
        "            return torch.zeros((self.num_frames, 3, self.subvol_size, self.subvol_size), dtype=torch.float32), syn_info, bbox_name\n",
        "\n",
        "        central_coord = (int(syn_info['central_coord_1']), int(syn_info['central_coord_2']), int(syn_info['central_coord_3']))\n",
        "        side1_coord = (int(syn_info['side_1_coord_1']), int(syn_info['side_1_coord_2']), int(syn_info['side_1_coord_3']))\n",
        "        side2_coord = (int(syn_info['side_2_coord_1']), int(syn_info['side_2_coord_2']), int(syn_info['side_2_coord_3']))\n",
        "\n",
        "        overlaid_cube = create_segmented_cube(\n",
        "            raw_vol=raw_vol,\n",
        "            seg_vol=seg_vol,\n",
        "            add_mask_vol=add_mask_vol,\n",
        "            central_coord=central_coord,\n",
        "            side1_coord=side1_coord,\n",
        "            side2_coord=side2_coord,\n",
        "            segmentation_type=self.segmentation_type,\n",
        "            subvolume_size=self.subvol_size,\n",
        "            alpha=self.alpha,\n",
        "        )\n",
        "\n",
        "        frames = [overlaid_cube[..., z] for z in range(overlaid_cube.shape[3])]\n",
        "        if len(frames) < self.num_frames:\n",
        "            frames += [frames[-1]] * (self.num_frames - len(frames))\n",
        "        elif len(frames) > self.num_frames:\n",
        "            indices = np.linspace(0, len(frames)-1, self.num_frames, dtype=int)\n",
        "            frames = [frames[i] for i in indices]\n",
        "\n",
        "        inputs = self.processor(frames, return_tensors=\"pt\")\n",
        "        return inputs[\"pixel_values\"].squeeze(0).float(), syn_info, bbox_name\n",
        "# ,'bbox2','bbox3','bbox4','bbox5','bbox6',\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"Synapse Dataset\")\n",
        "    parser.add_argument('--raw_base_dir', type=str, default='raw')\n",
        "    parser.add_argument('--seg_base_dir', type=str, default='seg')\n",
        "    parser.add_argument('--add_mask_base_dir', type=str, default='')\n",
        "    parser.add_argument('--bbox_name', type=str, default=['bbox1'], nargs='+')\n",
        "    parser.add_argument('--excel_file', type=str, default='')\n",
        "    parser.add_argument('--csv_output_dir', type=str, default='csv_outputs')\n",
        "    parser.add_argument('--size', type=tuple, default=(80, 80))\n",
        "    parser.add_argument('--subvol_size', type=int, default=80)\n",
        "    parser.add_argument('--num_frames', type=int, default=80)\n",
        "    parser.add_argument('--save_gifs_dir', type=str, default='gifs')\n",
        "    parser.add_argument('--alpha', type=float, default=0.5)\n",
        "    parser.add_argument('--segmentation_type', type=int, default=6, choices=range(0, 10),\n",
        "                        help='Type of segmentation overlay:\\n'\n",
        "                             '0 = Raw image\\n'\n",
        "                             '1 = Raw + Side1\\n'\n",
        "                             '2 = Raw + Side2\\n'\n",
        "                             '3 = Raw + Side1 + Side2\\n'\n",
        "                             '4 = Raw + Vesicles\\n'\n",
        "                             '5 = Raw + Side1 + Side2 + Vesicles\\n'\n",
        "                             '6 = vesicle cloud\\n'\n",
        "                             '7 = cleft\\n'\n",
        "                             '8 = Mitochondria\\n'\n",
        "                             '9 = Cleft + Density')\n",
        "    args, _ = parser.parse_known_args()\n",
        "    return args\n",
        "\n",
        "def create_segmented_cube(\n",
        "    raw_vol: np.ndarray,\n",
        "    seg_vol: np.ndarray,\n",
        "    add_mask_vol: np.ndarray,\n",
        "    central_coord: Tuple[int, int, int],\n",
        "    side1_coord: Tuple[int, int, int],\n",
        "    side2_coord: Tuple[int, int, int],\n",
        "    segmentation_type: int,\n",
        "    subvolume_size: int = 80,\n",
        "    alpha: float = 0.3,\n",
        ") -> np.ndarray:\n",
        "    def create_segment_masks(segmentation_volume, s1_coord, s2_coord):\n",
        "        x1, y1, z1 = s1_coord\n",
        "        x2, y2, z2 = s2_coord\n",
        "        seg_id_1 = segmentation_volume[z1, y1, x1]\n",
        "        seg_id_2 = segmentation_volume[z2, y2, x2]\n",
        "        # print(seg_id_2)\n",
        "        mask_2 = (segmentation_volume == seg_id_2) if seg_id_2 != 0 else np.zeros_like(segmentation_volume, dtype=bool)\n",
        "\n",
        "        mask_1 = (segmentation_volume == seg_id_1) if seg_id_1 != 0 else np.zeros_like(segmentation_volume, dtype=bool)\n",
        "        return mask_1, mask_2\n",
        "\n",
        "    mask_1_full, mask_2_full = create_segment_masks(seg_vol, side1_coord, side2_coord)\n",
        "    mask_3_full = (add_mask_vol > 0)\n",
        "\n",
        "    # New segmentation type handling\n",
        "    if segmentation_type == 0:\n",
        "        combined_mask_full = np.ones_like(add_mask_vol, dtype=bool)  # Raw image (no masking)\n",
        "    elif segmentation_type == 1:\n",
        "        combined_mask_full = mask_1_full  # Raw + Side1\n",
        "    elif segmentation_type == 2:\n",
        "        combined_mask_full = mask_2_full  # Raw + Side2\n",
        "    elif segmentation_type == 3:\n",
        "        combined_mask_full = np.logical_or(mask_1_full, mask_2_full)  # Raw + Side1 + Side2\n",
        "    elif segmentation_type == 4:\n",
        "        combined_mask_full = ( mask_3_full)  # Raw + Vesicles\n",
        "    elif segmentation_type == 5:\n",
        "        combined_mask_full = np.logical_or(mask_1_full, np.logical_or(mask_2_full, mask_3_full))  # All masks\n",
        "    elif segmentation_type == 6:\n",
        "        combined_mask_full = (add_mask_vol == 6)  # vesicle cloud\n",
        "    elif segmentation_type == 7:\n",
        "        combined_mask_full = (add_mask_vol == 7)  # cleft\n",
        "    elif segmentation_type == 8:\n",
        "        combined_mask_full = (add_mask_vol == 5)  # Mitochondria\n",
        "    elif segmentation_type == 9:\n",
        "        combined_mask_full = np.logical_or(add_mask_vol == 6, add_mask_vol == 7)  # Cleft + Density\n",
        "\n",
        "    half_size = subvolume_size // 2\n",
        "    cx, cy, cz = central_coord\n",
        "    x_start, x_end = max(cx - half_size, 0), min(cx + half_size, raw_vol.shape[2])\n",
        "    y_start, y_end = max(cy - half_size, 0), min(cy + half_size, raw_vol.shape[1])\n",
        "    z_start, z_end = max(cz - half_size, 0), min(cz + half_size, raw_vol.shape[0])\n",
        "    sub_raw = raw_vol[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "    sub_combined_mask = combined_mask_full[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "    pad_z = subvolume_size - sub_raw.shape[0]\n",
        "    pad_y = subvolume_size - sub_raw.shape[1]\n",
        "    pad_x = subvolume_size - sub_raw.shape[2]\n",
        "    if pad_z > 0 or pad_y > 0 or pad_x > 0:\n",
        "        sub_raw = np.pad(sub_raw, ((0, pad_z), (0, pad_y), (0, pad_x)), mode='constant', constant_values=0)\n",
        "        sub_combined_mask = np.pad(sub_combined_mask, ((0, pad_z), (0, pad_y), (0, pad_x)), mode='constant', constant_values=False)\n",
        "    sub_raw = sub_raw[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "    sub_combined_mask = sub_combined_mask[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "    overlaid_cube = np.zeros((subvolume_size, subvolume_size, 3, subvolume_size), dtype=np.uint8)\n",
        "    for z in range(subvolume_size):\n",
        "        raw_slice = sub_raw[z].astype(np.float32)\n",
        "        mn, mx = raw_slice.min(), raw_slice.max()\n",
        "        if mx > mn:\n",
        "            raw_slice = (raw_slice - mn) / (mx - mn)\n",
        "        else:\n",
        "            raw_slice = raw_slice - mn\n",
        "        raw_rgb = np.stack([raw_slice] * 3, axis=-1)\n",
        "        # Create 2D mask for unmasked regions\n",
        "        mask_factor = sub_combined_mask[z]  # This is now 2D\n",
        "        # Calculate average color of unmasked regions\n",
        "        unmasked_pixels = raw_slice[~mask_factor]  # Get unmasked pixel values\n",
        "        if len(unmasked_pixels) > 0:\n",
        "            avg_color = unmasked_pixels.mean()\n",
        "        else:\n",
        "            avg_color = 0  # In case there are no unmasked pixels\n",
        "        # Blend: apply transparency control (alpha_black)\n",
        "        raw_rgb = raw_rgb * mask_factor[:, :, np.newaxis] + (1 - mask_factor[:, :, np.newaxis]) * (alpha * avg_color + (1 - alpha) * raw_rgb)\n",
        "        overlaid_image = (np.clip(raw_rgb, 0, 1) * 255).astype(np.uint8)\n",
        "        overlaid_cube[:, :, :, z] = overlaid_image\n",
        "    return overlaid_cube\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    processor = Synapse3DProcessor(size=(80, 80))\n",
        "    vol_data_dict = {}\n",
        "\n",
        "    for bbox_name in args.bbox_name:\n",
        "        raw_vol, seg_vol, add_mask_vol = load_volumes(\n",
        "            bbox_name=bbox_name,\n",
        "            raw_base_dir=args.raw_base_dir,\n",
        "            seg_base_dir=args.seg_base_dir,\n",
        "            add_mask_base_dir=args.add_mask_base_dir\n",
        "        )\n",
        "        if raw_vol is not None:\n",
        "            vol_data_dict[bbox_name] = (raw_vol, seg_vol, add_mask_vol)\n",
        "\n",
        "    synapse_dfs = []\n",
        "    for bbox_name in args.bbox_name:\n",
        "        excel_path = os.path.join(args.excel_file, f\"{bbox_name}.xlsx\")\n",
        "        if os.path.exists(excel_path):\n",
        "            df = pd.read_excel(excel_path)\n",
        "            df['bbox_name'] = bbox_name\n",
        "            synapse_dfs.append(df)\n",
        "\n",
        "    syn_df = pd.concat(synapse_dfs, ignore_index=True)\n",
        "    dataset = SynapseDataset(\n",
        "        vol_data_dict=vol_data_dict,\n",
        "        synapse_df=syn_df,\n",
        "        processor=processor,\n",
        "        segmentation_type=args.segmentation_type,\n",
        "        subvol_size=args.subvol_size,\n",
        "        num_frames=args.num_frames,\n",
        "        alpha=args.alpha,\n",
        "    )\n",
        "\n",
        "    cubes = []\n",
        "    syn_info_list = []\n",
        "    for idx in range(len(dataset)):\n",
        "        pixel_values, syn_info, _ = dataset[idx]\n",
        "        cubes.append(pixel_values)\n",
        "        syn_info_list.append(syn_info)\n",
        "\n",
        "    print(f\"Processed {len(cubes)} cubes successfully.\")\n",
        "    return cubes, pd.DataFrame(syn_info_list)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "args = parse_args()\n",
        "args.segmentation_type=2\n",
        "args.bbox_name=['bbox1']\n",
        "cubes, sys_inf = main(args)\n",
        "print(f\"Final output: {len(cubes)} cubes\")\n",
        "\n",
        "# Extract the data tensor (cube) from the tuple returned by dataset[0]\n",
        "cube = cubes[0]\n",
        "\n",
        "mean = torch.tensor([0.485]).view(1, 1, 1, 1)\n",
        "std = torch.tensor([0.229]).view(1, 1, 1, 1)\n",
        "\n",
        "denormalized_cube = cube * std + mean\n",
        "denormalized_cube = torch.clamp(denormalized_cube, 0, 1)\n",
        "\n",
        "# Convert to RGB if needed\n",
        "frames = denormalized_cube.squeeze(1).numpy()\n",
        "frames = (frames * 255).astype(np.uint8)\n",
        "\n",
        "# Stack to create RGB\n",
        "frames = np.stack([frames, frames, frames], axis=-1)\n",
        "\n",
        "Gif_Name = f\"VGG_Gif_segmentation_type_{args.segmentation_type}_alpha_{args.alpha}\"\n",
        "output_gif_path = os.path.join(\"\", f\"{Gif_Name}.gif\")\n",
        "# Save the frames as a gif using imageio\n",
        "imageio.mimsave(output_gif_path, frames, fps=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HySCOqCkmrVP",
        "outputId": "23b33468-b072-4b7d-f5e9-064faae43060"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n",
            "[False  True]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Test for Dataloader"
      ],
      "metadata": {
        "id": "Urr0UTNApGF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir gifs"
      ],
      "metadata": {
        "id": "RA9AwB0JpeDf"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_gifs_for_all_segmentations(args):\n",
        "    # Loop through all segmentation types (0-9)\n",
        "    for segmentation_type in range(10):\n",
        "        print(f\"Generating GIF for segmentation type {segmentation_type}...\")\n",
        "\n",
        "        # Update the segmentation_type in the arguments\n",
        "        args.segmentation_type = segmentation_type\n",
        "\n",
        "        # Run the main function to process the dataset for the current segmentation type\n",
        "        cubes, sys_inf = main(args)\n",
        "\n",
        "        # Extract the first cube (sample) for GIF generation\n",
        "        cube = cubes[0]\n",
        "\n",
        "        # Denormalize the cube (if necessary)\n",
        "        mean = torch.tensor([0.485]).view(1, 1, 1, 1)\n",
        "        std = torch.tensor([0.229]).view(1, 1, 1, 1)\n",
        "        denormalized_cube = cube * std + mean\n",
        "        denormalized_cube = torch.clamp(denormalized_cube, 0, 1)\n",
        "\n",
        "        # Convert to RGB if needed\n",
        "        frames = denormalized_cube.squeeze(1).numpy()\n",
        "        frames = (frames * 255).astype(np.uint8)\n",
        "\n",
        "        # Stack to create RGB frames (if necessary)\n",
        "        frames_rgb = np.stack([frames, frames, frames], axis=-1)\n",
        "\n",
        "        # Generate the output GIF name\n",
        "        Gif_Name = f\"VGG_Gif_segmentation_type_{segmentation_type}_alpha_{args.alpha}\"\n",
        "        output_gif_path = os.path.join(args.save_gifs_dir, f\"{Gif_Name}.gif\")\n",
        "\n",
        "        # Save the frames as a GIF using imageio\n",
        "        imageio.mimsave(output_gif_path, frames_rgb, fps=10)\n",
        "\n",
        "        print(f\"GIF for segmentation type {segmentation_type} saved to {output_gif_path}\")\n",
        "\n",
        "# Run the function to generate GIFs for all segmentation types\n",
        "args = parse_args()\n",
        "args.save_gifs_dir = 'gifs'  # Make sure the save directory exists\n",
        "generate_gifs_for_all_segmentations(args)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNKNql92pIfZ",
        "outputId": "d9d3dbd5-e2da-4f79-e27b-48275c2531b8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating GIF for segmentation type 0...\n",
            "Processed 58 cubes successfully.\n",
            "GIF for segmentation type 0 saved to gifs/VGG_Gif_segmentation_type_0_alpha_0.5.gif\n",
            "Generating GIF for segmentation type 1...\n",
            "Processed 58 cubes successfully.\n",
            "GIF for segmentation type 1 saved to gifs/VGG_Gif_segmentation_type_1_alpha_0.5.gif\n",
            "Generating GIF for segmentation type 2...\n",
            "Processed 58 cubes successfully.\n",
            "GIF for segmentation type 2 saved to gifs/VGG_Gif_segmentation_type_2_alpha_0.5.gif\n",
            "Generating GIF for segmentation type 3...\n",
            "Processed 58 cubes successfully.\n",
            "GIF for segmentation type 3 saved to gifs/VGG_Gif_segmentation_type_3_alpha_0.5.gif\n",
            "Generating GIF for segmentation type 4...\n",
            "Processed 58 cubes successfully.\n",
            "GIF for segmentation type 4 saved to gifs/VGG_Gif_segmentation_type_4_alpha_0.5.gif\n",
            "Generating GIF for segmentation type 5...\n",
            "Processed 58 cubes successfully.\n",
            "GIF for segmentation type 5 saved to gifs/VGG_Gif_segmentation_type_5_alpha_0.5.gif\n",
            "Generating GIF for segmentation type 6...\n",
            "Processed 58 cubes successfully.\n",
            "GIF for segmentation type 6 saved to gifs/VGG_Gif_segmentation_type_6_alpha_0.5.gif\n",
            "Generating GIF for segmentation type 7...\n",
            "Processed 58 cubes successfully.\n",
            "GIF for segmentation type 7 saved to gifs/VGG_Gif_segmentation_type_7_alpha_0.5.gif\n",
            "Generating GIF for segmentation type 8...\n",
            "Processed 58 cubes successfully.\n",
            "GIF for segmentation type 8 saved to gifs/VGG_Gif_segmentation_type_8_alpha_0.5.gif\n",
            "Generating GIF for segmentation type 9...\n",
            "Processed 58 cubes successfully.\n",
            "GIF for segmentation type 9 saved to gifs/VGG_Gif_segmentation_type_9_alpha_0.5.gif\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Replace 'folder_name' with the name of your folder\n",
        "shutil.make_archive('/content/gifs', 'zip', '/content/gifs')\n",
        "from google.colab import files\n",
        "\n",
        "# Download the zip file\n",
        "files.download('/content/gifs.zip')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "yaK10mIRpTm4",
        "outputId": "d622e29b-db48-45ab-97d5-3a2a6cff4dca"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_63606dd2-3c54-4d91-9517-62f628ea3eab\", \"gifs.zip\", 5300845)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sZRu702ScSKA"
      }
    }
  ]
}