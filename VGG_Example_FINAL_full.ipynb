{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "daa58lj6b7qc",
        "o32CIf6a-VX0",
        "WcP9UfXuXZbi",
        "W4sXR_hb0mmu",
        "09dpZLkdZLwo",
        "lf_jtchGXcJz",
        "GpyUIuC7Wfwk",
        "RfOZInSDZYXK",
        "ZIKEZAyJjn9o",
        "iDUX9v4ij1lt",
        "NFaP91LVj9JS",
        "vrsHDN9QgsSw",
        "ZLYkGGVdklX0"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alim98/MPI/blob/main/VGG_Example_FINAL_full.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Essential downloads"
      ],
      "metadata": {
        "id": "daa58lj6b7qc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O downloaded_file.zip \"https://drive.usercontent.google.com/download?id=1iHPBdBOPEagvPTHZmrN__LD49emXwReY&export=download&authuser=0&confirm=t&uuid=631d60dd-569c-4bb1-a9e8-d681f0ed3d43&at=APvzH3r4me8x_LwP3n8O7lgPo8oK%3A1733988188000\"\n",
        "!wget -O vesicle_cloud__syn_interface__mitochondria_annotation.zip \"https://drive.usercontent.google.com/download?id=1qRibZL3kr7MQJQRgDFRquHMQlIGCN4XP&export=download&authuser=0&confirm=t&uuid=631d60dd-569c-4bb1-a9e8-d681f0ed3d43&at=APvzH3r4me8x_LwP3n8O7lgPo8oK%3A1733988188000\"\n",
        "\n",
        "!unzip -q downloaded_file.zip\n",
        "!unzip -q vesicle_cloud__syn_interface__mitochondria_annotation.zip"
      ],
      "metadata": {
        "id": "AIIIxarwTVT0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "628e30b2-dd37-41d9-e7e7-4dceef8b8e6a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-27 19:26:35--  https://drive.usercontent.google.com/download?id=1iHPBdBOPEagvPTHZmrN__LD49emXwReY&export=download&authuser=0&confirm=t&uuid=631d60dd-569c-4bb1-a9e8-d681f0ed3d43&at=APvzH3r4me8x_LwP3n8O7lgPo8oK%3A1733988188000\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.143.132, 2a00:1450:4013:c03::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.143.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1264688649 (1.2G) [application/octet-stream]\n",
            "Saving to: ‘downloaded_file.zip’\n",
            "\n",
            "downloaded_file.zip 100%[===================>]   1.18G  79.3MB/s    in 14s     \n",
            "\n",
            "2025-01-27 19:26:52 (85.5 MB/s) - ‘downloaded_file.zip’ saved [1264688649/1264688649]\n",
            "\n",
            "--2025-01-27 19:26:52--  https://drive.usercontent.google.com/download?id=1qRibZL3kr7MQJQRgDFRquHMQlIGCN4XP&export=download&authuser=0&confirm=t&uuid=631d60dd-569c-4bb1-a9e8-d681f0ed3d43&at=APvzH3r4me8x_LwP3n8O7lgPo8oK%3A1733988188000\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.143.132, 2a00:1450:4013:c03::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.143.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14246564 (14M) [application/octet-stream]\n",
            "Saving to: ‘vesicle_cloud__syn_interface__mitochondria_annotation.zip’\n",
            "\n",
            "vesicle_cloud__syn_ 100%[===================>]  13.59M  57.8MB/s    in 0.2s    \n",
            "\n",
            "2025-01-27 19:27:23 (57.8 MB/s) - ‘vesicle_cloud__syn_interface__mitochondria_annotation.zip’ saved [14246564/14246564]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install transformers scikit-learn matplotlib seaborn torch torchvision umap-learn git+https://github.com/funkelab/funlib.learn.torch.git\n",
        "!pip install openpyxl\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZwyJMBQTXV3",
        "outputId": "b02536c0-ee21-4e8c-8504-a586d86dd48c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/funkelab/funlib.learn.torch.git\n",
            "  Cloning https://github.com/funkelab/funlib.learn.torch.git to /tmp/pip-req-build-my41fwyp\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/funkelab/funlib.learn.torch.git /tmp/pip-req-build-my41fwyp\n",
            "  Resolved https://github.com/funkelab/funlib.learn.torch.git to commit 049729151c7a2c0320a446dc9d3244ac830f7ea8\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu121)\n",
            "Collecting umap-learn\n",
            "  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.55.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.60.0)\n",
            "Collecting pynndescent>=0.5 (from umap-learn)\n",
            "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Downloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: funlib.learn.torch\n",
            "  Building wheel for funlib.learn.torch (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for funlib.learn.torch: filename=funlib.learn.torch-0.1.0-py3-none-any.whl size=13995 sha256=9ab5f7778e28b7f3d5ddb7e5af2d8ee272aed608f417b71fd2155cb69175906d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-75z5niev/wheels/ae/7f/7b/ecbd355ccdfbd2bb0ab4f76ea45f60a02a572c88c1f4761e8d\n",
            "Successfully built funlib.learn.torch\n",
            "Installing collected packages: pynndescent, umap-learn, funlib.learn.torch\n",
            "Successfully installed funlib.learn.torch-0.1.0 pynndescent-0.5.13 umap-learn-0.5.7\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import glob\n",
        "import imageio.v2 as iio\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Patch, Rectangle\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import ViTImageProcessor, ViTModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns\n",
        "from umap import UMAP\n",
        "import torch.nn.functional as F\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "HRS_MN6YTekw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GradCam Funcs"
      ],
      "metadata": {
        "id": "o32CIf6a-VX0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Claudai"
      ],
      "metadata": {
        "id": "WcP9UfXuXZbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn.functional as F\n",
        "# import numpy as np\n",
        "# import imageio\n",
        "# import os\n",
        "# import torch\n",
        "# import torch.nn.functional as F\n",
        "# import numpy as np\n",
        "# import imageio\n",
        "# import os\n",
        "# import cv2\n",
        "# from matplotlib import cm\n",
        "\n",
        "# class GradCAM3D:\n",
        "#     def __init__(self, model, target_layer):\n",
        "#         self.model = model\n",
        "#         self.target_layer = target_layer\n",
        "#         self.gradients = None\n",
        "#         self.activations = None\n",
        "\n",
        "#         # Register hooks\n",
        "#         target_layer.register_forward_hook(self.save_activation)\n",
        "#         target_layer.register_full_backward_hook(self.save_gradient)\n",
        "\n",
        "#     def save_activation(self, module, input, output):\n",
        "#         self.activations = output.detach()\n",
        "\n",
        "#     def save_gradient(self, module, grad_input, grad_output):\n",
        "#         self.gradients = grad_output[0].detach()\n",
        "\n",
        "#     def generate_cam(self, input_tensor, target_class=None):\n",
        "#         # Forward pass\n",
        "#         model_output = self.model(input_tensor)\n",
        "\n",
        "#         if target_class is None:\n",
        "#             target_class = torch.argmax(model_output)\n",
        "\n",
        "#         # Zero all existing gradients\n",
        "#         self.model.zero_grad()\n",
        "\n",
        "#         # Target for backprop\n",
        "#         one_hot_output = torch.zeros_like(model_output)\n",
        "#         one_hot_output[0][target_class] = 1\n",
        "\n",
        "#         # Backward pass\n",
        "#         model_output.backward(gradient=one_hot_output)\n",
        "\n",
        "#         # Calculate weights\n",
        "#         weights = torch.mean(self.gradients, dim=(2, 3, 4))\n",
        "\n",
        "#         # Generate CAM\n",
        "#         cam = torch.zeros(self.activations.shape[2:], dtype=torch.float32).to(input_tensor.device)\n",
        "\n",
        "#         for i, w in enumerate(weights[0]):\n",
        "#             cam += w * self.activations[0, i]\n",
        "\n",
        "#         cam = F.relu(cam)  # Apply ReLU to focus on features that have a positive influence\n",
        "\n",
        "#         # Normalize\n",
        "#         cam = cam - torch.min(cam)\n",
        "#         cam = cam / torch.max(cam)\n",
        "\n",
        "#         return cam\n",
        "\n",
        "# def apply_colormap(gray_img):\n",
        "#     \"\"\"Apply jet colormap to grayscale image.\"\"\"\n",
        "#     colored = cm.jet(gray_img)  # Using jet colormap\n",
        "#     return (colored[:, :, :3] * 255).astype(np.uint8)\n",
        "\n",
        "# def apply_gradcam(model, dataset, args, num_samples=5):\n",
        "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#     model = model.to(device)\n",
        "#     model.eval()\n",
        "\n",
        "#     # Get the last convolutional layer\n",
        "#     target_layer = None\n",
        "#     for module in model.features:\n",
        "#         if isinstance(module, torch.nn.Conv3d):\n",
        "#             target_layer = module\n",
        "#     target_layer=model.features[27]\n",
        "#     print(target_layer)\n",
        "#     gradcam = GradCAM3D(model, target_layer)\n",
        "\n",
        "#     for idx in range(min(num_samples, len(dataset))):\n",
        "#         # Get sample\n",
        "#         idx=idx+80\n",
        "#         cube, syn_info, bbox_name = dataset[idx]\n",
        "#         input_tensor = cube.unsqueeze(0).permute(0, 2, 1, 3, 4).to(device)  # Add batch dimension and adjust channels\n",
        "\n",
        "#         # Generate CAM\n",
        "#         cam = gradcam.generate_cam(input_tensor)\n",
        "#         cam = cam.cpu().numpy()\n",
        "\n",
        "#         # Create heatmap visualization\n",
        "#         frames = []\n",
        "#         original_frames = cube.permute(0, 2, 3, 1).numpy()  # (T, H, W, C)\n",
        "\n",
        "#         # Ensure we have 80 frames by interpolating if necessary\n",
        "#         if cam.shape[0] != 80:\n",
        "#             # Create indices for interpolation\n",
        "#             old_indices = np.linspace(0, cam.shape[0]-1, cam.shape[0])\n",
        "#             new_indices = np.linspace(0, cam.shape[0]-1, 80)\n",
        "\n",
        "#             # Interpolate CAM to 80 frames\n",
        "#             new_cam = np.zeros((80, cam.shape[1], cam.shape[2]))\n",
        "#             for i in range(cam.shape[1]):\n",
        "#                 for j in range(cam.shape[2]):\n",
        "#                     new_cam[:, i, j] = np.interp(new_indices, old_indices, cam[:, i, j])\n",
        "#             cam = new_cam\n",
        "\n",
        "#             # Interpolate original frames if needed\n",
        "#             if original_frames.shape[0] != 80:\n",
        "#                 new_original_frames = np.zeros((80, original_frames.shape[1], original_frames.shape[2], original_frames.shape[3]))\n",
        "#                 for c in range(original_frames.shape[3]):\n",
        "#                     for i in range(original_frames.shape[1]):\n",
        "#                         for j in range(original_frames.shape[2]):\n",
        "#                             new_original_frames[:, i, j, c] = np.interp(new_indices, old_indices, original_frames[:, i, j, c])\n",
        "#                 original_frames = new_original_frames\n",
        "\n",
        "#         for t in range(80):  # Process all 80 frames\n",
        "#             # Normalize frame\n",
        "#             frame = original_frames[t]\n",
        "#             frame = (frame - frame.min()) / (frame.max() - frame.min())\n",
        "\n",
        "#             # Get CAM slice and resize to match frame size\n",
        "#             cam_slice = cam[t]\n",
        "#             cam_slice = cv2.resize(cam_slice, (frame.shape[1], frame.shape[0]))\n",
        "\n",
        "#             # Apply colormap to CAM\n",
        "#             heatmap = apply_colormap(cam_slice)\n",
        "\n",
        "#             # Convert original frame to RGB if it's not\n",
        "#             if frame.shape[-1] == 1:\n",
        "#                 frame = np.repeat(frame, 3, axis=-1)\n",
        "\n",
        "#             # Ensure frame is in 0-255 range\n",
        "#             frame = (frame * 255).astype(np.uint8)\n",
        "\n",
        "#             # Combine original frame and heatmap\n",
        "#             alpha = 0.4\n",
        "#             overlay = cv2.addWeighted(frame, 1 - alpha, heatmap, alpha, 0)\n",
        "#             frames.append(overlay)\n",
        "\n",
        "#         # Save as GIF\n",
        "#         output_dir = os.path.join(\"Result2\", \"gradcam\")\n",
        "#         os.makedirs(output_dir, exist_ok=True)\n",
        "#         output_path = os.path.join(output_dir, f\"gradcam_sample_{idx}_type_{args.segmentation_type}.gif\")\n",
        "#         imageio.mimsave(output_path, frames, fps=10)\n",
        "\n",
        "#         print(f\"Saved GradCAM visualization for sample {idx} to {output_path}\")\n",
        "\n",
        "# # Function to run GradCAM analysis\n",
        "# def run_gradcam_analysis(model, dataset, args):\n",
        "#     print(\"Starting GradCAM analysis...\")\n",
        "#     apply_gradcam(model, dataset, args)\n",
        "#     print(\"GradCAM analysis completed!\")\n",
        "# # Import the GradCAM implementation\n",
        "\n",
        "# # After your existing feature extraction code:\n",
        "# print(\"Running GradCAM analysis...\")\n",
        "# run_gradcam_analysis(model, dataset, args)\n",
        "\n",
        "# # Save features as before\n",
        "# segmentation_type_name = f\"VGG_CSV_segmentation_type_{args.segmentation_type}_input_mask_{args.input_mask}\"\n",
        "# output_csv_path = os.path.join(\"Result\", f\"{segmentation_type_name}_features.csv\")\n",
        "# # features_df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "# print(\"Analysis completed! Check the Result/gradcam directory for visualizations.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "i1oz72aO-XTG",
        "outputId": "db45a4f1-1c81-4a04-b332-6cdeb201eb36"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running GradCAM analysis...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-1847d81051a6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;31m# After your existing feature extraction code:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running GradCAM analysis...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m \u001b[0mrun_gradcam_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;31m# Save features as before\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn.functional as F\n",
        "# import numpy as np\n",
        "# import imageio\n",
        "# import os\n",
        "# import cv2\n",
        "# from matplotlib import pyplot as plt\n",
        "\n",
        "# class GradCAM3D:\n",
        "#     def __init__(self, model):\n",
        "#         self.model = model\n",
        "#         self.gradients = None\n",
        "#         self.activations = None\n",
        "\n",
        "#         # Get the last convolutional layer\n",
        "#         for module in model.modules():\n",
        "#             if isinstance(module, torch.nn.Conv3d):\n",
        "#                 self.target_layer = module\n",
        "\n",
        "#         if self.target_layer is None:\n",
        "#             raise ValueError(\"Could not find a convolutional layer\")\n",
        "\n",
        "#         # Register hooks\n",
        "#         self.target_layer.register_forward_hook(self.forward_hook)\n",
        "#         self.target_layer.register_forward_hook(lambda m, i, o: setattr(self, 'activations', o.detach()))\n",
        "#         self.target_layer.register_backward_hook(lambda m, i, o: setattr(self, 'gradients', o[0].detach()))\n",
        "\n",
        "#     def forward_hook(self, module, input, output):\n",
        "#         self.activations = output\n",
        "\n",
        "#     def generate_cam(self, input_tensor, target_class=None):\n",
        "#         # Ensure model is in eval mode and clear gradients\n",
        "#         self.model.eval()\n",
        "#         self.model.zero_grad()\n",
        "\n",
        "#         # Reshape input tensor for model\n",
        "#         input_tensor = input_tensor.permute(0, 2, 1, 3, 4)\n",
        "\n",
        "#         # Forward pass\n",
        "#         output = self.model(input_tensor)\n",
        "\n",
        "#         if target_class is None:\n",
        "#             target_class = output.argmax(dim=1)\n",
        "\n",
        "#         # Create one-hot encoding for target class\n",
        "#         target = torch.zeros_like(output, dtype=torch.float32)\n",
        "#         target[0, target_class] = 1.0\n",
        "\n",
        "#         # Backward pass\n",
        "#         output.backward(target, retain_graph=True)\n",
        "\n",
        "#         # Get gradients and activations\n",
        "#         gradients = self.gradients\n",
        "#         activations = self.activations\n",
        "\n",
        "#         # Calculate weights\n",
        "#         weights = torch.mean(gradients, dim=(2, 3, 4))[0]\n",
        "\n",
        "#         # Generate CAM\n",
        "#         cam = torch.zeros(activations.shape[2:], device=activations.device)\n",
        "#         for i, w in enumerate(weights):\n",
        "#             cam += w * activations[0, i]\n",
        "\n",
        "#         # Apply ReLU and normalize\n",
        "#         cam = F.relu(cam)\n",
        "#         cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
        "\n",
        "#         return cam.detach().cpu().numpy()\n",
        "\n",
        "# def apply_gradcam(model, dataset, args, num_samples=5):\n",
        "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#     model = model.to(device)\n",
        "\n",
        "#     # Initialize GradCAM\n",
        "#     gradcam = GradCAM3D(model)\n",
        "\n",
        "#     # Process samples\n",
        "#     for idx in range(min(num_samples, len(dataset))):\n",
        "#         # Get input data\n",
        "#         cube, syn_info, bbox_name = dataset[idx]\n",
        "#         input_tensor = cube.unsqueeze(0).to(device)\n",
        "\n",
        "#         # Generate CAM\n",
        "#         cam = gradcam.generate_cam(input_tensor)\n",
        "\n",
        "#         # Create visualization frames\n",
        "#         frames = []\n",
        "#         original_frames = cube.permute(0, 2, 3, 1).cpu().numpy()\n",
        "\n",
        "#         for t in range(80):\n",
        "#             # Get original frame\n",
        "#             frame = np.squeeze(original_frames[t])\n",
        "#             frame = (frame - frame.min()) / (frame.max() - frame.min() + 1e-8)\n",
        "\n",
        "#             # Get CAM slice\n",
        "#             cam_slice = cv2.resize(cam[t % cam.shape[0]], (frame.shape[1], frame.shape[0]))\n",
        "\n",
        "#             # Create heatmap\n",
        "#             heatmap = cv2.applyColorMap(np.uint8(255 * cam_slice), cv2.COLORMAP_JET)\n",
        "#             heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "#             # Convert frame to RGB\n",
        "#             frame_rgb = np.uint8(255 * np.stack([frame]*3, axis=-1))\n",
        "\n",
        "#             # Create overlay\n",
        "#             alpha = 0.4\n",
        "#             overlay = cv2.addWeighted(frame_rgb, 1-alpha, heatmap, alpha, 0)\n",
        "#             frames.append(overlay)\n",
        "\n",
        "#         # Create output directory\n",
        "#         output_dir = os.path.join(\"Result\", \"gradcam\")\n",
        "#         os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "#         # Save GIF\n",
        "#         output_path = os.path.join(\n",
        "#             output_dir,\n",
        "#             f\"gradcam_sample_{idx}_type_{args.segmentation_type}_inputmask_{args.input_mask}.gif\"\n",
        "#         )\n",
        "#         imageio.mimsave(output_path, frames, fps=10)\n",
        "\n",
        "#         # Create summary visualization\n",
        "#         fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
        "#         fig.suptitle(f'GradCAM Visualization - Sample {idx}')\n",
        "\n",
        "#         # Select middle frame\n",
        "#         mid_frame = 40\n",
        "\n",
        "#         # Axial view\n",
        "#         axs[0, 0].imshow(np.squeeze(original_frames[mid_frame]), cmap='gray')\n",
        "#         axs[0, 0].set_title('Original')\n",
        "#         axs[0, 0].axis('off')\n",
        "\n",
        "#         axs[0, 1].imshow(cam[mid_frame % cam.shape[0]], cmap='jet')\n",
        "#         axs[0, 1].set_title('GradCAM')\n",
        "#         axs[0, 1].axis('off')\n",
        "\n",
        "#         axs[0, 2].imshow(frames[mid_frame])\n",
        "#         axs[0, 2].set_title('Overlay')\n",
        "#         axs[0, 2].axis('off')\n",
        "\n",
        "#         # Sagittal view\n",
        "#         sagittal_orig = np.squeeze(original_frames[:, :, mid_frame]).mean(axis=2)\n",
        "#         sagittal_cam = np.mean(cam[:, :, mid_frame % cam.shape[0]], axis=1)\n",
        "\n",
        "#         axs[1, 0].imshow(sagittal_orig, cmap='gray')\n",
        "#         axs[1, 0].set_title('Sagittal Original')\n",
        "#         axs[1, 0].axis('off')\n",
        "\n",
        "#         axs[1, 1].imshow(sagittal_cam, cmap='jet')\n",
        "#         axs[1, 1].set_title('Sagittal GradCAM')\n",
        "#         axs[1, 1].axis('off')\n",
        "\n",
        "#         # Create sagittal overlay\n",
        "#         sagittal_heatmap = cv2.applyColorMap(np.uint8(255 * sagittal_cam), cv2.COLORMAP_JET)\n",
        "#         sagittal_heatmap = cv2.cvtColor(sagittal_heatmap, cv2.COLOR_BGR2RGB)\n",
        "#         sagittal_frame = np.uint8(255 * np.stack([sagittal_orig]*3, axis=-1))\n",
        "#         sagittal_overlay = cv2.addWeighted(sagittal_frame, 1-alpha, sagittal_heatmap, alpha, 0)\n",
        "\n",
        "#         axs[1, 2].imshow(sagittal_overlay)\n",
        "#         axs[1, 2].set_title('Sagittal Overlay')\n",
        "#         axs[1, 2].axis('off')\n",
        "\n",
        "#         # Save summary plot\n",
        "#         plt.savefig(os.path.join(output_dir, f'gradcam_summary_{idx}.png'))\n",
        "#         plt.close()\n",
        "\n",
        "#         print(f\"Saved GradCAM visualization for sample {idx}\")\n",
        "\n",
        "# def run_gradcam_analysis(model, dataset, args):\n",
        "#     print(\"Starting GradCAM analysis...\")\n",
        "#     apply_gradcam(model, dataset, args)\n",
        "#     print(\"GradCAM analysis completed!\")\n",
        "# # After your existing feature extraction code:\n",
        "# print(\"Running GradCAM analysis...\")\n",
        "# run_gradcam_analysis(model, dataset, args)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 825
        },
        "id": "lXblyJ4GYbfs",
        "outputId": "34f484a9-d17b-477f-f46e-3b648b3556a1"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running GradCAM analysis...\n",
            "Starting GradCAM analysis...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Cannot use both regular backward hooks and full backward hooks on a single Module. Please use only one of them.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-1200c839abc8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;31m# After your existing feature extraction code:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running GradCAM analysis...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m \u001b[0mrun_gradcam_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-63-1200c839abc8>\u001b[0m in \u001b[0;36mrun_gradcam_analysis\u001b[0;34m(model, dataset, args)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_gradcam_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting GradCAM analysis...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0mapply_gradcam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GradCAM analysis completed!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;31m# After your existing feature extraction code:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-63-1200c839abc8>\u001b[0m in \u001b[0;36mapply_gradcam\u001b[0;34m(model, dataset, args, num_samples)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# Initialize GradCAM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mgradcam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradCAM3D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;31m# Process samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-63-1200c839abc8>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_forward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_hook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_forward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'activations'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gradients'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mregister_backward_hook\u001b[0;34m(self, hook)\u001b[0m\n\u001b[1;32m   1404\u001b[0m         \"\"\"\n\u001b[1;32m   1405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_full_backward_hook\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1406\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1407\u001b[0m                 \u001b[0;34m\"Cannot use both regular backward hooks and full backward hooks on a \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m                 \u001b[0;34m\"single Module. Please use only one of them.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Cannot use both regular backward hooks and full backward hooks on a single Module. Please use only one of them."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oY8xpOO_YcsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.features[27]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hsyspl9rI2oA",
        "outputId": "787422dd-9edd-4b14-a1f6-eab5d29ec6ef"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv3d(96, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn.functional as F\n",
        "# import numpy as np\n",
        "# import imageio\n",
        "# import os\n",
        "# import cv2\n",
        "# from matplotlib import pyplot as plt\n",
        "\n",
        "# class GradCAM3D:\n",
        "#     def __init__(self, model, target_layer):\n",
        "#         self.model = model\n",
        "#         self.target_layer = target_layer\n",
        "#         self.activations = None\n",
        "#         self.gradients = None\n",
        "\n",
        "#         # Register hooks\n",
        "#         self.target_layer.register_forward_hook(self._save_activation)\n",
        "#         self.target_layer.register_full_backward_hook(self._save_gradient)\n",
        "\n",
        "#     def _save_activation(self, module, input, output):\n",
        "#         self.activations = output\n",
        "\n",
        "#     def _save_gradient(self, module, grad_input, grad_output):\n",
        "#         self.gradients = grad_output[0]\n",
        "\n",
        "#     def generate_cam(self, input_tensor, target_class=None):\n",
        "#         # Ensure model is in eval mode\n",
        "#         self.model.eval()\n",
        "\n",
        "#         # Forward pass to get class predictions\n",
        "#         pred = self.model(input_tensor)\n",
        "\n",
        "#         if target_class is None:\n",
        "#             target_class = pred.argmax(dim=1)\n",
        "\n",
        "#         # Zero gradients\n",
        "#         self.model.zero_grad()\n",
        "\n",
        "#         # Target for backprop\n",
        "#         one_hot = torch.zeros_like(pred)\n",
        "#         one_hot[0][target_class] = 1\n",
        "\n",
        "#         # Backward pass\n",
        "#         pred.backward(gradient=one_hot, retain_graph=True)\n",
        "\n",
        "#         # Get weights\n",
        "#         weights = torch.mean(self.gradients, dim=(2, 3, 4))\n",
        "\n",
        "#         # Generate CAM\n",
        "#         batch_size = self.activations.shape[0]\n",
        "#         cam = torch.zeros(self.activations.shape[2:], device=self.activations.device)\n",
        "\n",
        "#         # Weighted sum of activation maps\n",
        "#         for w, act in zip(weights[0], self.activations[0]):\n",
        "#             cam += w * act\n",
        "\n",
        "#         # Apply ReLU\n",
        "#         cam = F.relu(cam)\n",
        "\n",
        "#         # Normalize\n",
        "#         cam = cam - cam.min()\n",
        "#         cam = cam / (cam.max() + 1e-7)\n",
        "\n",
        "#         return cam.detach().cpu().numpy()\n",
        "\n",
        "# def apply_gradcam(model, dataset, args, num_samples=5, layer_name='conv3d_18'):\n",
        "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#     model = model.to(device)\n",
        "\n",
        "#     # Get target layer\n",
        "#     target_layer = None\n",
        "#     for name, module in model.named_modules():\n",
        "#         if isinstance(module, torch.nn.Conv3d) and name == layer_name:\n",
        "#             target_layer = module\n",
        "#             break\n",
        "\n",
        "#     # if target_layer is None:\n",
        "#     #     raise ValueError(f\"Could not find layer {layer_name}\")\n",
        "#     target_layer=model.features[27]\n",
        "\n",
        "#     gradcam = GradCAM3D(model, target_layer)\n",
        "\n",
        "#     for idx in range(min(num_samples, len(dataset))):\n",
        "#         # Get input data\n",
        "#         cube, syn_info, bbox_name = dataset[idx]\n",
        "#         input_tensor = cube.unsqueeze(0).to(device)\n",
        "\n",
        "#         # Generate GradCAM\n",
        "#         cam = gradcam.generate_cam(input_tensor)\n",
        "\n",
        "#         # Create visualization\n",
        "#         frames = []\n",
        "#         original_frames = cube.permute(0, 2, 3, 1).cpu().numpy()\n",
        "\n",
        "#         for t in range(80):  # Process all 80 frames\n",
        "#             # Get original frame\n",
        "#             frame = original_frames[t]\n",
        "#             frame = (frame - frame.min()) / (frame.max() - frame.min() + 1e-7)\n",
        "\n",
        "#             # Get CAM slice and resize\n",
        "#             if t < cam.shape[0]:\n",
        "#                 cam_slice = cam[t]\n",
        "#             else:\n",
        "#                 # For frames beyond CAM depth, use the last available slice\n",
        "#                 cam_slice = cam[-1]\n",
        "\n",
        "#             # Resize CAM to match frame size\n",
        "#             cam_slice = cv2.resize(cam_slice, (frame.shape[1], frame.shape[0]))\n",
        "\n",
        "#             # Apply colormap\n",
        "#             heatmap = cv2.applyColorMap(np.uint8(255 * cam_slice), cv2.COLORMAP_JET)\n",
        "#             heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "#             # Convert grayscale frame to RGB\n",
        "#             frame_rgb = np.uint8(255 * np.stack([frame]*3, axis=-1))\n",
        "\n",
        "#             # Create overlay\n",
        "#             alpha = 0.4\n",
        "#             overlay = cv2.addWeighted(frame_rgb, 1-alpha, heatmap, alpha, 0)\n",
        "#             frames.append(overlay)\n",
        "\n",
        "#         # Save GIF\n",
        "#         output_dir = os.path.join(\"Result\", \"gradcam\")\n",
        "#         os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "#         output_path = os.path.join(\n",
        "#             output_dir,\n",
        "#             f\"gradcam_sample_{idx}_type_{args.segmentation_type}_inputmask_{args.input_mask}.gif\"\n",
        "#         )\n",
        "\n",
        "#         # Save with consistent frame rate\n",
        "#         imageio.mimsave(output_path, frames, fps=10)\n",
        "\n",
        "#         # Create and save summary visualization\n",
        "#         fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
        "#         fig.suptitle(f'GradCAM Visualization - Sample {idx}')\n",
        "\n",
        "#         # Select representative slices\n",
        "#         mid_frame = 40  # Middle frame\n",
        "\n",
        "#         # Axial view\n",
        "#         axs[0, 0].imshow(original_frames[mid_frame], cmap='gray')\n",
        "#         axs[0, 0].set_title('Original')\n",
        "#         axs[0, 0].axis('off')\n",
        "\n",
        "#         axs[0, 1].imshow(cam[mid_frame], cmap='jet')\n",
        "#         axs[0, 1].set_title('GradCAM')\n",
        "#         axs[0, 1].axis('off')\n",
        "\n",
        "#         axs[0, 2].imshow(frames[mid_frame])\n",
        "#         axs[0, 2].set_title('Overlay')\n",
        "#         axs[0, 2].axis('off')\n",
        "\n",
        "#         # Sagittal view\n",
        "#         sagittal_orig = original_frames[:, :, mid_frame].mean(axis=2)\n",
        "#         sagittal_cam = cam[:, :, mid_frame]\n",
        "\n",
        "#         axs[1, 0].imshow(sagittal_orig, cmap='gray')\n",
        "#         axs[1, 0].set_title('Sagittal Original')\n",
        "#         axs[1, 0].axis('off')\n",
        "\n",
        "#         axs[1, 1].imshow(sagittal_cam, cmap='jet')\n",
        "#         axs[1, 1].set_title('Sagittal GradCAM')\n",
        "#         axs[1, 1].axis('off')\n",
        "\n",
        "#         # Create sagittal overlay\n",
        "#         sagittal_heatmap = cv2.applyColorMap(np.uint8(255 * sagittal_cam), cv2.COLORMAP_JET)\n",
        "#         sagittal_heatmap = cv2.cvtColor(sagittal_heatmap, cv2.COLOR_BGR2RGB)\n",
        "#         sagittal_frame = np.uint8(255 * np.stack([sagittal_orig]*3, axis=-1))\n",
        "#         sagittal_overlay = cv2.addWeighted(sagittal_frame, 1-alpha, sagittal_heatmap, alpha, 0)\n",
        "\n",
        "#         axs[1, 2].imshow(sagittal_overlay)\n",
        "#         axs[1, 2].set_title('Sagittal Overlay')\n",
        "#         axs[1, 2].axis('off')\n",
        "\n",
        "#         # Save plot\n",
        "#         plt.savefig(os.path.join(output_dir, f'gradcam_summary2_{idx}.png'))\n",
        "#         plt.close()\n",
        "\n",
        "#         print(f\"Saved GradCAM visualization for sample {idx}\")\n",
        "\n",
        "# def run_gradcam_analysis(model, dataset, args):\n",
        "#     print(\"Starting GradCAM analysis...\")\n",
        "#     apply_gradcam(model, dataset, args)\n",
        "#     print(\"GradCAM analysis completed!\")\n",
        "\n",
        "# # Import the GradCAM implementation\n",
        "\n",
        "# # After your feature extraction and before saving the features:\n",
        "# print(\"Running GradCAM analysis...\")\n",
        "# run_gradcam_analysis(model, dataset, args)\n",
        "\n",
        "# # # Continue with your existing feature saving code\n",
        "# # print(\"Saving features...\")\n",
        "# # segmentation_type_name = f\"VGG_CSV_segmentation_type_{args.segmentation_type}_input_mask_{args.input_mask}\"\n",
        "# # output_csv_path = os.path.join(\"Result\", f\"{segmentation_type_name}_features.csv\")\n",
        "# # features_df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "# print(\"Analysis completed! Check the Result/gradcam directory for visualizations.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d8TJAfLLEWNp",
        "outputId": "e83fcb87-a288-4399-f153-bd69210b1898"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running GradCAM analysis...\n",
            "Starting GradCAM analysis...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Given groups=1, weight of size [24, 1, 3, 3, 3], expected input[1, 80, 1, 80, 80] to have 1 channels, but got 80 channels instead",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-5e87d24d6bd8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;31m# After your feature extraction and before saving the features:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running GradCAM analysis...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m \u001b[0mrun_gradcam_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;31m# # Continue with your existing feature saving code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-54-5e87d24d6bd8>\u001b[0m in \u001b[0;36mrun_gradcam_analysis\u001b[0;34m(model, dataset, args)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_gradcam_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting GradCAM analysis...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m     \u001b[0mapply_gradcam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GradCAM analysis completed!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-54-5e87d24d6bd8>\u001b[0m in \u001b[0;36mapply_gradcam\u001b[0;34m(model, dataset, args, num_samples, layer_name)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Generate GradCAM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mcam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradcam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_cam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# Create visualization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-54-5e87d24d6bd8>\u001b[0m in \u001b[0;36mgenerate_cam\u001b[0;34m(self, input_tensor, target_class)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Forward pass to get class predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtarget_class\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-16434c063c55>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, return_features)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m  \u001b[0;31m# Return raw features before flattening\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    718\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m             )\n\u001b[0;32m--> 720\u001b[0;31m         return F.conv3d(\n\u001b[0m\u001b[1;32m    721\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [24, 1, 3, 3, 3], expected input[1, 80, 1, 80, 80] to have 1 channels, but got 80 channels instead"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Visualizing Feature Maps (Activations)\n",
        ""
      ],
      "metadata": {
        "id": "W4sXR_hb0mmu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand where model attends to in terms of feature extraction, we can visualize the feature maps from different layers of the model. The activations represent which regions of the 3D input are being activated by different filters at each convolutional layer."
      ],
      "metadata": {
        "id": "FwpHWBvu062o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "args = parse_args()\n",
        "args.input_mask=False\n",
        "args.segmentation_type=0\n",
        "# Initialize the arguments (or use your existing args)\n",
        "# Use the main function to load data and prepare the dataset\n",
        "# cubes, sys_inf = main(args)\n",
        "\n",
        "# Define the processor (if not already done)\n",
        "processor = SimpleVideoProcessor(size=(80, 80))\n",
        "\n",
        "# Prepare the dataset\n",
        "vol_data_dict = {}\n",
        "for bbox_name in args.bbox_name:\n",
        "    raw_vol, seg_vol, add_mask_vol = load_volumes(\n",
        "        bbox_name=bbox_name,\n",
        "        raw_base_dir=args.raw_base_dir,\n",
        "        seg_base_dir=args.seg_base_dir,\n",
        "        add_mask_base_dir=args.add_mask_base_dir\n",
        "    )\n",
        "    if raw_vol is not None:\n",
        "        vol_data_dict[bbox_name] = (raw_vol, seg_vol, add_mask_vol)\n",
        "\n",
        "synapse_dfs = []\n",
        "for bbox_name in args.bbox_name:\n",
        "    excel_path = os.path.join(args.excel_file, f\"{bbox_name}.xlsx\")\n",
        "    if os.path.exists(excel_path):\n",
        "        df = pd.read_excel(excel_path)\n",
        "        df['bbox_name'] = bbox_name\n",
        "        synapse_dfs.append(df)\n",
        "\n",
        "syn_df = pd.concat(synapse_dfs, ignore_index=True)\n",
        "\n",
        "# Prepare the dataset\n",
        "dataset = VideoMAEDataset(\n",
        "    vol_data_dict=vol_data_dict,\n",
        "    synapse_df=syn_df,\n",
        "    processor=processor,\n",
        "    segmentation_type=args.segmentation_type,\n",
        "    subvol_size=args.subvol_size,\n",
        "    num_frames=args.num_frames,\n",
        "    alpha=args.alpha,\n",
        "    input_mask=False\n",
        ")"
      ],
      "metadata": {
        "id": "h5x7-cKa0rsA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import ndimage\n",
        "from tqdm import tqdm\n",
        "\n",
        "def visualize_sample_attention(model, dataset, sample_idx=0, output_dir=\"attention_gifs\", layer_index=30):\n",
        "    \"\"\"\n",
        "    Visualize attention for a specific sample from your dataset\n",
        "    Returns GIF path and metadata for the visualized sample\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device).eval()\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Get sample from your dataset\n",
        "    pixel_values, syn_info, bbox_name = dataset[sample_idx]\n",
        "\n",
        "    # Convert to model input format\n",
        "    input_tensor = pixel_values.permute(1, 0, 2, 3).unsqueeze(0).to(device)  # [1, C, D, H, W]\n",
        "\n",
        "    # Setup activation hook\n",
        "    activation = {}\n",
        "    def get_activation(name):\n",
        "        def hook(model, input, output):\n",
        "            activation[name] = output.detach()\n",
        "        return hook\n",
        "\n",
        "    hook = model.features[layer_index].register_forward_hook(get_activation('conv'))\n",
        "\n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "        _ = model.features(input_tensor)\n",
        "\n",
        "    hook.remove()\n",
        "\n",
        "    # Process activations\n",
        "    activations = activation['conv'][0].cpu().numpy()  # [C, D, H, W]\n",
        "    attention_map = np.mean(activations, axis=0)  # Average across channels\n",
        "    attention_map = (attention_map - attention_map.min()) / (attention_map.max() - attention_map.min())\n",
        "\n",
        "    # Get original data from dataset\n",
        "    original_vol = pixel_values.numpy().transpose(1, 0, 2, 3)  # [C, D, H, W]\n",
        "\n",
        "    # Resize attention map to match original volume if needed\n",
        "    if attention_map.shape != original_vol.shape[1:]:\n",
        "        zoom_factors = [\n",
        "            original_vol.shape[1]/attention_map.shape[0],\n",
        "            original_vol.shape[2]/attention_map.shape[1],\n",
        "            original_vol.shape[3]/attention_map.shape[2]\n",
        "        ]\n",
        "        attention_map = ndimage.zoom(attention_map, zoom_factors, order=1)\n",
        "\n",
        "    # Create overlay frames\n",
        "    frames = []\n",
        "    for z in range(original_vol.shape[1]):\n",
        "        fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "        # Original slice\n",
        "        ax[0].imshow(original_vol[0, z], cmap='gray')\n",
        "        ax[0].set_title(f'Original Slice Z{z}\\n{bbox_name}')\n",
        "        ax[0].axis('off')\n",
        "\n",
        "        # Overlay attention\n",
        "        ax[1].imshow(original_vol[0, z], cmap='gray')\n",
        "        im = ax[1].imshow(attention_map[z], cmap='jet', alpha=0.5)\n",
        "        plt.colorbar(im, ax=ax[1], fraction=0.046, pad=0.04)\n",
        "        ax[1].set_title(f'Attention Map (Layer {layer_index})\\n{syn_info[\"Var1\"]}')\n",
        "        ax[1].axis('off')\n",
        "\n",
        "        # Save frame\n",
        "        fig.canvas.draw()\n",
        "        frame = np.array(fig.canvas.renderer._renderer)\n",
        "        frames.append(frame)\n",
        "        plt.close(fig)\n",
        "\n",
        "    # Save GIF with metadata in filename\n",
        "    gif_name = f\"{bbox_name}_syn{syn_info['Var1']}_layer{layer_index}.gif\"\n",
        "    gif_path = os.path.join(output_dir, gif_name)\n",
        "    imageio.mimsave(gif_path, frames, duration=0.2)\n",
        "\n",
        "    return gif_path, syn_info\n",
        "\n",
        "# def visualize_multiple_samples(model, dataset, num_samples=5):\n",
        "#     \"\"\"Visualize attention for multiple random samples\"\"\"\n",
        "#     results = []\n",
        "#     for _ in tqdm(range(num_samples)):\n",
        "#         # Random sample respecting bbox boundaries\n",
        "#         sample_idx = np.random.randint(0, len(dataset))\n",
        "#         gif_path, syn_info = visualize_sample_attention(model, dataset, sample_idx)\n",
        "#         results.append((gif_path, syn_info))\n",
        "#     return results\n",
        "\n",
        "# Visualize first 5 samples\n",
        "# results = visualize_multiple_samples(model, dataset, num_samples=5)\n",
        "\n",
        "# Print results\n",
        "for gif_path, syn_info in results:\n",
        "    print(f\"Generated visualization for {syn_info['bbox_name']} synapse {syn_info['Var1']}\")\n",
        "    print(f\"GIF saved at: {gif_path}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNldeTQm0uxf",
        "outputId": "7bc2ded0-8e5b-428a-ae9f-0a2fa2bbfeb6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:57<00:00, 11.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated visualization for bbox1 synapse non_spine_synapse_023\n",
            "GIF saved at: attention_gifs/bbox1_synnon_spine_synapse_023_layer30.gif\n",
            "\n",
            "Generated visualization for bbox3 synapse non_spine_synapse_005\n",
            "GIF saved at: attention_gifs/bbox3_synnon_spine_synapse_005_layer30.gif\n",
            "\n",
            "Generated visualization for bbox1 synapse non_spine_synapse_052\n",
            "GIF saved at: attention_gifs/bbox1_synnon_spine_synapse_052_layer30.gif\n",
            "\n",
            "Generated visualization for bbox1 synapse non_spine_synapse_048\n",
            "GIF saved at: attention_gifs/bbox1_synnon_spine_synapse_048_layer30.gif\n",
            "\n",
            "Generated visualization for bbox5 synapse non_spine_synapse_011\n",
            "GIF saved at: attention_gifs/bbox5_synnon_spine_synapse_011_layer30.gif\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def visualize_multiple_samples(model, dataset, num_samples=5, layer_index=33):\n",
        "    \"\"\"Visualize attention for multiple random samples with layer index\"\"\"\n",
        "    results = []\n",
        "    for _ in tqdm(range(num_samples)):\n",
        "        sample_idx = np.random.randint(0, len(dataset))\n",
        "        gif_path, syn_info = visualize_sample_attention(\n",
        "            model, dataset, sample_idx, layer_index=layer_index\n",
        "        )\n",
        "        results.append((gif_path, syn_info))\n",
        "    return results\n",
        "\n",
        "# ... [Your existing dataset preparation code] ...\n",
        "\n",
        "# Main analysis loops\n",
        "def run_visualization_analysis(model, args, vol_data_dict, syn_df, processor):\n",
        "    # Original dataset configuration\n",
        "    base_dataset = VideoMAEDataset(\n",
        "        vol_data_dict=vol_data_dict,\n",
        "        synapse_df=syn_df,\n",
        "        processor=processor,\n",
        "        segmentation_type=args.segmentation_type,\n",
        "        subvol_size=args.subvol_size,\n",
        "        num_frames=args.num_frames,\n",
        "        alpha=args.alpha,\n",
        "        input_mask=args.input_mask\n",
        "    )\n",
        "\n",
        "    # 1. Different segmentation types\n",
        "    for seg_type in [0, 1, 2, 3, 4, 5]:\n",
        "        args.segmentation_type = seg_type\n",
        "        seg_dataset = VideoMAEDataset(\n",
        "            vol_data_dict=vol_data_dict,\n",
        "            synapse_df=syn_df,\n",
        "            processor=processor,\n",
        "            segmentation_type=seg_type,\n",
        "            subvol_size=args.subvol_size,\n",
        "            num_frames=args.num_frames,\n",
        "            alpha=args.alpha,\n",
        "            input_mask=args.input_mask\n",
        "        )\n",
        "        print(f\"\\nVisualizing segmentation type {seg_type}\")\n",
        "        _ = visualize_multiple_samples(model, seg_dataset, num_samples=3)\n",
        "\n",
        "    # 2. Input masking comparison\n",
        "    for use_mask in [True, False]:\n",
        "        args.input_mask = use_mask\n",
        "        mask_dataset = VideoMAEDataset(\n",
        "            vol_data_dict=vol_data_dict,\n",
        "            synapse_df=syn_df,\n",
        "            processor=processor,\n",
        "            segmentation_type=args.segmentation_type,\n",
        "            subvol_size=args.subvol_size,\n",
        "            num_frames=args.num_frames,\n",
        "            alpha=args.alpha,\n",
        "            input_mask=use_mask\n",
        "        )\n",
        "        print(f\"\\nVisualizing with input_mask={use_mask}\")\n",
        "        _ = visualize_multiple_samples(model, mask_dataset, num_samples=3)\n",
        "\n",
        "    # 3. Different layer indices\n",
        "    for layer_idx in [12, 19, 26, 33]:\n",
        "        print(f\"\\nVisualizing layer {layer_idx}\")\n",
        "        _ = visualize_multiple_samples(model, base_dataset, num_samples=3, layer_index=layer_idx)\n",
        "\n",
        "# Run the analysis\n",
        "run_visualization_analysis(model, args, vol_data_dict, syn_df, processor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KovOHiwh5kRa",
        "outputId": "6634111b-2588-4755-d4a2-78dbc9fc0582"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Visualizing segmentation type 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:34<00:00, 11.65s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Visualizing segmentation type 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:32<00:00, 10.93s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Visualizing segmentation type 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:34<00:00, 11.38s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Visualizing segmentation type 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:33<00:00, 11.06s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Visualizing segmentation type 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:34<00:00, 11.37s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Visualizing segmentation type 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:30<00:00, 10.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Visualizing with input_mask=True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:34<00:00, 11.65s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Visualizing with input_mask=False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:36<00:00, 12.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Visualizing layer 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:31<00:00, 10.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Visualizing layer 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:37<00:00, 12.35s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Visualizing layer 26\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:30<00:00, 10.24s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Visualizing layer 33\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:30<00:00, 10.29s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: download /content/attention_gifs this folder\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Create a zip archive of the folder\n",
        "shutil.make_archive(\"attention_gifs\", 'zip', \"/content/attention_gifs6\")\n",
        "\n",
        "# # Download the zip archive\n",
        "from google.colab import files\n",
        "files.download(\"attention_gifs.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "o9i7ES9h7QiM",
        "outputId": "f6ef02f5-8159-4629-e1cc-1f7340fb4bf3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6128c195-1456-417b-9422-a41461524a4e\", \"attention_gifs.zip\", 169573117)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!cp attention_gifs.zip /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "CKvCPrYUYX_Y"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYz0dqWbYfUK",
        "outputId": "70beb3c3-23e7-4867-f642-b121f5b4b555"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## fixed samples final grad"
      ],
      "metadata": {
        "id": "vc2xOtmTls9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import ndimage\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "def visualize_sample_attention(model, dataset, sample_idx=0, output_dir=\"attention_gifs6\", layer_index=30):\n",
        "    \"\"\"\n",
        "    Visualize attention for a specific sample with metadata in title/filename\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device).eval()\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Get sample data\n",
        "    pixel_values, syn_info, bbox_name = dataset[sample_idx]\n",
        "\n",
        "    # Forward pass setup\n",
        "    input_tensor = pixel_values.permute(1, 0, 2, 3).unsqueeze(0).to(device)\n",
        "    activation = {}\n",
        "\n",
        "    # Hook setup\n",
        "    def get_activation(name):\n",
        "        def hook(model, input, output):\n",
        "            activation[name] = output.detach()\n",
        "        return hook\n",
        "\n",
        "    hook = model.features[layer_index].register_forward_hook(get_activation('conv'))\n",
        "\n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "        _ = model.features(input_tensor)\n",
        "    hook.remove()\n",
        "\n",
        "    # Process activations\n",
        "    activations = activation['conv'][0].cpu().numpy()\n",
        "    attention_map = np.mean(activations, axis=0)\n",
        "    attention_map = (attention_map - attention_map.min()) / (attention_map.max() - attention_map.min())\n",
        "\n",
        "    # Get dataset parameters\n",
        "    seg_type = dataset.segmentation_type\n",
        "    use_mask = dataset.input_mask\n",
        "    original_vol = pixel_values.numpy().transpose(1, 0, 2, 3)\n",
        "\n",
        "    # Resize attention map if needed\n",
        "    if attention_map.shape != original_vol.shape[1:]:\n",
        "        zoom_factors = [\n",
        "            original_vol.shape[1]/attention_map.shape[0],\n",
        "            original_vol.shape[2]/attention_map.shape[1],\n",
        "            original_vol.shape[3]/attention_map.shape[2]\n",
        "        ]\n",
        "        attention_map = ndimage.zoom(attention_map, zoom_factors, order=1)\n",
        "\n",
        "    # Create frames with metadata\n",
        "    frames = []\n",
        "    for z in range(original_vol.shape[1]):\n",
        "        fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "        # Original slice\n",
        "        ax[0].imshow(original_vol[0, z], cmap='gray')\n",
        "        ax[0].set_title(f'Original Slice Z{z}\\n{bbox_name}')\n",
        "        ax[0].axis('off')\n",
        "\n",
        "        # Attention overlay\n",
        "        ax[1].imshow(original_vol[0, z], cmap='gray')\n",
        "        im = ax[1].imshow(attention_map[z], cmap='jet', alpha=0.5)\n",
        "        plt.colorbar(im, ax=ax[1], fraction=0.046, pad=0.04)\n",
        "        title = (\n",
        "            f\"Attention Map (Layer {layer_index}, Seg {seg_type}, Mask {use_mask})\\n\"\n",
        "            f\"{syn_info['Var1']}\"\n",
        "        )\n",
        "        ax[1].set_title(title)\n",
        "        ax[1].axis('off')\n",
        "\n",
        "        # Save frame\n",
        "        fig.canvas.draw()\n",
        "        frames.append(np.array(fig.canvas.renderer._renderer))\n",
        "        plt.close(fig)\n",
        "\n",
        "    # Save GIF with comprehensive filename\n",
        "    gif_name = (\n",
        "        f\"{bbox_name}_syn{syn_info['Var1']}_\"\n",
        "        f\"layer{layer_index}_seg{seg_type}_mask{use_mask}.gif\"\n",
        "    )\n",
        "    gif_path = os.path.join(output_dir, gif_name)\n",
        "    imageio.mimsave(gif_path, frames, duration=0.2)\n",
        "\n",
        "    return gif_path, syn_info\n",
        "\n",
        "# def visualize_multiple_samples(model, dataset, num_samples=5, layer_index=30):\n",
        "#     \"\"\"Visualize samples ensuring one per B-box\"\"\"\n",
        "#     results = []\n",
        "#     bbox_list = ['bbox1','bbox2','bbox3','bbox4','bbox5','bbox6', ]\n",
        "#     np.random.shuffle(bbox_list)\n",
        "\n",
        "#     for bbox_name in bbox_list[:num_samples]:\n",
        "#         indices = dataset.bbox_to_indices[bbox_name]\n",
        "#         sample_idx = np.random.choice(indices)\n",
        "#         gif_path, syn_info = visualize_sample_attention(\n",
        "#             model, dataset, sample_idx, layer_index=layer_index\n",
        "#         )\n",
        "#         results.append((gif_path, syn_info))\n",
        "\n",
        "#     return results\n",
        "# def visualize_multiple_samples(model, dataset, num_samples=5, layer_index=30):\n",
        "#     \"\"\"Visualize samples ensuring one per B-box\"\"\"\n",
        "#     results = []\n",
        "#     bbox_list = ['bbox1', 'bbox2', 'bbox3', 'bbox4', 'bbox5', 'bbox6']\n",
        "#     np.random.shuffle(bbox_list)\n",
        "\n",
        "#     for bbox_name in tqdm(bbox_list[:num_samples]):\n",
        "#         # Instead of looking up indices, we'll find samples that match the bbox_name\n",
        "#         sample_indices = []\n",
        "#         for i in range(len(dataset)):\n",
        "#             _, _, sample_bbox_name = dataset[i]\n",
        "#             if sample_bbox_name == bbox_name:\n",
        "#                 sample_indices.append(i)\n",
        "\n",
        "#         if sample_indices:\n",
        "#             sample_idx = np.random.choice(sample_indices)\n",
        "#             gif_path, syn_info = visualize_sample_attention(\n",
        "#                 model, dataset, sample_idx, layer_index=layer_index\n",
        "#             )\n",
        "#             results.append((gif_path, syn_info))\n",
        "#         else:\n",
        "#             print(f\"No samples found for {bbox_name}\")\n",
        "\n",
        "#     return results\n",
        "# sample_idx = np.random.randint(0, len(dataset))\n",
        "# sample_idx2 = np.random.randint(0, len(dataset))\n",
        "# sample_idx3 = np.random.randint(0, len(dataset))\n",
        "sample_idx_list = [np.random.randint(0, len(dataset)) for _ in range(3)]\n",
        "def visualize_multiple_samples(model, dataset,  layer_index=33,sample_idx=sample_idx_list):\n",
        "    \"\"\"Visualize attention for multiple random samples without B-box sampling\"\"\"\n",
        "    results = []\n",
        "    for i in tqdm(range(len(sample_idx_list))):\n",
        "        # Randomly select a sample index\n",
        "        # sample_idx = np.random.randint(0, len(dataset))\n",
        "        gif_path, syn_info = visualize_sample_attention(\n",
        "            model, dataset, sample_idx_list[i], layer_index=layer_index\n",
        "        )\n",
        "        results.append((gif_path, syn_info))\n",
        "    return results\n",
        "\n",
        "def run_visualization_analysis(model, args, vol_data_dict, syn_df, processor):\n",
        "    \"\"\"Main analysis with correct parameter combinations\"\"\"\n",
        "    # Segmentation types 0-5 with mask=False\n",
        "    for seg_type in [0, 3, 4, 5]:\n",
        "        dataset = VideoMAEDataset(\n",
        "            vol_data_dict=vol_data_dict,\n",
        "            synapse_df=syn_df,\n",
        "            processor=processor,\n",
        "            segmentation_type=seg_type,\n",
        "            subvol_size=args.subvol_size,\n",
        "            num_frames=args.num_frames,\n",
        "            alpha=args.alpha,\n",
        "            input_mask=False\n",
        "        )\n",
        "        print(f\"\\nVisualizing seg_type={seg_type}, mask=False\")\n",
        "        _ = visualize_multiple_samples(model, dataset)\n",
        "\n",
        "    # Segmentation types 1-5 with mask=True\n",
        "    for seg_type in [1,  4, 5]:\n",
        "        dataset = VideoMAEDataset(\n",
        "            vol_data_dict=vol_data_dict,\n",
        "            synapse_df=syn_df,\n",
        "            processor=processor,\n",
        "            segmentation_type=seg_type,\n",
        "            subvol_size=args.subvol_size,\n",
        "            num_frames=args.num_frames,\n",
        "            alpha=args.alpha,\n",
        "            input_mask=True\n",
        "        )\n",
        "        print(f\"\\nVisualizing seg_type={seg_type}, mask=True\")\n",
        "        _ = visualize_multiple_samples(model, dataset)\n",
        "\n",
        "    # Layer analysis with base configuration\n",
        "    base_dataset = VideoMAEDataset(\n",
        "        vol_data_dict=vol_data_dict,\n",
        "        synapse_df=syn_df,\n",
        "        processor=processor,\n",
        "        segmentation_type=args.segmentation_type,\n",
        "        subvol_size=args.subvol_size,\n",
        "        num_frames=args.num_frames,\n",
        "        alpha=args.alpha,\n",
        "        input_mask=args.input_mask\n",
        "    )\n",
        "    for layer_idx in [12, 19, 26, 33]:\n",
        "        print(f\"\\nVisualizing layer {layer_idx}\")\n",
        "        _ = visualize_multiple_samples(model, base_dataset, layer_index=layer_idx)\n",
        "\n",
        "run_visualization_analysis(model, args, vol_data_dict, syn_df, processor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLFyjKSk_tDY",
        "outputId": "276f51a3-4e0a-4d13-80fc-59359b8388de"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Visualizing seg_type=0, mask=False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:45<00:00, 15.31s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Visualizing seg_type=3, mask=False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:36<00:00, 12.19s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Visualizing seg_type=4, mask=False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:38<00:00, 12.76s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Visualizing seg_type=5, mask=False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:40<00:00, 13.62s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Visualizing seg_type=1, mask=True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:41<00:00, 13.81s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Visualizing seg_type=4, mask=True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:42<00:00, 14.06s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Visualizing seg_type=5, mask=True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:38<00:00, 12.78s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Visualizing layer 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:44<00:00, 14.96s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Visualizing layer 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:38<00:00, 12.99s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Visualizing layer 26\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:45<00:00, 15.02s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Visualizing layer 33\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:38<00:00, 12.91s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RWQsPUFM_-5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VGG Example\n"
      ],
      "metadata": {
        "id": "fyxp7ZMWQgfo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1Data"
      ],
      "metadata": {
        "id": "09dpZLkdZLwo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just Uncomment either 1.1 or 1.2 for data loading"
      ],
      "metadata": {
        "id": "hRxENzoKXrZ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Dark background (sectoins that have not segmentaions be 0)"
      ],
      "metadata": {
        "id": "lf_jtchGXcJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import io\n",
        "import argparse\n",
        "import multiprocessing\n",
        "from typing import List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import imageio.v3 as iio\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision import transforms\n",
        "class SimpleVideoProcessor:\n",
        "    def __init__(self, size=(80, 80), mean=(0.485,),  # Use a single channel for grayscale\n",
        "                 std=(0.229,)):\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize(size),\n",
        "            transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=mean, std=std),\n",
        "        ])\n",
        "\n",
        "    def __call__(self, frames, return_tensors=None):\n",
        "        processed_frames = [self.transform(frame) for frame in frames]\n",
        "        pixel_values = torch.stack(processed_frames)\n",
        "        if return_tensors == \"pt\":\n",
        "            return {\"pixel_values\": pixel_values}\n",
        "        else:\n",
        "            return pixel_values\n",
        "\n",
        "\n",
        "def load_volumes(bbox_name: str, raw_base_dir: str, seg_base_dir: str, add_mask_base_dir: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    raw_dir = os.path.join(raw_base_dir, bbox_name)\n",
        "    seg_dir = os.path.join(seg_base_dir, bbox_name)\n",
        "\n",
        "    if bbox_name.startswith(\"bbox\"):\n",
        "        bbox_num = bbox_name.replace(\"bbox\", \"\")\n",
        "        add_mask_dir = os.path.join(add_mask_base_dir, f\"bbox_{bbox_num}\")\n",
        "    else:\n",
        "        add_mask_dir = os.path.join(add_mask_base_dir, bbox_name)\n",
        "\n",
        "    raw_tif_files = sorted(glob.glob(os.path.join(raw_dir, 'slice_*.tif')))\n",
        "    seg_tif_files = sorted(glob.glob(os.path.join(seg_dir, 'slice_*.tif')))\n",
        "    add_mask_tif_files = sorted(glob.glob(os.path.join(add_mask_dir, 'slice_*.tif')))\n",
        "\n",
        "    if not (len(raw_tif_files) == len(seg_tif_files) == len(add_mask_tif_files)):\n",
        "        return None, None, None\n",
        "\n",
        "    try:\n",
        "        raw_vol = np.stack([iio.imread(f) for f in raw_tif_files], axis=0)\n",
        "        seg_vol = np.stack([iio.imread(f).astype(np.uint32) for f in seg_tif_files], axis=0)\n",
        "        add_mask_vol = np.stack([iio.imread(f).astype(np.uint32) for f in add_mask_tif_files], axis=0)\n",
        "        return raw_vol, seg_vol, add_mask_vol\n",
        "    except Exception as e:\n",
        "        return None, None, None\n",
        "\n",
        "def create_segmented_cube(\n",
        "    raw_vol: np.ndarray,\n",
        "    seg_vol: np.ndarray,\n",
        "    add_mask_vol: np.ndarray,\n",
        "    central_coord: Tuple[int, int, int],\n",
        "    side1_coord: Tuple[int, int, int],\n",
        "    side2_coord: Tuple[int, int, int],\n",
        "    segmentation_type: int,\n",
        "    subvolume_size: int = 80,\n",
        "    alpha: float = 0.3,\n",
        "    input_mask: bool = False  # New parameter\n",
        ") -> np.ndarray:\n",
        "    def create_segment_masks(segmentation_volume, s1_coord, s2_coord):\n",
        "        x1, y1, z1 = s1_coord\n",
        "        x2, y2, z2 = s2_coord\n",
        "        seg_id_1 = segmentation_volume[z1, y1, x1]\n",
        "        seg_id_2 = segmentation_volume[z2, y2, x2]\n",
        "\n",
        "        mask_1 = (segmentation_volume == seg_id_1) if seg_id_1 != 0 else np.zeros_like(segmentation_volume, dtype=bool)\n",
        "        mask_2 = (segmentation_volume == seg_id_2) if seg_id_2 != 0 else np.zeros_like(segmentation_volume, dtype=bool)\n",
        "        return mask_1, mask_2\n",
        "\n",
        "    mask_1_full, mask_2_full = create_segment_masks(seg_vol, side1_coord, side2_coord)\n",
        "    mask_3_full = (add_mask_vol > 0)\n",
        "\n",
        "    half_size = subvolume_size // 2\n",
        "    cx, cy, cz = central_coord\n",
        "    x_start, x_end = max(cx - half_size, 0), min(cx + half_size, raw_vol.shape[2])\n",
        "    y_start, y_end = max(cy - half_size, 0), min(cy + half_size, raw_vol.shape[1])\n",
        "    z_start, z_end = max(cz - half_size, 0), min(cz + half_size, raw_vol.shape[0])\n",
        "\n",
        "    sub_raw = raw_vol[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "    sub_mask_1 = mask_1_full[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "    sub_mask_2 = mask_2_full[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "    sub_mask_3 = mask_3_full[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "\n",
        "    pad_z = subvolume_size - sub_raw.shape[0]\n",
        "    pad_y = subvolume_size - sub_raw.shape[1]\n",
        "    pad_x = subvolume_size - sub_raw.shape[2]\n",
        "\n",
        "    if pad_z > 0 or pad_y > 0 or pad_x > 0:\n",
        "        sub_raw = np.pad(sub_raw, ((0, pad_z), (0, pad_y), (0, pad_x)), mode='constant', constant_values=0)\n",
        "        sub_mask_1 = np.pad(sub_mask_1, ((0, pad_z), (0, pad_y), (0, pad_x)), mode='constant', constant_values=False)\n",
        "        sub_mask_2 = np.pad(sub_mask_2, ((0, pad_z), (0, pad_y), (0, pad_x)), mode='constant', constant_values=False)\n",
        "        sub_mask_3 = np.pad(sub_mask_3, ((0, pad_z), (0, pad_y), (0, pad_x)), mode='constant', constant_values=False)\n",
        "\n",
        "    sub_raw = sub_raw[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "    sub_mask_1 = sub_mask_1[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "    sub_mask_2 = sub_mask_2[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "    sub_mask_3 = sub_mask_3[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "\n",
        "    overlaid_cube = np.zeros((subvolume_size, subvolume_size, 3, subvolume_size), dtype=np.uint8)\n",
        "\n",
        "    for z in range(subvolume_size):\n",
        "        raw_slice = sub_raw[z].astype(np.float32)\n",
        "        mn, mx = raw_slice.min(), raw_slice.max()\n",
        "        if mx > mn:\n",
        "            raw_slice = (raw_slice - mn) / (mx - mn)\n",
        "        else:\n",
        "            raw_slice = raw_slice - mn\n",
        "\n",
        "        if input_mask:  # New masking logic\n",
        "            combined_mask = np.zeros_like(raw_slice, dtype=np.float32)\n",
        "            if segmentation_type in [1, 3, 5]:\n",
        "                combined_mask = np.logical_or(combined_mask, sub_mask_1[z])\n",
        "            if segmentation_type in [2, 3, 5]:\n",
        "                combined_mask = np.logical_or(combined_mask, sub_mask_2[z])\n",
        "            if segmentation_type in [4, 5]:\n",
        "                combined_mask = np.logical_or(combined_mask, sub_mask_3[z])\n",
        "\n",
        "            masked_raw = raw_slice * combined_mask\n",
        "            masked_rgb = np.stack([masked_raw]*3, axis=-1)\n",
        "            overlaid_image = (masked_rgb * 255).astype(np.uint8)\n",
        "        else:  # Original overlay logic\n",
        "            raw_rgb = np.stack([raw_slice]*3, axis=-1)\n",
        "            mask1_rgb = np.zeros_like(raw_rgb)\n",
        "            mask2_rgb = np.zeros_like(raw_rgb)\n",
        "            mask3_rgb = np.zeros_like(raw_rgb)\n",
        "\n",
        "            if segmentation_type in [1, 3, 5]:\n",
        "                mask1_rgb[sub_mask_1[z]] = [1, 0, 0]\n",
        "            if segmentation_type in [2, 3, 5]:\n",
        "                mask2_rgb[sub_mask_2[z]] = [0, 0, 1]\n",
        "            if segmentation_type in [4, 5]:\n",
        "                mask3_rgb[sub_mask_3[z]] = [0, 1, 0]\n",
        "\n",
        "            combined_masks = mask1_rgb + mask2_rgb + mask3_rgb\n",
        "            combined_masks = np.clip(combined_masks, 0, 1)\n",
        "            overlaid_image = (1 - alpha) * raw_rgb + alpha * combined_masks\n",
        "            overlaid_image = (np.clip(overlaid_image, 0, 1) * 255).astype(np.uint8)\n",
        "\n",
        "        overlaid_cube[:, :, :, z] = overlaid_image\n",
        "\n",
        "    return overlaid_cube\n",
        "\n",
        "class VideoMAEDataset(Dataset):\n",
        "    def __init__(self, vol_data_dict: dict, synapse_df: pd.DataFrame, processor,\n",
        "                 segmentation_type: int, subvol_size: int = 80, num_frames: int = 16,\n",
        "                 alpha: float = 0.3, input_mask: bool = False):  # Added input_mask\n",
        "        self.vol_data_dict = vol_data_dict\n",
        "        self.synapse_df = synapse_df.reset_index(drop=True)\n",
        "        self.processor = processor\n",
        "        self.segmentation_type = segmentation_type\n",
        "        self.subvol_size = subvol_size\n",
        "        self.num_frames = num_frames\n",
        "        self.alpha = alpha\n",
        "        self.input_mask = input_mask  # Store input_mask flag\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.synapse_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        syn_info = self.synapse_df.iloc[idx]\n",
        "        bbox_name = syn_info['bbox_name']\n",
        "        raw_vol, seg_vol, add_mask_vol = self.vol_data_dict.get(bbox_name, (None, None, None))\n",
        "\n",
        "        if raw_vol is None:\n",
        "            return torch.zeros((self.num_frames, 3, self.subvol_size, self.subvol_size), dtype=torch.float32), syn_info, bbox_name\n",
        "\n",
        "        central_coord = (int(syn_info['central_coord_1']), int(syn_info['central_coord_2']), int(syn_info['central_coord_3']))\n",
        "        side1_coord = (int(syn_info['side_1_coord_1']), int(syn_info['side_1_coord_2']), int(syn_info['side_1_coord_3']))\n",
        "        side2_coord = (int(syn_info['side_2_coord_1']), int(syn_info['side_2_coord_2']), int(syn_info['side_2_coord_3']))\n",
        "\n",
        "        overlaid_cube = create_segmented_cube(\n",
        "            raw_vol=raw_vol,\n",
        "            seg_vol=seg_vol,\n",
        "            add_mask_vol=add_mask_vol,\n",
        "            central_coord=central_coord,\n",
        "            side1_coord=side1_coord,\n",
        "            side2_coord=side2_coord,\n",
        "            segmentation_type=self.segmentation_type,\n",
        "            subvolume_size=self.subvol_size,\n",
        "            alpha=self.alpha,\n",
        "            input_mask=self.input_mask  # Pass the flag\n",
        "        )\n",
        "\n",
        "        frames = [overlaid_cube[..., z] for z in range(overlaid_cube.shape[3])]\n",
        "        if len(frames) < self.num_frames:\n",
        "            frames += [frames[-1]] * (self.num_frames - len(frames))\n",
        "        elif len(frames) > self.num_frames:\n",
        "            indices = np.linspace(0, len(frames)-1, self.num_frames, dtype=int)\n",
        "            frames = [frames[i] for i in indices]\n",
        "\n",
        "        inputs = self.processor(frames, return_tensors=\"pt\")\n",
        "        return inputs[\"pixel_values\"].squeeze(0).float(), syn_info, bbox_name\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"VideoMAE Pre-training Script with Segmented Videos and Additional Masks\")\n",
        "    parser.add_argument('--raw_base_dir', type=str, default='raw')\n",
        "    parser.add_argument('--seg_base_dir', type=str, default='seg')\n",
        "    parser.add_argument('--add_mask_base_dir', type=str, default='')\n",
        "    parser.add_argument('--bbox_name', type=str, default=['bbox1','bbox2','bbox3','bbox4','bbox5','bbox6', ], nargs='+')\n",
        "    parser.add_argument('--excel_file', type=str, default='')\n",
        "    parser.add_argument('--csv_output_dir', type=str, default='csv_outputs')\n",
        "    parser.add_argument('--checkpoint_dir', type=str, default='checkpoints')\n",
        "    parser.add_argument('--log_dir', type=str, default='logs')\n",
        "    parser.add_argument('--size', type=tuple, default=(80,80))\n",
        "    parser.add_argument('--batch_size', type=int, default=2)\n",
        "    parser.add_argument('--num_epochs', type=int, default=5)\n",
        "    parser.add_argument('--learning_rate', type=float, default=1e-4)\n",
        "    parser.add_argument('--weight_decay', type=float, default=1e-2)\n",
        "    parser.add_argument('--subvol_size', type=int, default=80)\n",
        "    parser.add_argument('--num_frames', type=int, default=80)\n",
        "    parser.add_argument('--mask_ratio', type=float, default=0.75)\n",
        "    parser.add_argument('--patience', type=int, default=3)\n",
        "    parser.add_argument('--resume_checkpoint', type=str, default=None)\n",
        "    parser.add_argument('--save_gifs_dir', type=str, default='gifs')\n",
        "    parser.add_argument('--num_gifs', type=int, default=10)\n",
        "    parser.add_argument('--alpha', type=float, default=0.3)\n",
        "    parser.add_argument('--segmentation_type', type=int, default=0, choices=range(0, 6))\n",
        "    parser.add_argument('--input_mask', action='store_true', # Added argument\n",
        "                       help='Mask input image using segmentation_type regions')\n",
        "    args, _ = parser.parse_known_args()\n",
        "\n",
        "    return args\n",
        "\n",
        "def main(args):\n",
        "    processor = SimpleVideoProcessor(size=(80, 80))\n",
        "    vol_data_dict = {}\n",
        "\n",
        "    for bbox_name in args.bbox_name:\n",
        "        raw_vol, seg_vol, add_mask_vol = load_volumes(\n",
        "            bbox_name=bbox_name,\n",
        "            raw_base_dir=args.raw_base_dir,\n",
        "            seg_base_dir=args.seg_base_dir,\n",
        "            add_mask_base_dir=args.add_mask_base_dir\n",
        "        )\n",
        "        if raw_vol is not None:\n",
        "            vol_data_dict[bbox_name] = (raw_vol, seg_vol, add_mask_vol)\n",
        "\n",
        "    synapse_dfs = []\n",
        "    for bbox_name in args.bbox_name:\n",
        "        excel_path = os.path.join(args.excel_file, f\"{bbox_name}.xlsx\")\n",
        "        if os.path.exists(excel_path):\n",
        "            df = pd.read_excel(excel_path)\n",
        "            df['bbox_name'] = bbox_name\n",
        "            synapse_dfs.append(df)\n",
        "\n",
        "    syn_df = pd.concat(synapse_dfs, ignore_index=True)\n",
        "    dataset = VideoMAEDataset(\n",
        "        vol_data_dict=vol_data_dict,\n",
        "        synapse_df=syn_df,\n",
        "        processor=processor,\n",
        "        segmentation_type=args.segmentation_type,\n",
        "        subvol_size=args.subvol_size,\n",
        "        num_frames=args.num_frames,\n",
        "        alpha=args.alpha,\n",
        "        input_mask=args.input_mask  # Pass the flag\n",
        "    )\n",
        "\n",
        "    cubes = []\n",
        "    syn_info_list = []\n",
        "    for idx in range(len(dataset)):\n",
        "        pixel_values, syn_info, _ = dataset[idx]\n",
        "        cubes.append(pixel_values)\n",
        "        syn_info_list.append(syn_info)\n",
        "\n",
        "    print(f\"Processed {len(cubes)} cubes successfully.\")\n",
        "    return cubes, pd.DataFrame(syn_info_list)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     args = parse_args()\n",
        "#     cubes, sys_inf = main(args)\n",
        "#     print(f\"Final output: {len(cubes)} cubes\")\n",
        "\n",
        "\n",
        "# import torch\n",
        "# import numpy as np\n",
        "# import imageio\n",
        "\n",
        "# cube = cubes[0]\n",
        "# mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "# std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "\n",
        "# denormalized_cube = cube * std + mean\n",
        "\n",
        "# denormalized_cube = torch.clamp(denormalized_cube, 0, 1)\n",
        "# frames = denormalized_cube.permute(0, 2, 3, 1).numpy()  # Change to (T, H, W, C)\n",
        "# frames = (frames * 255).astype(np.uint8)  # Convert to 0-255\n",
        "\n",
        "# imageio.mimsave('synapse_cube.gif', frames, fps=10)\n",
        "\n",
        "# print(\"GIF saved successfully!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "LOPaOUse7KEd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Test Average (sections that have not seg. be avg pix)"
      ],
      "metadata": {
        "id": "GpyUIuC7Wfwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import io\n",
        "import argparse\n",
        "import multiprocessing\n",
        "from typing import List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import imageio.v3 as iio\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision import transforms\n",
        "class SimpleVideoProcessor:\n",
        "    def __init__(self, size=(80, 80), mean=(0.485,),  # Use a single channel for grayscale\n",
        "                 std=(0.229,)):\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize(size),\n",
        "            transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=mean, std=std),\n",
        "        ])\n",
        "\n",
        "    def __call__(self, frames, return_tensors=None):\n",
        "        processed_frames = [self.transform(frame) for frame in frames]\n",
        "        pixel_values = torch.stack(processed_frames)\n",
        "        if return_tensors == \"pt\":\n",
        "            return {\"pixel_values\": pixel_values}\n",
        "        else:\n",
        "            return pixel_values\n",
        "\n",
        "\n",
        "def load_volumes(bbox_name: str, raw_base_dir: str, seg_base_dir: str, add_mask_base_dir: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    raw_dir = os.path.join(raw_base_dir, bbox_name)\n",
        "    seg_dir = os.path.join(seg_base_dir, bbox_name)\n",
        "\n",
        "    if bbox_name.startswith(\"bbox\"):\n",
        "        bbox_num = bbox_name.replace(\"bbox\", \"\")\n",
        "        add_mask_dir = os.path.join(add_mask_base_dir, f\"bbox_{bbox_num}\")\n",
        "    else:\n",
        "        add_mask_dir = os.path.join(add_mask_base_dir, bbox_name)\n",
        "\n",
        "    raw_tif_files = sorted(glob.glob(os.path.join(raw_dir, 'slice_*.tif')))\n",
        "    seg_tif_files = sorted(glob.glob(os.path.join(seg_dir, 'slice_*.tif')))\n",
        "    add_mask_tif_files = sorted(glob.glob(os.path.join(add_mask_dir, 'slice_*.tif')))\n",
        "\n",
        "    if not (len(raw_tif_files) == len(seg_tif_files) == len(add_mask_tif_files)):\n",
        "        return None, None, None\n",
        "\n",
        "    try:\n",
        "        raw_vol = np.stack([iio.imread(f) for f in raw_tif_files], axis=0)\n",
        "        seg_vol = np.stack([iio.imread(f).astype(np.uint32) for f in seg_tif_files], axis=0)\n",
        "        add_mask_vol = np.stack([iio.imread(f).astype(np.uint32) for f in add_mask_tif_files], axis=0)\n",
        "        return raw_vol, seg_vol, add_mask_vol\n",
        "    except Exception as e:\n",
        "        return None, None, None\n",
        "def create_segmented_cube(\n",
        "    raw_vol: np.ndarray,\n",
        "    seg_vol: np.ndarray,\n",
        "    add_mask_vol: np.ndarray,\n",
        "    central_coord: Tuple[int, int, int],\n",
        "    side1_coord: Tuple[int, int, int],\n",
        "    side2_coord: Tuple[int, int, int],\n",
        "    segmentation_type: int,\n",
        "    subvolume_size: int = 80,\n",
        "    alpha: float = 0.3,\n",
        "    input_mask: bool = False  # New parameter\n",
        ") -> np.ndarray:\n",
        "    def create_segment_masks(segmentation_volume, s1_coord, s2_coord):\n",
        "        x1, y1, z1 = s1_coord\n",
        "        x2, y2, z2 = s2_coord\n",
        "        seg_id_1 = segmentation_volume[z1, y1, x1]\n",
        "        seg_id_2 = segmentation_volume[z2, y2, x2]\n",
        "\n",
        "        mask_1 = (segmentation_volume == seg_id_1) if seg_id_1 != 0 else np.zeros_like(segmentation_volume, dtype=bool)\n",
        "        mask_2 = (segmentation_volume == seg_id_2) if seg_id_2 != 0 else np.zeros_like(segmentation_volume, dtype=bool)\n",
        "        return mask_1, mask_2\n",
        "\n",
        "    mask_1_full, mask_2_full = create_segment_masks(seg_vol, side1_coord, side2_coord)\n",
        "    mask_3_full = (add_mask_vol > 0)\n",
        "\n",
        "    half_size = subvolume_size // 2\n",
        "    cx, cy, cz = central_coord\n",
        "    x_start, x_end = max(cx - half_size, 0), min(cx + half_size, raw_vol.shape[2])\n",
        "    y_start, y_end = max(cy - half_size, 0), min(cy + half_size, raw_vol.shape[1])\n",
        "    z_start, z_end = max(cz - half_size, 0), min(cz + half_size, raw_vol.shape[0])\n",
        "\n",
        "    sub_raw = raw_vol[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "    sub_mask_1 = mask_1_full[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "    sub_mask_2 = mask_2_full[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "    sub_mask_3 = mask_3_full[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "\n",
        "    pad_z = subvolume_size - sub_raw.shape[0]\n",
        "    pad_y = subvolume_size - sub_raw.shape[1]\n",
        "    pad_x = subvolume_size - sub_raw.shape[2]\n",
        "\n",
        "    if pad_z > 0 or pad_y > 0 or pad_x > 0:\n",
        "        sub_raw = np.pad(sub_raw, ((0, pad_z), (0, pad_y), (0, pad_x)), mode='constant', constant_values=0)\n",
        "        sub_mask_1 = np.pad(sub_mask_1, ((0, pad_z), (0, pad_y), (0, pad_x)), mode='constant', constant_values=False)\n",
        "        sub_mask_2 = np.pad(sub_mask_2, ((0, pad_z), (0, pad_y), (0, pad_x)), mode='constant', constant_values=False)\n",
        "        sub_mask_3 = np.pad(sub_mask_3, ((0, pad_z), (0, pad_y), (0, pad_x)), mode='constant', constant_values=False)\n",
        "\n",
        "    sub_raw = sub_raw[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "    sub_mask_1 = sub_mask_1[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "    sub_mask_2 = sub_mask_2[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "    sub_mask_3 = sub_mask_3[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "\n",
        "    overlaid_cube = np.zeros((subvolume_size, subvolume_size, 3, subvolume_size), dtype=np.uint8)\n",
        "\n",
        "    for z in range(subvolume_size):\n",
        "        raw_slice = sub_raw[z].astype(np.float32)\n",
        "        mn, mx = raw_slice.min(), raw_slice.max()\n",
        "        if mx > mn:\n",
        "            raw_slice = (raw_slice - mn) / (mx - mn)\n",
        "        else:\n",
        "            raw_slice = raw_slice - mn\n",
        "\n",
        "        if input_mask:  # New masking logic with average pixel filling\n",
        "            combined_mask = np.zeros_like(raw_slice, dtype=np.float32)\n",
        "            if segmentation_type in [1, 3, 5]:\n",
        "                combined_mask = np.logical_or(combined_mask, sub_mask_1[z])\n",
        "            if segmentation_type in [2, 3, 5]:\n",
        "                combined_mask = np.logical_or(combined_mask, sub_mask_2[z])\n",
        "            if segmentation_type in [4, 5]:\n",
        "                combined_mask = np.logical_or(combined_mask, sub_mask_3[z])\n",
        "\n",
        "            # Fill the non-segmented regions with the average of the segmented pixels\n",
        "            if np.any(~combined_mask):  # Non-masked areas\n",
        "                avg_value = np.mean(raw_slice[combined_mask == 0])  # Average of segmented pixels\n",
        "                raw_slice[combined_mask == 0] = avg_value  # Replace non-segmented areas with the average\n",
        "\n",
        "            # Convert to RGB (average pixel value in all channels)\n",
        "            masked_rgb = np.stack([raw_slice] * 3, axis=-1)\n",
        "            overlaid_image = (masked_rgb * 255).astype(np.uint8)\n",
        "        else:  # Original overlay logic\n",
        "            raw_rgb = np.stack([raw_slice] * 3, axis=-1)\n",
        "            mask1_rgb = np.zeros_like(raw_rgb)\n",
        "            mask2_rgb = np.zeros_like(raw_rgb)\n",
        "            mask3_rgb = np.zeros_like(raw_rgb)\n",
        "\n",
        "            if segmentation_type in [1, 3, 5]:\n",
        "                mask1_rgb[sub_mask_1[z]] = [1, 0, 0]\n",
        "            if segmentation_type in [2, 3, 5]:\n",
        "                mask2_rgb[sub_mask_2[z]] = [0, 0, 1]\n",
        "            if segmentation_type in [4, 5]:\n",
        "                mask3_rgb[sub_mask_3[z]] = [0, 1, 0]\n",
        "\n",
        "            combined_masks = mask1_rgb + mask2_rgb + mask3_rgb\n",
        "            combined_masks = np.clip(combined_masks, 0, 1)\n",
        "            overlaid_image = (1 - alpha) * raw_rgb + alpha * combined_masks\n",
        "            overlaid_image = (np.clip(overlaid_image, 0, 1) * 255).astype(np.uint8)\n",
        "\n",
        "        overlaid_cube[:, :, :, z] = overlaid_image\n",
        "\n",
        "    return overlaid_cube\n",
        "\n",
        "\n",
        "class VideoMAEDataset(Dataset):\n",
        "    def __init__(self, vol_data_dict: dict, synapse_df: pd.DataFrame, processor,\n",
        "                 segmentation_type: int, subvol_size: int = 80, num_frames: int = 16,\n",
        "                 alpha: float = 0.3, input_mask: bool = False):  # Added input_mask\n",
        "        self.vol_data_dict = vol_data_dict\n",
        "        self.synapse_df = synapse_df.reset_index(drop=True)\n",
        "        self.processor = processor\n",
        "        self.segmentation_type = segmentation_type\n",
        "        self.subvol_size = subvol_size\n",
        "        self.num_frames = num_frames\n",
        "        self.alpha = alpha\n",
        "        self.input_mask = input_mask  # Store input_mask flag\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.synapse_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        syn_info = self.synapse_df.iloc[idx]\n",
        "        bbox_name = syn_info['bbox_name']\n",
        "        raw_vol, seg_vol, add_mask_vol = self.vol_data_dict.get(bbox_name, (None, None, None))\n",
        "\n",
        "        if raw_vol is None:\n",
        "            return torch.zeros((self.num_frames, 3, self.subvol_size, self.subvol_size), dtype=torch.float32), syn_info, bbox_name\n",
        "\n",
        "        central_coord = (int(syn_info['central_coord_1']), int(syn_info['central_coord_2']), int(syn_info['central_coord_3']))\n",
        "        side1_coord = (int(syn_info['side_1_coord_1']), int(syn_info['side_1_coord_2']), int(syn_info['side_1_coord_3']))\n",
        "        side2_coord = (int(syn_info['side_2_coord_1']), int(syn_info['side_2_coord_2']), int(syn_info['side_2_coord_3']))\n",
        "\n",
        "        overlaid_cube = create_segmented_cube(\n",
        "            raw_vol=raw_vol,\n",
        "            seg_vol=seg_vol,\n",
        "            add_mask_vol=add_mask_vol,\n",
        "            central_coord=central_coord,\n",
        "            side1_coord=side1_coord,\n",
        "            side2_coord=side2_coord,\n",
        "            segmentation_type=self.segmentation_type,\n",
        "            subvolume_size=self.subvol_size,\n",
        "            alpha=self.alpha,\n",
        "            input_mask=self.input_mask  # Pass the flag\n",
        "        )\n",
        "\n",
        "        frames = [overlaid_cube[..., z] for z in range(overlaid_cube.shape[3])]\n",
        "        if len(frames) < self.num_frames:\n",
        "            frames += [frames[-1]] * (self.num_frames - len(frames))\n",
        "        elif len(frames) > self.num_frames:\n",
        "            indices = np.linspace(0, len(frames)-1, self.num_frames, dtype=int)\n",
        "            frames = [frames[i] for i in indices]\n",
        "\n",
        "        inputs = self.processor(frames, return_tensors=\"pt\")\n",
        "        return inputs[\"pixel_values\"].squeeze(0).float(), syn_info, bbox_name\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"VideoMAE Pre-training Script with Segmented Videos and Additional Masks\")\n",
        "    parser.add_argument('--raw_base_dir', type=str, default='raw')\n",
        "    parser.add_argument('--seg_base_dir', type=str, default='seg')\n",
        "    parser.add_argument('--add_mask_base_dir', type=str, default='')\n",
        "    parser.add_argument('--bbox_name', type=str, default=['bbox1','bbox2','bbox3','bbox4','bbox5','bbox6', ], nargs='+')\n",
        "    parser.add_argument('--excel_file', type=str, default='')\n",
        "    parser.add_argument('--csv_output_dir', type=str, default='csv_outputs')\n",
        "    parser.add_argument('--checkpoint_dir', type=str, default='checkpoints')\n",
        "    parser.add_argument('--log_dir', type=str, default='logs')\n",
        "    parser.add_argument('--size', type=tuple, default=(80,80))\n",
        "    parser.add_argument('--batch_size', type=int, default=2)\n",
        "    parser.add_argument('--num_epochs', type=int, default=5)\n",
        "    parser.add_argument('--learning_rate', type=float, default=1e-4)\n",
        "    parser.add_argument('--weight_decay', type=float, default=1e-2)\n",
        "    parser.add_argument('--subvol_size', type=int, default=80)\n",
        "    parser.add_argument('--num_frames', type=int, default=80)\n",
        "    parser.add_argument('--mask_ratio', type=float, default=0.75)\n",
        "    parser.add_argument('--patience', type=int, default=3)\n",
        "    parser.add_argument('--resume_checkpoint', type=str, default=None)\n",
        "    parser.add_argument('--save_gifs_dir', type=str, default='gifs')\n",
        "    parser.add_argument('--num_gifs', type=int, default=10)\n",
        "    parser.add_argument('--alpha', type=float, default=0.3)\n",
        "    parser.add_argument('--segmentation_type', type=int, default=5, choices=range(0, 6))\n",
        "    parser.add_argument('--input_mask', action='store_true', # Added argument\n",
        "                       help='Mask input image using segmentation_type regions')\n",
        "    args, _ = parser.parse_known_args()\n",
        "\n",
        "    return args\n",
        "\n",
        "def main(args):\n",
        "    processor = SimpleVideoProcessor(size=(80, 80))\n",
        "    vol_data_dict = {}\n",
        "\n",
        "    for bbox_name in args.bbox_name:\n",
        "        raw_vol, seg_vol, add_mask_vol = load_volumes(\n",
        "            bbox_name=bbox_name,\n",
        "            raw_base_dir=args.raw_base_dir,\n",
        "            seg_base_dir=args.seg_base_dir,\n",
        "            add_mask_base_dir=args.add_mask_base_dir\n",
        "        )\n",
        "        if raw_vol is not None:\n",
        "            vol_data_dict[bbox_name] = (raw_vol, seg_vol, add_mask_vol)\n",
        "\n",
        "    synapse_dfs = []\n",
        "    for bbox_name in args.bbox_name:\n",
        "        excel_path = os.path.join(args.excel_file, f\"{bbox_name}.xlsx\")\n",
        "        if os.path.exists(excel_path):\n",
        "            df = pd.read_excel(excel_path)\n",
        "            df['bbox_name'] = bbox_name\n",
        "            synapse_dfs.append(df)\n",
        "\n",
        "    syn_df = pd.concat(synapse_dfs, ignore_index=True)\n",
        "    dataset = VideoMAEDataset(\n",
        "        vol_data_dict=vol_data_dict,\n",
        "        synapse_df=syn_df,\n",
        "        processor=processor,\n",
        "        segmentation_type=args.segmentation_type,\n",
        "        subvol_size=args.subvol_size,\n",
        "        num_frames=args.num_frames,\n",
        "        alpha=args.alpha,\n",
        "        input_mask=args.input_mask  # Pass the flag\n",
        "    )\n",
        "\n",
        "    cubes = []\n",
        "    syn_info_list = []\n",
        "    for idx in range(len(dataset)):\n",
        "        pixel_values, syn_info, _ = dataset[idx]\n",
        "        cubes.append(pixel_values)\n",
        "        syn_info_list.append(syn_info)\n",
        "\n",
        "    print(f\"Processed {len(cubes)} cubes successfully.\")\n",
        "    return cubes, pd.DataFrame(syn_info_list)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     args = parse_args()\n",
        "#     args.input_mask=True\n",
        "#     cubes, sys_inf = main(args)\n",
        "#     print(f\"Final output: {len(cubes)} cubes\")\n",
        "\n",
        "# # Visualizeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee\n",
        "# import torch\n",
        "# import numpy as np\n",
        "# import imageio\n",
        "\n",
        "# cube = cubes[0]\n",
        "# mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "# std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "\n",
        "# denormalized_cube = cube * std + mean\n",
        "\n",
        "# denormalized_cube = torch.clamp(denormalized_cube, 0, 1)\n",
        "# frames = denormalized_cube.permute(0, 2, 3, 1).numpy()  # Change to (T, H, W, C)\n",
        "# frames = (frames * 255).astype(np.uint8)  # Convert to 0-255\n",
        "\n",
        "# imageio.mimsave('synapse_cube.gif', frames, fps=10)\n",
        "\n",
        "# print(\"GIF saved successfully!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "pyZu4WqtWhCe"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VGG Model and Feature Extracture"
      ],
      "metadata": {
        "id": "RfOZInSDZYXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "class Vgg3D(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=(80, 80, 80),\n",
        "        fmaps=24,\n",
        "        downsample_factors=[(2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2)],\n",
        "        fmap_inc=(2, 2, 2, 2),\n",
        "        n_convolutions=(4, 2, 2, 2),\n",
        "        output_classes=7,\n",
        "        input_fmaps=3,\n",
        "    ):\n",
        "        super(Vgg3D, self).__init__()\n",
        "\n",
        "        # Validate input parameters\n",
        "        if len(downsample_factors) != len(fmap_inc):\n",
        "            raise ValueError(\"fmap_inc needs to have same length as downsample factors\")\n",
        "        if len(n_convolutions) != len(fmap_inc):\n",
        "            raise ValueError(\"n_convolutions needs to have the same length as downsample factors\")\n",
        "        if np.any(np.array(n_convolutions) < 1):\n",
        "            raise ValueError(\"Each layer must have at least one convolution\")\n",
        "\n",
        "        current_fmaps = input_fmaps\n",
        "        current_size = np.array(input_size)\n",
        "\n",
        "        # Feature extraction layers\n",
        "        layers = []\n",
        "        for i, (df, nc) in enumerate(zip(downsample_factors, n_convolutions)):\n",
        "            # Convolution block\n",
        "            layers += [\n",
        "                nn.Conv3d(current_fmaps, fmaps, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm3d(fmaps),\n",
        "                nn.ReLU(inplace=True)\n",
        "            ]\n",
        "\n",
        "            # Additional convolutions\n",
        "            for _ in range(nc - 1):\n",
        "                layers += [\n",
        "                    nn.Conv3d(fmaps, fmaps, kernel_size=3, padding=1),\n",
        "                    nn.BatchNorm3d(fmaps),\n",
        "                    nn.ReLU(inplace=True)\n",
        "                ]\n",
        "\n",
        "            # Downsampling\n",
        "            layers.append(nn.MaxPool3d(df))\n",
        "\n",
        "            # Update feature map size\n",
        "            current_fmaps = fmaps\n",
        "            fmaps *= fmap_inc[i]\n",
        "\n",
        "            # Update spatial dimensions\n",
        "            current_size = np.floor(current_size / np.array(df))\n",
        "            # logger.info(f\"Block {i+1}: features {current_fmaps}, size {current_size}\")\n",
        "\n",
        "        self.features = nn.Sequential(*layers)\n",
        "\n",
        "        # Classifier (not used for feature extraction)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(int(np.prod(current_size)) * current_fmaps, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, output_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, return_features=False):\n",
        "        x = self.features(x)\n",
        "        if return_features:\n",
        "            return x  # Return raw features before flattening\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "def extract_features(model, dataset, args):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device).eval()\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=args.batch_size,\n",
        "        num_workers=2,\n",
        "        collate_fn=lambda b: (\n",
        "            torch.stack([item[0] for item in b]),  # Pixel values\n",
        "            [item[1] for item in b],               # Synapse info\n",
        "            [item[2] for item in b]                # Bbox names\n",
        "        )\n",
        "    )\n",
        "\n",
        "    features = []\n",
        "    metadata = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            pixels, info, names = batch\n",
        "            inputs = pixels.permute(0, 2, 1, 3, 4).to(device)  # Reshape for 3D convolution\n",
        "\n",
        "            batch_features = model.features(inputs)\n",
        "            pooled_features = nn.AdaptiveAvgPool3d((1, 1, 1))(batch_features)\n",
        "\n",
        "            # Flatten and convert to numpy immediately\n",
        "            features.append(pooled_features.cpu().numpy().squeeze())\n",
        "            metadata.extend(zip(names, info))\n",
        "\n",
        "    # Combine all batch features\n",
        "    features = np.concatenate(features, axis=0)\n",
        "\n",
        "    # Create metadata DataFrame\n",
        "    metadata_df = pd.DataFrame([\n",
        "        {\"bbox\": name, **info.to_dict()}\n",
        "        for name, info in metadata\n",
        "    ])\n",
        "\n",
        "    # Create feature columns\n",
        "    feature_columns = [f'feat_{i+1}' for i in range(features.shape[1])]\n",
        "    features_df = pd.DataFrame(features, columns=feature_columns)\n",
        "\n",
        "    # Combine with metadata\n",
        "    combined_df = pd.concat([metadata_df, features_df], axis=1)\n",
        "\n",
        "    return combined_df\n",
        "# # Initialize the model\n",
        "# model = Vgg3D(input_size=(80, 80, 80), fmaps=24, downsample_factors=[(2, 2, 2), (2, 2, 2)],\n",
        "#               fmap_inc=(2, 2), n_convolutions=(4, 2), output_classes=7, input_fmaps=1)\n",
        "\n",
        "# Load model from checkpoint\n",
        "def load_model_from_checkpoint(model, checkpoint_path):\n",
        "    # Load the checkpoint\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "    # Load the state_dict into the model\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # If the checkpoint includes the optimizer state, you can load that as well\n",
        "    # model.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])  # if needed\n",
        "\n",
        "    # Set the model to evaluation mode (disable dropout, batch norm updates, etc.)\n",
        "    model.eval()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Example: Load your model with the checkpoint\n",
        "model = Vgg3D(input_size=(80, 80, 80), fmaps=24,output_classes=7, input_fmaps=1)\n",
        "\n",
        "# Paths and directories\n",
        "checkpoint_url = \"https://dl.dropboxusercontent.com/scl/fo/mfejaomhu43aa6oqs6zsf/AKMAAgT7OrUtruR0AQXZBy0/hemibrain_production.checkpoint.20220225?rlkey=6cmwxdvehy4ylztvsbgkfnrfc&dl=0\"\n",
        "checkpoint_path = 'hemibrain_production.checkpoint'\n",
        "\n",
        "# Download the checkpoint if it doesn't exist\n",
        "if not os.path.exists(checkpoint_path):\n",
        "    os.system(f\"wget -O {checkpoint_path} '{checkpoint_url}'\")\n",
        "    print(\"Downloaded VGG3D checkpoint.\")\n",
        "else:\n",
        "    print(\"VGG3D checkpoint already exists.\")\n",
        "\n",
        "\n",
        "checkpoint_path = 'hemibrain_production.checkpoint'  # Replace with the actual path to your checkpoint\n",
        "model = load_model_from_checkpoint(model, checkpoint_path)\n"
      ],
      "metadata": {
        "id": "1QB2QlZYZbp4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a9c6e98-c648-491b-e870-35fad2cd8349"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded VGG3D checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-16434c063c55>:136: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "brZg-Z5GdI5K"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir Result\n"
      ],
      "metadata": {
        "id": "tbS9B5TmgiCJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#raw"
      ],
      "metadata": {
        "id": "ZIKEZAyJjn9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "args = parse_args()\n",
        "args.input_mask=False\n",
        "args.segmentation_type=0\n",
        "# Initialize the arguments (or use your existing args)\n",
        "# Use the main function to load data and prepare the dataset\n",
        "# cubes, sys_inf = main(args)\n",
        "\n",
        "# Define the processor (if not already done)\n",
        "processor = SimpleVideoProcessor(size=(80, 80))\n",
        "\n",
        "# Prepare the dataset\n",
        "vol_data_dict = {}\n",
        "for bbox_name in args.bbox_name:\n",
        "    raw_vol, seg_vol, add_mask_vol = load_volumes(\n",
        "        bbox_name=bbox_name,\n",
        "        raw_base_dir=args.raw_base_dir,\n",
        "        seg_base_dir=args.seg_base_dir,\n",
        "        add_mask_base_dir=args.add_mask_base_dir\n",
        "    )\n",
        "    if raw_vol is not None:\n",
        "        vol_data_dict[bbox_name] = (raw_vol, seg_vol, add_mask_vol)\n",
        "\n",
        "synapse_dfs = []\n",
        "for bbox_name in args.bbox_name:\n",
        "    excel_path = os.path.join(args.excel_file, f\"{bbox_name}.xlsx\")\n",
        "    if os.path.exists(excel_path):\n",
        "        df = pd.read_excel(excel_path)\n",
        "        df['bbox_name'] = bbox_name\n",
        "        synapse_dfs.append(df)\n",
        "\n",
        "syn_df = pd.concat(synapse_dfs, ignore_index=True)\n",
        "\n",
        "# Prepare the dataset\n",
        "dataset = VideoMAEDataset(\n",
        "    vol_data_dict=vol_data_dict,\n",
        "    synapse_df=syn_df,\n",
        "    processor=processor,\n",
        "    segmentation_type=args.segmentation_type,\n",
        "    subvol_size=args.subvol_size,\n",
        "    num_frames=args.num_frames,\n",
        "    alpha=args.alpha,\n",
        "    input_mask=False\n",
        ")\n",
        "\n",
        "# Now extract features from the model\n",
        "features_df = extract_features(model, dataset, args)\n",
        "\n",
        "# Print the resulting DataFrame\n",
        "# print(features_df.head())\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import imageio\n",
        "\n",
        "# Extract the data tensor (cube) from the tuple returned by dataset[0]\n",
        "cube, syn_info, bbox_name = dataset[0]\n",
        "\n",
        "mean = torch.tensor([0.485]).view(1, 1, 1, 1)\n",
        "std = torch.tensor([0.229]).view(1, 1, 1, 1)\n",
        "\n",
        "denormalized_cube = cube * std + mean\n",
        "denormalized_cube = torch.clamp(denormalized_cube, 0, 1)\n",
        "\n",
        "# Convert to RGB if needed\n",
        "frames = denormalized_cube.squeeze(1).numpy()  # Remove channel dimension\n",
        "frames = (frames * 255).astype(np.uint8)\n",
        "\n",
        "# Stack to create RGB\n",
        "frames = np.stack([frames, frames, frames], axis=-1)\n",
        "\n",
        "Gif_Name = f\"VGG_Gif_segmentation_type_{args.segmentation_type}_input_mask_{args.input_mask}\"\n",
        "# Gif_Name = f\"VGG_Gif_segmentation_type_{args.segmentation_type}_input_mask_True\"\n",
        "output_gif_path = os.path.join(\"Result\", f\"{Gif_Name}.gif\")\n",
        "# Save the frames as a gif using imageio\n",
        "imageio.mimsave(output_gif_path, frames, fps=10)\n",
        "\n",
        "print(\"GIF saved successfully!\")\n",
        "\n",
        "segmentation_type_name = f\"VGG_CSV_segmentation_type_{args.segmentation_type}_input_mask_{args.input_mask}\"\n",
        "# segmentation_type_name = f\"VGG_CSV_segmentation_type_{args.segmentation_type}_input_mask_True\"\n",
        "output_csv_path = os.path.join(\"Result\", f\"{segmentation_type_name}_features.csv\")\n",
        "\n",
        "# Save the features_df DataFrame as a CSV file\n",
        "features_df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "print(f\"Features saved successfully as {output_csv_path}\")\n",
        "\n",
        "# Set device\n",
        "\n",
        "# Example of processing with Grad-CAM\n",
        "\n"
      ],
      "metadata": {
        "id": "WY_cBQIMdE6u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd211bea-3e6a-4a61-89ab-551983ddbf8a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GIF saved successfully!\n",
            "Features saved successfully as Result/VGG_CSV_segmentation_type_0_input_mask_False_features.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # # Extract the data tensor (cube) from the tuple returned by dataset[0]\n",
        "# # cube, syn_info, bbox_name = dataset[0]\n",
        "\n",
        "# # # Define the mean and std values for denormalization\n",
        "# # mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)  # 1x3x1x1 for RGB\n",
        "# # std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)  # 1x3x1x1 for RGB\n",
        "\n",
        "# mean = torch.tensor([0.485]).view(1, 1, 1, 1)\n",
        "# std = torch.tensor([0.229]).view(1, 1, 1, 1)\n",
        "\n",
        "# denormalized_cube = cube * std + mean\n",
        "# denormalized_cube = torch.clamp(denormalized_cube, 0, 1)\n",
        "\n",
        "# # Convert to RGB if needed\n",
        "# frames = denormalized_cube.squeeze(1).numpy()  # Remove channel dimension\n",
        "# frames = (frames * 255).astype(np.uint8)\n",
        "\n",
        "# # Stack to create RGB\n",
        "# frames = np.stack([frames, frames, frames], axis=-1)\n",
        "\n",
        "# Gif_Name = f\"VGG_Gif_segmentation_type_{args.segmentation_type}_input_mask_{args.input_mask}\"\n",
        "# # Gif_Name = f\"VGG_Gif_segmentation_type_{args.segmentation_type}_input_mask_True\"\n",
        "# output_gif_path = os.path.join(\"Result\", f\"{Gif_Name}.gif\")\n",
        "# # Save the frames as a gif using imageio\n",
        "# imageio.mimsave(output_gif_path, frames, fps=10)\n",
        "\n",
        "# print(\"GIF saved successfully!\")\n",
        "\n",
        "# segmentation_type_name = f\"VGG_CSV_segmentation_type_{args.segmentation_type}_input_mask_{args.input_mask}\"\n",
        "# # segmentation_type_name = f\"VGG_CSV_segmentation_type_{args.segmentation_type}_input_mask_True\"\n",
        "# output_csv_path = os.path.join(\"Result\", f\"{segmentation_type_name}_features.csv\")\n",
        "\n",
        "# # Save the features_df DataFrame as a CSV file\n",
        "# features_df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "# print(f\"Features saved successfully as {output_csv_path}\")\n",
        "\n",
        "# # Set device\n",
        "\n",
        "# # Example of processing with Grad-CAM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAPq-8mrN2rX",
        "outputId": "ebc1cb7b-3790-4a5c-f8cf-98bf751f789a"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GIF saved successfully!\n",
            "Features saved successfully as Result/VGG_CSV_segmentation_type_0_input_mask_False_features.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the layers of the Vgg3D model\n",
        "def print_model_layers(model):\n",
        "    for name, module in model.named_modules():\n",
        "        print(f\"{name}: {module}\")\n",
        "\n",
        "print_model_layers(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_F5jBwO_6RS",
        "outputId": "cbd26377-ec80-49a9-baed-5e514e979a8f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ": Vgg3D(\n",
            "  (features): Sequential(\n",
            "    (0): Conv3d(1, 24, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "    (1): BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv3d(24, 24, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "    (4): BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Conv3d(24, 24, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "    (7): BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): Conv3d(24, 24, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "    (10): BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "    (13): Conv3d(24, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "    (14): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "    (17): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (18): ReLU(inplace=True)\n",
            "    (19): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "    (20): Conv3d(48, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "    (21): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "    (24): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (25): ReLU(inplace=True)\n",
            "    (26): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "    (27): Conv3d(96, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "    (28): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "    (31): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (32): ReLU(inplace=True)\n",
            "    (33): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=24000, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=7, bias=True)\n",
            "  )\n",
            ")\n",
            "features: Sequential(\n",
            "  (0): Conv3d(1, 24, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  (1): BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (2): ReLU(inplace=True)\n",
            "  (3): Conv3d(24, 24, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  (4): BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (5): ReLU(inplace=True)\n",
            "  (6): Conv3d(24, 24, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  (7): BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (8): ReLU(inplace=True)\n",
            "  (9): Conv3d(24, 24, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  (10): BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (11): ReLU(inplace=True)\n",
            "  (12): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "  (13): Conv3d(24, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  (14): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (15): ReLU(inplace=True)\n",
            "  (16): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  (17): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (18): ReLU(inplace=True)\n",
            "  (19): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "  (20): Conv3d(48, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  (21): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (22): ReLU(inplace=True)\n",
            "  (23): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  (24): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (25): ReLU(inplace=True)\n",
            "  (26): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "  (27): Conv3d(96, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  (28): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (29): ReLU(inplace=True)\n",
            "  (30): Conv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  (31): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (32): ReLU(inplace=True)\n",
            "  (33): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            ")\n",
            "features.0: Conv3d(1, 24, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "features.1: BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "features.2: ReLU(inplace=True)\n",
            "features.3: Conv3d(24, 24, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "features.4: BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "features.5: ReLU(inplace=True)\n",
            "features.6: Conv3d(24, 24, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "features.7: BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "features.8: ReLU(inplace=True)\n",
            "features.9: Conv3d(24, 24, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "features.10: BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "features.11: ReLU(inplace=True)\n",
            "features.12: MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "features.13: Conv3d(24, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "features.14: BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "features.15: ReLU(inplace=True)\n",
            "features.16: Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "features.17: BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "features.18: ReLU(inplace=True)\n",
            "features.19: MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "features.20: Conv3d(48, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "features.21: BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "features.22: ReLU(inplace=True)\n",
            "features.23: Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "features.24: BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "features.25: ReLU(inplace=True)\n",
            "features.26: MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "features.27: Conv3d(96, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "features.28: BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "features.29: ReLU(inplace=True)\n",
            "features.30: Conv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "features.31: BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "features.32: ReLU(inplace=True)\n",
            "features.33: MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "classifier: Sequential(\n",
            "  (0): Linear(in_features=24000, out_features=4096, bias=True)\n",
            "  (1): ReLU(inplace=True)\n",
            "  (2): Dropout(p=0.5, inplace=False)\n",
            "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "  (4): ReLU(inplace=True)\n",
            "  (5): Dropout(p=0.5, inplace=False)\n",
            "  (6): Linear(in_features=4096, out_features=7, bias=True)\n",
            ")\n",
            "classifier.0: Linear(in_features=24000, out_features=4096, bias=True)\n",
            "classifier.1: ReLU(inplace=True)\n",
            "classifier.2: Dropout(p=0.5, inplace=False)\n",
            "classifier.3: Linear(in_features=4096, out_features=4096, bias=True)\n",
            "classifier.4: ReLU(inplace=True)\n",
            "classifier.5: Dropout(p=0.5, inplace=False)\n",
            "classifier.6: Linear(in_features=4096, out_features=7, bias=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mTXEKKpq-fiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "process_with_grad_cam(model, dataset, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "s25ZNWMO-o_X",
        "outputId": "3f443a13-b1ec-4c3e-a4e8-cd3be734323d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Given groups=1, weight of size [24, 1, 3, 3, 3], expected input[1, 80, 1, 80, 80] to have 1 channels, but got 80 channels instead",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-a1052816c6bd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprocess_with_grad_cam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-09c544dcfe1f>\u001b[0m in \u001b[0;36mprocess_with_grad_cam\u001b[0;34m(model, dataset, device, layer_name)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;31m# Generate Grad-CAM heatmap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mheatmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_cam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcube\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;31m# Visualize the Grad-CAM for each frame in the cube\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-09c544dcfe1f>\u001b[0m in \u001b[0;36mgrad_cam\u001b[0;34m(model, input_tensor, target_class, layer_name)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Add batch dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget_class\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtarget_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Use the predicted class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-16434c063c55>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, return_features)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m  \u001b[0;31m# Return raw features before flattening\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1843\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1844\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1845\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m             \u001b[0;31m# run always called hooks if they have not already been run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36minner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1788\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1790\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1791\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m                 for hook_id, hook in (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    718\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m             )\n\u001b[0;32m--> 720\u001b[0;31m         return F.conv3d(\n\u001b[0m\u001b[1;32m    721\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [24, 1, 3, 3, 3], expected input[1, 80, 1, 80, 80] to have 1 channels, but got 80 channels instead"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Seg 5 False"
      ],
      "metadata": {
        "id": "iDUX9v4ij1lt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# checkpoint_path = 'hemibrain_production.checkpoint'  # Replace with the actual path to your checkpoint\n",
        "# model = load_model_from_checkpoint(model, checkpoint_path)\n",
        "\n",
        "# Initialize the arguments (or use your existing args)\n",
        "args = parse_args()\n",
        "\n",
        "args.input_mask=False\n",
        "args.segmentation_type=5\n",
        "# Use the main function to load data and prepare the dataset\n",
        "cubes, sys_inf = main(args)\n",
        "\n",
        "# Define the processor (if not already done)\n",
        "processor = SimpleVideoProcessor(size=(80, 80))\n",
        "\n",
        "# Prepare the dataset\n",
        "vol_data_dict = {}\n",
        "for bbox_name in args.bbox_name:\n",
        "    raw_vol, seg_vol, add_mask_vol = load_volumes(\n",
        "        bbox_name=bbox_name,\n",
        "        raw_base_dir=args.raw_base_dir,\n",
        "        seg_base_dir=args.seg_base_dir,\n",
        "        add_mask_base_dir=args.add_mask_base_dir\n",
        "    )\n",
        "    if raw_vol is not None:\n",
        "        vol_data_dict[bbox_name] = (raw_vol, seg_vol, add_mask_vol)\n",
        "\n",
        "synapse_dfs = []\n",
        "for bbox_name in args.bbox_name:\n",
        "    excel_path = os.path.join(args.excel_file, f\"{bbox_name}.xlsx\")\n",
        "    if os.path.exists(excel_path):\n",
        "        df = pd.read_excel(excel_path)\n",
        "        df['bbox_name'] = bbox_name\n",
        "        synapse_dfs.append(df)\n",
        "\n",
        "syn_df = pd.concat(synapse_dfs, ignore_index=True)\n",
        "\n",
        "# Prepare the dataset\n",
        "dataset = VideoMAEDataset(\n",
        "    vol_data_dict=vol_data_dict,\n",
        "    synapse_df=syn_df,\n",
        "    processor=processor,\n",
        "    segmentation_type=5,\n",
        "    subvol_size=args.subvol_size,\n",
        "    num_frames=args.num_frames,\n",
        "    alpha=args.alpha,\n",
        "    input_mask=False\n",
        ")\n",
        "\n",
        "# Now extract features from the model\n",
        "features_df = extract_features(model, dataset, args)\n",
        "\n",
        "# Print the resulting DataFrame\n",
        "# print(features_df.head())\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import imageio\n",
        "\n",
        "# Extract the data tensor (cube) from the tuple returned by dataset[0]\n",
        "cube, syn_info, bbox_name = dataset[0]\n",
        "\n",
        "mean = torch.tensor([0.485]).view(1, 1, 1, 1)\n",
        "std = torch.tensor([0.229]).view(1, 1, 1, 1)\n",
        "\n",
        "denormalized_cube = cube * std + mean\n",
        "denormalized_cube = torch.clamp(denormalized_cube, 0, 1)\n",
        "\n",
        "# Convert to RGB if needed\n",
        "frames = denormalized_cube.squeeze(1).numpy()  # Remove channel dimension\n",
        "frames = (frames * 255).astype(np.uint8)\n",
        "\n",
        "# Stack to create RGB\n",
        "frames = np.stack([frames, frames, frames], axis=-1)\n",
        "\n",
        "Gif_Name = f\"VGG_Gif_segmentation_type_{args.segmentation_type}_input_mask_{args.input_mask}\"\n",
        "# Gif_Name = \"VGG_Gif_segmentation_type_5_input_mask_False\"\n",
        "output_gif_path = os.path.join(\"Result\", f\"{Gif_Name}.gif\")\n",
        "# Save the frames as a gif using imageio\n",
        "imageio.mimsave(output_gif_path, frames, fps=10)\n",
        "\n",
        "print(\"GIF saved successfully!\")\n",
        "\n",
        "segmentation_type_name = f\"VGG_CSV_segmentation_type_{args.segmentation_type}_input_mask_{args.input_mask}\"\n",
        "# segmentation_type_name = \"VGG_CSV_segmentation_type_5_input_mask_False\"\n",
        "output_csv_path = os.path.join(\"Result\", f\"{segmentation_type_name}_features.csv\")\n",
        "\n",
        "# Save the features_df DataFrame as a CSV file\n",
        "features_df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "print(f\"Features saved successfully as {output_csv_path}\")\n"
      ],
      "metadata": {
        "id": "T-JqloVlesAf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "f17a9070-5010-4d3c-a727-0b92cef3f565"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 444 cubes successfully.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "cannot register a hook on a tensor that doesn't require gradient",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-94a68b7ab0fc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# Now extract features from the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mfeatures_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Print the resulting DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-16434c063c55>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(model, dataset, args)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpixels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Reshape for 3D convolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mbatch_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m             \u001b[0mpooled_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdaptiveAvgPool3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1843\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1844\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1845\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m             \u001b[0;31m# run always called hooks if they have not already been run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36minner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1788\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1790\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1791\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m                 for hook_id, hook in (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1843\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1844\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1845\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m             \u001b[0;31m# run always called hooks if they have not already been run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36minner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1801\u001b[0m                         \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1803\u001b[0;31m                         \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1805\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mhook_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-37957915306c>\u001b[0m in \u001b[0;36mforward_hook\u001b[0;34m(module, input, output)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Register the backward hook to get gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackward_hook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Hook for saving gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mregister_hook\u001b[0;34m(self, hook)\u001b[0m\n\u001b[1;32m    619\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_hook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m    622\u001b[0m                 \u001b[0;34m\"cannot register a hook on a tensor that doesn't require gradient\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: cannot register a hook on a tensor that doesn't require gradient"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Seg5 True Black"
      ],
      "metadata": {
        "id": "NFaP91LVj9JS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = parse_args()\n",
        "\n",
        "args.input_mask=True\n",
        "args.segmentation_type=5\n",
        "# Initialize the arguments (or use your existing args)\n",
        "\n",
        "# Use the main function to load data and prepare the dataset\n",
        "# cubes, sys_inf = main(args)\n",
        "\n",
        "# Define the processor (if not already done)\n",
        "processor = SimpleVideoProcessor(size=(80, 80))\n",
        "\n",
        "# Prepare the dataset\n",
        "vol_data_dict = {}\n",
        "for bbox_name in args.bbox_name:\n",
        "    raw_vol, seg_vol, add_mask_vol = load_volumes(\n",
        "        bbox_name=bbox_name,\n",
        "        raw_base_dir=args.raw_base_dir,\n",
        "        seg_base_dir=args.seg_base_dir,\n",
        "        add_mask_base_dir=args.add_mask_base_dir\n",
        "    )\n",
        "    if raw_vol is not None:\n",
        "        vol_data_dict[bbox_name] = (raw_vol, seg_vol, add_mask_vol)\n",
        "\n",
        "synapse_dfs = []\n",
        "for bbox_name in args.bbox_name:\n",
        "    excel_path = os.path.join(args.excel_file, f\"{bbox_name}.xlsx\")\n",
        "    if os.path.exists(excel_path):\n",
        "        df = pd.read_excel(excel_path)\n",
        "        df['bbox_name'] = bbox_name\n",
        "        synapse_dfs.append(df)\n",
        "\n",
        "syn_df = pd.concat(synapse_dfs, ignore_index=True)\n",
        "\n",
        "# Prepare the dataset\n",
        "dataset = VideoMAEDataset(\n",
        "    vol_data_dict=vol_data_dict,\n",
        "    synapse_df=syn_df,\n",
        "    processor=processor,\n",
        "    segmentation_type=5,\n",
        "    subvol_size=args.subvol_size,\n",
        "    num_frames=args.num_frames,\n",
        "    alpha=args.alpha,\n",
        "    input_mask=True\n",
        ")\n",
        "\n",
        "# Now extract features from the model\n",
        "features_df = extract_features(model, dataset, args)\n",
        "\n",
        "# Print the resulting DataFrame\n",
        "# print(features_df.head())\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import imageio\n",
        "\n",
        "# Extract the data tensor (cube) from the tuple returned by dataset[0]\n",
        "cube, syn_info, bbox_name = dataset[0]\n",
        "\n",
        "mean = torch.tensor([0.485]).view(1, 1, 1, 1)\n",
        "std = torch.tensor([0.229]).view(1, 1, 1, 1)\n",
        "\n",
        "denormalized_cube = cube * std + mean\n",
        "denormalized_cube = torch.clamp(denormalized_cube, 0, 1)\n",
        "\n",
        "# Convert to RGB if needed\n",
        "frames = denormalized_cube.squeeze(1).numpy()  # Remove channel dimension\n",
        "frames = (frames * 255).astype(np.uint8)\n",
        "\n",
        "# Stack to create RGB\n",
        "frames = np.stack([frames, frames, frames], axis=-1)\n",
        "\n",
        "Gif_Name = f\"VGG_Gif_segmentation_type_{args.segmentation_type}_input_mask_{args.input_mask}\"\n",
        "# Gif_Name = \"VGG_Gif_segmentation_type_5_input_mask_True\"\n",
        "output_gif_path = os.path.join(\"Result\", f\"{Gif_Name}.gif\")\n",
        "# Save the frames as a gif using imageio\n",
        "imageio.mimsave(output_gif_path, frames, fps=10)\n",
        "\n",
        "print(\"GIF saved successfully!\")\n",
        "\n",
        "segmentation_type_name = f\"VGG_CSV_segmentation_type_{args.segmentation_type}_input_mask_{args.input_mask}_Black\"\n",
        "# segmentation_type_name = \"VGG_CSV_segmentation_type_5_input_mask_True\"\n",
        "output_csv_path = os.path.join(\"Result\", f\"{segmentation_type_name}_Black_features.csv\")\n",
        "\n",
        "# Save the features_df DataFrame as a CSV file\n",
        "features_df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "print(f\"Features saved successfully as {output_csv_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43L_QB4Oek8H",
        "outputId": "e7d2ee5d-d73b-456c-eb3c-d6c0c9416147"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GIF saved successfully!\n",
            "Features saved successfully as Result/VGG_CSV_segmentation_type_5_input_mask_True_Black_Black_features.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-htkZtBYpDBH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# seg5 True Avg"
      ],
      "metadata": {
        "id": "vrsHDN9QgsSw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Before run this run 1.2"
      ],
      "metadata": {
        "id": "ZLYkGGVdklX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# checkpoint_path = 'hemibrain_production.checkpoint'  # Replace with the actual path to your checkpoint\n",
        "# model = load_model_from_checkpoint(model, checkpoint_path)\n",
        "\n",
        "# Initialize the arguments (or use your existing args)\n",
        "args = parse_args()\n",
        "\n",
        "args.input_mask=True\n",
        "args.segmentation_type=5\n",
        "# Use the main function to load data and prepare the dataset\n",
        "# cubes, sys_inf = main(args)\n",
        "\n",
        "# Define the processor (if not already done)\n",
        "processor = SimpleVideoProcessor(size=(80, 80))\n",
        "\n",
        "# Prepare the dataset\n",
        "vol_data_dict = {}\n",
        "for bbox_name in args.bbox_name:\n",
        "    raw_vol, seg_vol, add_mask_vol = load_volumes(\n",
        "        bbox_name=bbox_name,\n",
        "        raw_base_dir=args.raw_base_dir,\n",
        "        seg_base_dir=args.seg_base_dir,\n",
        "        add_mask_base_dir=args.add_mask_base_dir\n",
        "    )\n",
        "    if raw_vol is not None:\n",
        "        vol_data_dict[bbox_name] = (raw_vol, seg_vol, add_mask_vol)\n",
        "\n",
        "synapse_dfs = []\n",
        "for bbox_name in args.bbox_name:\n",
        "    excel_path = os.path.join(args.excel_file, f\"{bbox_name}.xlsx\")\n",
        "    if os.path.exists(excel_path):\n",
        "        df = pd.read_excel(excel_path)\n",
        "        df['bbox_name'] = bbox_name\n",
        "        synapse_dfs.append(df)\n",
        "\n",
        "syn_df = pd.concat(synapse_dfs, ignore_index=True)\n",
        "\n",
        "# Prepare the dataset\n",
        "dataset = VideoMAEDataset(\n",
        "    vol_data_dict=vol_data_dict,\n",
        "    synapse_df=syn_df,\n",
        "    processor=processor,\n",
        "    segmentation_type=5,\n",
        "    subvol_size=args.subvol_size,\n",
        "    num_frames=args.num_frames,\n",
        "    alpha=args.alpha,\n",
        "    input_mask=True\n",
        ")\n",
        "\n",
        "# Now extract features from the model\n",
        "features_df = extract_features(model, dataset, args)\n",
        "\n",
        "# Print the resulting DataFrame\n",
        "# print(features_df.head())\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import imageio\n",
        "\n",
        "# Extract the data tensor (cube) from the tuple returned by dataset[0]\n",
        "cube, syn_info, bbox_name = dataset[0]\n",
        "\n",
        "mean = torch.tensor([0.485]).view(1, 1, 1, 1)\n",
        "std = torch.tensor([0.229]).view(1, 1, 1, 1)\n",
        "\n",
        "denormalized_cube = cube * std + mean\n",
        "denormalized_cube = torch.clamp(denormalized_cube, 0, 1)\n",
        "\n",
        "# Convert to RGB if needed\n",
        "frames = denormalized_cube.squeeze(1).numpy()  # Remove channel dimension\n",
        "frames = (frames * 255).astype(np.uint8)\n",
        "\n",
        "# Stack to create RGB\n",
        "frames = np.stack([frames, frames, frames], axis=-1)\n",
        "\n",
        "# Gif_Name = f\"VGG_Gif_segmentation_type_{args.segmentation_type}_input_mask_{args.input_mask}\"\n",
        "Gif_Name = \"VGG_Gif_segmentation_type_5_input_mask_True_AVG\"\n",
        "output_gif_path = os.path.join(\"Result\", f\"{Gif_Name}.gif\")\n",
        "# Save the frames as a gif using imageio\n",
        "imageio.mimsave(output_gif_path, frames, fps=10)\n",
        "\n",
        "print(\"GIF saved successfully!\")\n",
        "\n",
        "# segmentation_type_name = f\"VGG_CSV_segmentation_type_{args.segmentation_type}_input_mask_{args.input_mask}\"\n",
        "segmentation_type_name = \"VGG_CSV_segmentation_type_5_input_mask_True_AVG\"\n",
        "output_csv_path = os.path.join(\"Result\", f\"{segmentation_type_name}_features.csv\")\n",
        "\n",
        "# Save the features_df DataFrame as a CSV file\n",
        "features_df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "print(f\"Features saved successfully as {output_csv_path}\")\n"
      ],
      "metadata": {
        "id": "F2kY7_HbguWh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fd8329c-ef03-4512-d596-61fb1e294781"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GIF saved successfully!\n",
            "Features saved successfully as Result/VGG_CSV_segmentation_type_5_input_mask_True_AVG_features.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Replace 'folder_name' with the name of your folder\n",
        "shutil.make_archive('/content/Result', 'zip', '/content/Result')\n",
        "from google.colab import files\n",
        "\n",
        "# Download the zip file\n",
        "files.download('/content/Result.zip')\n"
      ],
      "metadata": {
        "id": "6ak79mJzfmN_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "978fc852-6fe1-491f-8548-1bff4b1518d8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_41d81438-08a3-44cc-aa07-302bccaad58f\", \"Result.zip\", 2141480)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!dir"
      ],
      "metadata": {
        "id": "0SpY9s7liHtd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5439238-0959-4f86-eab1-06d10b7d9ace"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bbox_1\t    bbox_7\n",
            "bbox1.xlsx  bbox7.xlsx\n",
            "bbox_2\t    downloaded_file.zip\n",
            "bbox2.xlsx  hemibrain_production.checkpoint\n",
            "bbox_3\t    __MACOSX\n",
            "bbox3.xlsx  raw\n",
            "bbox_4\t    Result\n",
            "bbox4.xlsx  Result.zip\n",
            "bbox_5\t    sample_data\n",
            "bbox5.xlsx  seg\n",
            "bbox_6\t    vesicle_cloud__syn_interface__mitochondria_annotation.zip\n",
            "bbox6.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification"
      ],
      "metadata": {
        "id": "Lkpyg-3IhpOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify(model, dataset, args):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device).eval()\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=args.batch_size,\n",
        "        num_workers=2,\n",
        "        collate_fn=lambda b: (\n",
        "            torch.stack([item[0] for item in b]),  # Pixel values\n",
        "            [item[1] for item in b],               # Synapse info\n",
        "            [item[2] for item in b]                # Bbox names\n",
        "        )\n",
        "    )\n",
        "\n",
        "    all_preds = []\n",
        "    metadata = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            pixels, info, names = batch\n",
        "            inputs = pixels.permute(0, 2, 1, 3, 4).to(device)  # [batch, channels, depth, H, W]\n",
        "\n",
        "            # Get model predictions\n",
        "            outputs = model(inputs)\n",
        "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
        "            preds = torch.argmax(probs, dim=1)\n",
        "\n",
        "            all_preds.append(preds.cpu().numpy())\n",
        "            metadata.extend(zip(names, info))\n",
        "\n",
        "    # Combine results\n",
        "    all_preds = np.concatenate(all_preds, axis=0)\n",
        "\n",
        "    # Create DataFrame\n",
        "    results_df = pd.DataFrame([\n",
        "        {\"bbox\": name, **info.to_dict(), \"predicted_class\": pred}\n",
        "        for (name, info), pred in zip(metadata, all_preds)\n",
        "    ])\n",
        "\n",
        "    return results_df"
      ],
      "metadata": {
        "id": "a6BAu4XShqad"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res=classify(model, dataset,args)"
      ],
      "metadata": {
        "id": "l7eW_M37h2BC"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "VtOKawu3h28M",
        "outputId": "939b77ec-7ef2-4dad-ba49-111881a55c5d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    bbox                    Var1  central_coord_1  central_coord_2  \\\n",
              "0  bbox1  non_spine_synapsed_056              171              260   \n",
              "1  bbox1   non_spine_synapse_057              223              113   \n",
              "2  bbox1   non_spine_synapse_058              280              102   \n",
              "3  bbox1   non_spine_synapse_063              455              131   \n",
              "4  bbox1   non_spine_synapse_062              138              121   \n",
              "\n",
              "   central_coord_3  side_1_coord_1  side_1_coord_2  side_1_coord_3  \\\n",
              "0              350             171             268             359   \n",
              "1              425             223             112             438   \n",
              "2              377             280              94             400   \n",
              "3              162             455             134             181   \n",
              "4              302             135             113             298   \n",
              "\n",
              "   side_2_coord_1  side_2_coord_2  side_2_coord_3 bbox_name  predicted_class  \n",
              "0             171             260             340     bbox1                6  \n",
              "1             223             114             407     bbox1                6  \n",
              "2             280             108             364     bbox1                6  \n",
              "3             455             127             145     bbox1                6  \n",
              "4             140             127             312     bbox1                6  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-25c53a84-eee3-4e8e-80da-64cbffc7b68c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bbox</th>\n",
              "      <th>Var1</th>\n",
              "      <th>central_coord_1</th>\n",
              "      <th>central_coord_2</th>\n",
              "      <th>central_coord_3</th>\n",
              "      <th>side_1_coord_1</th>\n",
              "      <th>side_1_coord_2</th>\n",
              "      <th>side_1_coord_3</th>\n",
              "      <th>side_2_coord_1</th>\n",
              "      <th>side_2_coord_2</th>\n",
              "      <th>side_2_coord_3</th>\n",
              "      <th>bbox_name</th>\n",
              "      <th>predicted_class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>bbox1</td>\n",
              "      <td>non_spine_synapsed_056</td>\n",
              "      <td>171</td>\n",
              "      <td>260</td>\n",
              "      <td>350</td>\n",
              "      <td>171</td>\n",
              "      <td>268</td>\n",
              "      <td>359</td>\n",
              "      <td>171</td>\n",
              "      <td>260</td>\n",
              "      <td>340</td>\n",
              "      <td>bbox1</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>bbox1</td>\n",
              "      <td>non_spine_synapse_057</td>\n",
              "      <td>223</td>\n",
              "      <td>113</td>\n",
              "      <td>425</td>\n",
              "      <td>223</td>\n",
              "      <td>112</td>\n",
              "      <td>438</td>\n",
              "      <td>223</td>\n",
              "      <td>114</td>\n",
              "      <td>407</td>\n",
              "      <td>bbox1</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>bbox1</td>\n",
              "      <td>non_spine_synapse_058</td>\n",
              "      <td>280</td>\n",
              "      <td>102</td>\n",
              "      <td>377</td>\n",
              "      <td>280</td>\n",
              "      <td>94</td>\n",
              "      <td>400</td>\n",
              "      <td>280</td>\n",
              "      <td>108</td>\n",
              "      <td>364</td>\n",
              "      <td>bbox1</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>bbox1</td>\n",
              "      <td>non_spine_synapse_063</td>\n",
              "      <td>455</td>\n",
              "      <td>131</td>\n",
              "      <td>162</td>\n",
              "      <td>455</td>\n",
              "      <td>134</td>\n",
              "      <td>181</td>\n",
              "      <td>455</td>\n",
              "      <td>127</td>\n",
              "      <td>145</td>\n",
              "      <td>bbox1</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>bbox1</td>\n",
              "      <td>non_spine_synapse_062</td>\n",
              "      <td>138</td>\n",
              "      <td>121</td>\n",
              "      <td>302</td>\n",
              "      <td>135</td>\n",
              "      <td>113</td>\n",
              "      <td>298</td>\n",
              "      <td>140</td>\n",
              "      <td>127</td>\n",
              "      <td>312</td>\n",
              "      <td>bbox1</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-25c53a84-eee3-4e8e-80da-64cbffc7b68c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-25c53a84-eee3-4e8e-80da-64cbffc7b68c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-25c53a84-eee3-4e8e-80da-64cbffc7b68c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3e11b06f-c044-4966-8e33-152490be368a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3e11b06f-c044-4966-8e33-152490be368a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3e11b06f-c044-4966-8e33-152490be368a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "res",
              "summary": "{\n  \"name\": \"res\",\n  \"rows\": 444,\n  \"fields\": [\n    {\n      \"column\": \"bbox\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"bbox1\",\n          \"bbox2\",\n          \"bbox6\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Var1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 289,\n        \"samples\": [\n          \"non_spine_synapse_013\",\n          \"explorative_2024-08-28_Cora_Wolter_003\",\n          \"spine_synapse_026\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"central_coord_1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 106,\n        \"min\": 82,\n        \"max\": 478,\n        \"num_unique_values\": 268,\n        \"samples\": [\n          246,\n          163,\n          396\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"central_coord_2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 99,\n        \"min\": 84,\n        \"max\": 478,\n        \"num_unique_values\": 255,\n        \"samples\": [\n          317,\n          383,\n          295\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"central_coord_3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 90,\n        \"min\": 91,\n        \"max\": 491,\n        \"num_unique_values\": 232,\n        \"samples\": [\n          125,\n          122,\n          242\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"side_1_coord_1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 106,\n        \"min\": 82,\n        \"max\": 476,\n        \"num_unique_values\": 255,\n        \"samples\": [\n          236,\n          175,\n          230\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"side_1_coord_2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 99,\n        \"min\": 71,\n        \"max\": 483,\n        \"num_unique_values\": 260,\n        \"samples\": [\n          337,\n          287,\n          299\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"side_1_coord_3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 91,\n        \"min\": 73,\n        \"max\": 509,\n        \"num_unique_values\": 241,\n        \"samples\": [\n          263,\n          294,\n          141\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"side_2_coord_1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 106,\n        \"min\": 81,\n        \"max\": 489,\n        \"num_unique_values\": 259,\n        \"samples\": [\n          475,\n          188,\n          467\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"side_2_coord_2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 100,\n        \"min\": 90,\n        \"max\": 479,\n        \"num_unique_values\": 253,\n        \"samples\": [\n          152,\n          369,\n          223\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"side_2_coord_3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 90,\n        \"min\": 100,\n        \"max\": 470,\n        \"num_unique_values\": 237,\n        \"samples\": [\n          413,\n          318,\n          233\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bbox_name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"bbox1\",\n          \"bbox2\",\n          \"bbox6\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"predicted_class\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 6,\n        \"max\": 6,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: save res at csv\n",
        "\n",
        "res.to_csv('Result/classification_results.csv', index=False)"
      ],
      "metadata": {
        "id": "0UsSO2_GimoC"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_with_probs(model, dataset, args):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device).eval()\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=args.batch_size,\n",
        "        num_workers=2,\n",
        "        collate_fn=lambda b: (\n",
        "            torch.stack([item[0] for item in b]),  # Pixel values\n",
        "            [item[1] for item in b],               # Synapse info\n",
        "            [item[2] for item in b]                # Bbox names\n",
        "        )\n",
        "    )\n",
        "\n",
        "    all_preds = []\n",
        "    all_probs = []\n",
        "    metadata = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            pixels, info, names = batch\n",
        "            inputs = pixels.permute(0, 2, 1, 3, 4).to(device)  # [batch, channels, depth, H, W]\n",
        "\n",
        "            # Get model predictions\n",
        "            outputs = model(inputs)\n",
        "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
        "            preds = torch.argmax(probs, dim=1)\n",
        "\n",
        "            # Store results\n",
        "            all_probs.append(probs.cpu().numpy())\n",
        "            all_preds.append(preds.cpu().numpy())\n",
        "            metadata.extend(zip(names, info))\n",
        "\n",
        "    # Combine results\n",
        "    all_preds = np.concatenate(all_preds, axis=0)\n",
        "    all_probs = np.concatenate(all_probs, axis=0)\n",
        "\n",
        "    # Create DataFrame with probabilities\n",
        "    results_df = pd.DataFrame([\n",
        "        {\n",
        "            \"bbox\": name,\n",
        "            **info.to_dict(),\n",
        "            \"predicted_class\": pred,\n",
        "            **{f\"prob_class_{i}\": prob[i] for i in range(len(prob))}\n",
        "        }\n",
        "        for (name, info), pred, prob in zip(metadata, all_preds, all_probs)\n",
        "    ])\n",
        "\n",
        "    return results_df\n",
        "\n",
        "# Run classification with probabilities\n",
        "results_with_probs = classify_with_probs(model, dataset, args)\n",
        "\n",
        "# Save full results\n",
        "results_with_probs.to_csv(\"Result/classification_results_with_probabilities.csv\", index=False)\n",
        "print(\"Results with probabilities saved to Result/classification_results_with_probabilities.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEWK4Szsjk4r",
        "outputId": "cf0f4852-4e74-41e2-ccb0-4393b1c051ff"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results with probabilities saved to Result/classification_results_with_probabilities.csv\n"
          ]
        }
      ]
    }
  ]
}