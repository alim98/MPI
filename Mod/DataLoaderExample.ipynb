{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNSh8saw1z7F0sT276Fx12z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alim98/MPI/blob/main/Mod/DataLoaderExample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!wget -O downloaded_file.zip \"https://drive.usercontent.google.com/download?id=1iHPBdBOPEagvPTHZmrN__LD49emXwReY&export=download&authuser=0&confirm=t&uuid=631d60dd-569c-4bb1-a9e8-d681f0ed3d43&at=APvzH3r4me8x_LwP3n8O7lgPo8oK%3A1733988188000\"\n",
        "\n",
        "# !pip -q install transformers scikit-learn matplotlib seaborn torch torchvision umap-learn openpyxl imageio\n",
        "\n",
        "\n",
        "!wget -O vesicle_cloud__syn_interface__mitochondria_annotation.zip \"https://drive.usercontent.google.com/download?id=1qRibZL3kr7MQJQRgDFRquHMQlIGCN4XP&export=download&authuser=0&confirm=t&uuid=631d60dd-569c-4bb1-a9e8-d681f0ed3d43&at=APvzH3r4me8x_LwP3n8O7lgPo8oK%3A1733988188000\"\n",
        "\n",
        "!unzip -q downloaded_file.zip\n",
        "!unzip -q vesicle_cloud__syn_interface__mitochondria_annotation.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTfdGXpI_lbv",
        "outputId": "536b652d-8a5d-4c96-c403-8e51d5f73a0b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-22 07:51:18--  https://drive.usercontent.google.com/download?id=1iHPBdBOPEagvPTHZmrN__LD49emXwReY&export=download&authuser=0&confirm=t&uuid=631d60dd-569c-4bb1-a9e8-d681f0ed3d43&at=APvzH3r4me8x_LwP3n8O7lgPo8oK%3A1733988188000\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 172.253.62.132, 2607:f8b0:4004:c07::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|172.253.62.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1264688649 (1.2G) [application/octet-stream]\n",
            "Saving to: ‘downloaded_file.zip’\n",
            "\n",
            "downloaded_file.zip 100%[===================>]   1.18G  81.8MB/s    in 22s     \n",
            "\n",
            "2025-01-22 07:51:42 (55.0 MB/s) - ‘downloaded_file.zip’ saved [1264688649/1264688649]\n",
            "\n",
            "--2025-01-22 07:51:42--  https://drive.usercontent.google.com/download?id=1qRibZL3kr7MQJQRgDFRquHMQlIGCN4XP&export=download&authuser=0&confirm=t&uuid=631d60dd-569c-4bb1-a9e8-d681f0ed3d43&at=APvzH3r4me8x_LwP3n8O7lgPo8oK%3A1733988188000\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 172.253.62.132, 2607:f8b0:4004:c07::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|172.253.62.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14246564 (14M) [application/octet-stream]\n",
            "Saving to: ‘vesicle_cloud__syn_interface__mitochondria_annotation.zip’\n",
            "\n",
            "vesicle_cloud__syn_ 100%[===================>]  13.59M  37.0MB/s    in 0.4s    \n",
            "\n",
            "2025-01-22 07:52:13 (37.0 MB/s) - ‘vesicle_cloud__syn_interface__mitochondria_annotation.zip’ saved [14246564/14246564]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrgZ8wcX_eBo",
        "outputId": "909633d6-bae6-4f66-e678-37bda5f75919"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bbox1\n",
            "Processed 58 cubes successfully.\n",
            "Processed 58 cubes successfully.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import io\n",
        "import argparse\n",
        "import multiprocessing\n",
        "from typing import List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import imageio.v3 as iio\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision import transforms\n",
        "\n",
        "# import wandb  # Uncomment if using Weights & Biases for logging\n",
        "\n",
        "\n",
        "class SimpleVideoProcessor:\n",
        "    def __init__(self, size=(80, 80), mean=(0.485, 0.456, 0.406),\n",
        "                 std=(0.229, 0.224, 0.225)):\n",
        "        \"\"\"\n",
        "        Initializes the processor with resizing and normalization transforms.\n",
        "\n",
        "        Args:\n",
        "            size (tuple): Desired output size (height, width).\n",
        "            mean (tuple): Mean for normalization.\n",
        "            std (tuple): Standard deviation for normalization.\n",
        "        \"\"\"\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),          # Convert NumPy array to PIL Image\n",
        "            transforms.Resize(size),          # Resize to desired size\n",
        "            transforms.ToTensor(),            # Convert PIL Image to Tensor\n",
        "            transforms.Normalize(mean=mean, std=std),  # Normalize\n",
        "        ])\n",
        "\n",
        "    def __call__(self, frames, return_tensors=None):\n",
        "        \"\"\"\n",
        "        Processes a list of frames.\n",
        "\n",
        "        Args:\n",
        "            frames (List[np.ndarray]): List of frames as NumPy arrays.\n",
        "            return_tensors (str, optional): Type of tensors to return. Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "            dict or torch.Tensor: Dictionary containing processed pixel values or tensor.\n",
        "        \"\"\"\n",
        "        # Apply transformations to each frame\n",
        "        processed_frames = [self.transform(frame) for frame in frames]\n",
        "\n",
        "        # Stack frames to create a tensor of shape (num_frames, 3, H, W)\n",
        "        pixel_values = torch.stack(processed_frames)\n",
        "\n",
        "        if return_tensors == \"pt\":\n",
        "            return {\"pixel_values\": pixel_values}\n",
        "        else:\n",
        "            return pixel_values\n",
        "\n",
        "\n",
        "def load_volumes(bbox_name: str, raw_base_dir: str, seg_base_dir: str, add_mask_base_dir: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Load raw volume, segmentation volume, and additional mask volume for a bounding box.\n",
        "\n",
        "    Args:\n",
        "        bbox_name (str): Name of the bounding box directory (e.g., 'bbox1').\n",
        "        raw_base_dir (str): Base directory for raw data.\n",
        "        seg_base_dir (str): Base directory for segmentation data.\n",
        "        add_mask_base_dir (str): Base directory for additional masks.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (raw_vol, seg_vol, add_mask_vol) each as np.ndarray\n",
        "    \"\"\"\n",
        "    raw_dir = os.path.join(raw_base_dir, bbox_name)\n",
        "    seg_dir = os.path.join(seg_base_dir, bbox_name)\n",
        "\n",
        "    # Transform 'bbox1' to 'bbox_1' for additional masks\n",
        "    if bbox_name.startswith(\"bbox\"):\n",
        "        bbox_num = bbox_name.replace(\"bbox\", \"\")\n",
        "        add_mask_dir = os.path.join(add_mask_base_dir, f\"bbox_{bbox_num}\")\n",
        "    else:\n",
        "        add_mask_dir = os.path.join(add_mask_base_dir, bbox_name)\n",
        "\n",
        "    raw_tif_files = sorted(glob.glob(os.path.join(raw_dir, 'slice_*.tif')))\n",
        "    seg_tif_files = sorted(glob.glob(os.path.join(seg_dir, 'slice_*.tif')))\n",
        "    add_mask_tif_files = sorted(glob.glob(os.path.join(add_mask_dir, 'slice_*.tif')))\n",
        "\n",
        "    if len(raw_tif_files) == 0:\n",
        "        print(f\"No raw files found for {bbox_name} in {raw_dir}\")\n",
        "        return None, None, None\n",
        "\n",
        "    if len(seg_tif_files) == 0:\n",
        "        print(f\"No segmentation files found for {bbox_name} in {seg_dir}\")\n",
        "        return None, None, None\n",
        "\n",
        "    if len(add_mask_tif_files) == 0:\n",
        "        print(f\"No additional mask files found for {bbox_name} in {add_mask_dir}\")\n",
        "        return None, None, None\n",
        "\n",
        "    if not (len(raw_tif_files) == len(seg_tif_files) == len(add_mask_tif_files)):\n",
        "        print(f\"Mismatch in number of raw, seg, and additional mask slices for {bbox_name}. Skipping.\")\n",
        "        return None, None, None\n",
        "\n",
        "    try:\n",
        "        raw_vol = np.stack([iio.imread(f) for f in raw_tif_files], axis=0)  # shape: (Z, Y, X)\n",
        "        seg_vol = np.stack([iio.imread(f).astype(np.uint32) for f in seg_tif_files], axis=0)\n",
        "        add_mask_vol = np.stack([iio.imread(f).astype(np.uint32) for f in add_mask_tif_files], axis=0)\n",
        "        return raw_vol, seg_vol, add_mask_vol\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading volumes for {bbox_name}: {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "\n",
        "def create_segmented_cube(\n",
        "    raw_vol: np.ndarray,\n",
        "    seg_vol: np.ndarray,\n",
        "    add_mask_vol: np.ndarray,\n",
        "    central_coord: Tuple[int, int, int],\n",
        "    side1_coord: Tuple[int, int, int],\n",
        "    side2_coord: Tuple[int, int, int],\n",
        "    segmentation_type: int,\n",
        "    subvolume_size: int = 80,\n",
        "    alpha: float = 0.3\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Constructs an 80x80x80 segmented 3D cube around the specified synapse coordinates\n",
        "    and overlays selected segmentation masks on the raw data with specified transparency for each slice.\n",
        "\n",
        "    Args:\n",
        "        raw_vol (np.ndarray): Raw volumetric data.\n",
        "        seg_vol (np.ndarray): Segmentation volumetric data.\n",
        "        add_mask_vol (np.ndarray): Additional mask volumetric data.\n",
        "        central_coord (tuple): Central coordinate (x, y, z).\n",
        "        side1_coord (tuple): Side 1 coordinate (x, y, z).\n",
        "        side2_coord (tuple): Side 2 coordinate (x, y, z).\n",
        "        segmentation_type (int): Type of segmentation overlay (0-5).\n",
        "        subvolume_size (int, optional): Size of the subvolume. Defaults to 80.\n",
        "        alpha (float, optional): Transparency factor. Defaults to 0.3.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Overlaid cube of shape (height, width, 3, depth).\n",
        "    \"\"\"\n",
        "\n",
        "    def create_segment_masks(segmentation_volume, s1_coord, s2_coord):\n",
        "        x1, y1, z1 = s1_coord\n",
        "        x2, y2, z2 = s2_coord\n",
        "        # Validate within volume\n",
        "        if not (0 <= z1 < segmentation_volume.shape[0] and\n",
        "                0 <= y1 < segmentation_volume.shape[1] and\n",
        "                0 <= x1 < segmentation_volume.shape[2]):\n",
        "            raise ValueError(\"Side1 coordinates are out of bounds.\")\n",
        "\n",
        "        if not (0 <= z2 < segmentation_volume.shape[0] and\n",
        "                0 <= y2 < segmentation_volume.shape[1] and\n",
        "                0 <= x2 < segmentation_volume.shape[2]):\n",
        "            raise ValueError(\"Side2 coordinates are out of bounds.\")\n",
        "\n",
        "        seg_id_1 = segmentation_volume[z1, y1, x1]\n",
        "        seg_id_2 = segmentation_volume[z2, y2, x2]\n",
        "\n",
        "        # If seg_id == 0, it means no segment at that voxel\n",
        "        if seg_id_1 == 0:\n",
        "            mask_1 = np.zeros_like(segmentation_volume, dtype=bool)\n",
        "        else:\n",
        "            mask_1 = (segmentation_volume == seg_id_1)\n",
        "\n",
        "        if seg_id_2 == 0:\n",
        "            mask_2 = np.zeros_like(segmentation_volume, dtype=bool)\n",
        "        else:\n",
        "            mask_2 = (segmentation_volume == seg_id_2)\n",
        "\n",
        "        return mask_1, mask_2\n",
        "\n",
        "    # Build masks\n",
        "    mask_1_full, mask_2_full = create_segment_masks(seg_vol, side1_coord, side2_coord)\n",
        "    mask_3_full = (add_mask_vol > 0)  # Assuming binary masks; adjust if necessary\n",
        "\n",
        "    # Define subvolume bounds\n",
        "    half_size = subvolume_size // 2\n",
        "    cx, cy, cz = central_coord\n",
        "\n",
        "    x_start, x_end = max(cx - half_size, 0), min(cx + half_size, raw_vol.shape[2])\n",
        "    y_start, y_end = max(cy - half_size, 0), min(cy + half_size, raw_vol.shape[1])\n",
        "    z_start, z_end = max(cz - half_size, 0), min(cz + half_size, raw_vol.shape[0])\n",
        "\n",
        "    # Extract subvolumes\n",
        "    sub_raw = raw_vol[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "    sub_mask_1 = mask_1_full[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "    sub_mask_2 = mask_2_full[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "    sub_mask_3 = mask_3_full[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "\n",
        "    # Pad if smaller than subvolume_size\n",
        "    pad_z = subvolume_size - sub_raw.shape[0]\n",
        "    pad_y = subvolume_size - sub_raw.shape[1]\n",
        "    pad_x = subvolume_size - sub_raw.shape[2]\n",
        "\n",
        "    if pad_z > 0 or pad_y > 0 or pad_x > 0:\n",
        "        sub_raw = np.pad(sub_raw, ((0, pad_z), (0, pad_y), (0, pad_x)),\n",
        "                         mode='constant', constant_values=0)\n",
        "        sub_mask_1 = np.pad(sub_mask_1, ((0, pad_z), (0, pad_y), (0, pad_x)),\n",
        "                            mode='constant', constant_values=False)\n",
        "        sub_mask_2 = np.pad(sub_mask_2, ((0, pad_z), (0, pad_y), (0, pad_x)),\n",
        "                            mode='constant', constant_values=False)\n",
        "        sub_mask_3 = np.pad(sub_mask_3, ((0, pad_z), (0, pad_y), (0, pad_x)),\n",
        "                            mode='constant', constant_values=False)\n",
        "\n",
        "    # Slice to exact shape\n",
        "    sub_raw = sub_raw[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "    sub_mask_1 = sub_mask_1[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "    sub_mask_2 = sub_mask_2[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "    sub_mask_3 = sub_mask_3[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "\n",
        "    # We'll build an overlaid cube: shape => (H, W, 3, D)\n",
        "    overlaid_cube = np.zeros((subvolume_size, subvolume_size, 3, subvolume_size), dtype=np.uint8)\n",
        "\n",
        "    # Define colors\n",
        "    side1_color = np.array([1, 0, 0], dtype=np.float32)           # Red\n",
        "    side2_color = np.array([0, 0, 1], dtype=np.float32)           # Blue\n",
        "    vesicles_color = np.array([0, 1, 0], dtype=np.float32)        # Green\n",
        "\n",
        "    for z in range(subvolume_size):\n",
        "        # Normalize raw slice to [0, 1]\n",
        "        raw_slice = sub_raw[z].astype(np.float32)\n",
        "        mn, mx = raw_slice.min(), raw_slice.max()\n",
        "        if mx > mn:\n",
        "            raw_slice = (raw_slice - mn) / (mx - mn)\n",
        "        else:\n",
        "            raw_slice = raw_slice - mn  # all zeros if mn=mx\n",
        "\n",
        "        raw_rgb = np.stack([raw_slice]*3, axis=-1)  # shape (H, W, 3)\n",
        "\n",
        "        # Initialize colored masks\n",
        "        mask1_rgb = np.zeros_like(raw_rgb)\n",
        "        mask2_rgb = np.zeros_like(raw_rgb)\n",
        "        mask3_rgb = np.zeros_like(raw_rgb)\n",
        "\n",
        "        # Overlay masks based on segmentation_type\n",
        "        if segmentation_type in [1, 3, 5]:\n",
        "            mask1_rgb[sub_mask_1[z]] = side1_color\n",
        "        if segmentation_type in [2, 3, 5]:\n",
        "            mask2_rgb[sub_mask_2[z]] = side2_color\n",
        "        if segmentation_type in [4, 5]:\n",
        "            mask3_rgb[sub_mask_3[z]] = vesicles_color\n",
        "\n",
        "        # Combine masks\n",
        "        combined_masks = mask1_rgb + mask2_rgb + mask3_rgb\n",
        "        # Ensure that combined masks do not exceed 1\n",
        "        combined_masks = np.clip(combined_masks, 0, 1)\n",
        "\n",
        "        # Blend raw image with masks\n",
        "        overlaid_image = (1 - alpha) * raw_rgb + alpha * combined_masks\n",
        "        overlaid_image = np.clip(overlaid_image, 0, 1)\n",
        "\n",
        "        # Convert to uint8\n",
        "        overlaid_image = (overlaid_image * 255).astype(np.uint8)\n",
        "        overlaid_cube[:, :, :, z] = overlaid_image\n",
        "\n",
        "    return overlaid_cube\n",
        "\n",
        "class VideoMAEDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset class that uses segmented volumes (side1 & side2) and additional masks for VideoMAE pre-training.\n",
        "    \"\"\"\n",
        "    def __init__(self, vol_data_list: List[Tuple[np.ndarray, np.ndarray, np.ndarray]],\n",
        "                 synapse_df: pd.DataFrame,\n",
        "                 processor,\n",
        "                 segmentation_type: int,\n",
        "                 subvol_size: int = 80,\n",
        "                 num_frames: int = 16,\n",
        "                 alpha: float = 0.3):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            vol_data_list (List[Tuple[np.ndarray, np.ndarray, np.ndarray]]): List of (raw_vol, seg_vol, add_mask_vol).\n",
        "            synapse_df (pd.DataFrame): DataFrame with synapse coordinates (central, side1, side2).\n",
        "            processor: Processor for VideoMAE.\n",
        "            segmentation_type (int): Type of segmentation overlay (0-5).\n",
        "            subvol_size (int): Size of the sub-volume to extract.\n",
        "            num_frames (int): Number of frames for the model.\n",
        "            alpha (float): Blending alpha for segmentation.\n",
        "        \"\"\"\n",
        "        self.vol_data_list = vol_data_list\n",
        "        self.synapse_df = synapse_df.reset_index(drop=True)\n",
        "        self.processor = processor\n",
        "        self.segmentation_type = segmentation_type\n",
        "        self.subvol_size = subvol_size\n",
        "        self.num_frames = num_frames\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.synapse_df)\n",
        "class VideoMAEDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset class that uses segmented volumes (side1 & side2) and additional masks for VideoMAE pre-training.\n",
        "    \"\"\"\n",
        "    def __init__(self, vol_data_dict: dict,\n",
        "                 synapse_df: pd.DataFrame,\n",
        "                 processor,\n",
        "                 segmentation_type: int,\n",
        "                 subvol_size: int = 80,\n",
        "                 num_frames: int = 16,\n",
        "                 alpha: float = 0.3):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            vol_data_dict (dict): Dictionary with keys as bbox_name and values as tuples (raw_vol, seg_vol, add_mask_vol).\n",
        "            synapse_df (pd.DataFrame): DataFrame with synapse coordinates (central, side1, side2).\n",
        "            processor: Processor for VideoMAE.\n",
        "            segmentation_type (int): Type of segmentation overlay (0-5).\n",
        "            subvol_size (int): Size of the sub-volume to extract.\n",
        "            num_frames (int): Number of frames for the model.\n",
        "            alpha (float): Blending alpha for segmentation.\n",
        "        \"\"\"\n",
        "        self.vol_data_dict = vol_data_dict  # Changed to dictionary\n",
        "        self.synapse_df = synapse_df.reset_index(drop=True)\n",
        "        self.processor = processor\n",
        "        self.segmentation_type = segmentation_type\n",
        "        self.subvol_size = subvol_size\n",
        "        self.num_frames = num_frames\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.synapse_df)\n",
        "    def __getitem__(self, idx):\n",
        "        syn_info = self.synapse_df.iloc[idx]\n",
        "        bbox_name = syn_info['bbox_name']  # Use bbox_name instead of bbox_index\n",
        "\n",
        "        # Unpack the volumes using bbox_name instead of bbox_index\n",
        "        raw_vol, seg_vol, add_mask_vol = self.vol_data_dict.get(bbox_name, (None, None, None))\n",
        "\n",
        "        if raw_vol is None or seg_vol is None or add_mask_vol is None:\n",
        "            # Return dummy data if volumes not found\n",
        "            pixel_values = torch.zeros((self.num_frames, 3, self.subvol_size, self.subvol_size), dtype=torch.float32)\n",
        "            return pixel_values, syn_info, bbox_name\n",
        "\n",
        "        # Coordinates\n",
        "        central_coord = (\n",
        "            int(syn_info['central_coord_1']),\n",
        "            int(syn_info['central_coord_2']),\n",
        "            int(syn_info['central_coord_3'])\n",
        "        )\n",
        "        side1_coord = (\n",
        "            int(syn_info['side_1_coord_1']),\n",
        "            int(syn_info['side_1_coord_2']),\n",
        "            int(syn_info['side_1_coord_3'])\n",
        "        )\n",
        "        side2_coord = (\n",
        "            int(syn_info['side_2_coord_1']),\n",
        "            int(syn_info['side_2_coord_2']),\n",
        "            int(syn_info['side_2_coord_3'])\n",
        "        )\n",
        "\n",
        "        # Create the overlaid segmented cube with the additional mask based on segmentation_type\n",
        "        overlaid_cube = create_segmented_cube(\n",
        "            raw_vol=raw_vol,\n",
        "            seg_vol=seg_vol,\n",
        "            add_mask_vol=add_mask_vol,\n",
        "            central_coord=central_coord,\n",
        "            side1_coord=side1_coord,\n",
        "            side2_coord=side2_coord,\n",
        "            segmentation_type=self.segmentation_type,\n",
        "            subvolume_size=self.subvol_size,\n",
        "            alpha=self.alpha\n",
        "        )  # shape: (80, 80, 3, 80)\n",
        "\n",
        "        # We interpret the last dimension (depth) as frames\n",
        "        frames = []\n",
        "        for z in range(overlaid_cube.shape[3]):  # 80 slices\n",
        "            frame_rgb = overlaid_cube[..., z]  # (80, 80, 3)\n",
        "            frames.append(frame_rgb)\n",
        "\n",
        "        # Now reduce or expand to self.num_frames\n",
        "        total_slices = len(frames)  # 80\n",
        "        if total_slices < self.num_frames:\n",
        "            while len(frames) < self.num_frames:\n",
        "                frames.append(frames[-1])\n",
        "        elif total_slices > self.num_frames:\n",
        "            indices = np.linspace(0, total_slices - 1, self.num_frames, dtype=int)\n",
        "            frames = [frames[i] for i in indices]\n",
        "\n",
        "        # Process using the VideoMAEImageProcessor\n",
        "        inputs = self.processor(frames, return_tensors=\"pt\")\n",
        "        pixel_values = inputs[\"pixel_values\"].squeeze(0)  # (num_frames, 3, H, W)\n",
        "        pixel_values = pixel_values.float()\n",
        "\n",
        "        # Return pixel values, the corresponding DataFrame row, and the bbox name\n",
        "        return pixel_values, syn_info, bbox_name  # Return the pixel values, DataFrame row, and bbox_name\n",
        "\n",
        "def parse_args():\n",
        "    \"\"\"\n",
        "    Parse command-line arguments for configurable paths and training parameters.\n",
        "    \"\"\"\n",
        "    parser = argparse.ArgumentParser(description=\"VideoMAE Pre-training Script with Segmented Videos and Additional Masks\")\n",
        "\n",
        "    # Data directories\n",
        "    parser.add_argument('--raw_base_dir', type=str, default='raw', help='Path to raw data directory')\n",
        "    parser.add_argument('--seg_base_dir', type=str, default='seg', help='Path to segmentation data directory')\n",
        "    parser.add_argument('--add_mask_base_dir', type=str, default='', help='Path to additional masks directory')\n",
        "    parser.add_argument('--bbox_name', type=str, default=['bbox1'], help='Name of the bounding box directory')\n",
        "    parser.add_argument('--excel_file', type=str, default='', help='Excel file with synapse coordinates')\n",
        "    # Output and logging directories\n",
        "    parser.add_argument('--csv_output_dir', type=str, default='csv_outputs', help='Directory to save CSV outputs')\n",
        "    parser.add_argument('--checkpoint_dir', type=str, default='checkpoints', help='Directory to save model checkpoints')\n",
        "    parser.add_argument('--log_dir', type=str, default='logs', help='Directory for TensorBoard logs')\n",
        "    parser.add_argument('--size',type=tuple,default=(80,80),help='Size of the image')\n",
        "    # Training parameters\n",
        "    parser.add_argument('--batch_size', type=int, default=2, help='Batch size for training')\n",
        "    parser.add_argument('--num_epochs', type=int, default=5, help='Number of training epochs')\n",
        "    parser.add_argument('--learning_rate', type=float, default=1e-4, help='Learning rate for optimizer')\n",
        "    parser.add_argument('--weight_decay', type=float, default=1e-2, help='Weight decay for optimizer')\n",
        "    parser.add_argument('--subvol_size', type=int, default=80, help='Size of the sub-volume to extract')\n",
        "    parser.add_argument('--num_frames', type=int, default=80, help='Number of frames per video clip')\n",
        "    parser.add_argument('--mask_ratio', type=float, default=0.75, help='Mask ratio for VideoMAE')\n",
        "    parser.add_argument('--patience', type=int, default=3, help='Patience for early stopping')\n",
        "    parser.add_argument('--resume_checkpoint', type=str, default=None, help='Path to resume checkpoint')\n",
        "\n",
        "    # GIF saving parameters\n",
        "    parser.add_argument('--save_gifs_dir', type=str, default='gifs', help='Directory to save sample GIFs')\n",
        "    parser.add_argument('--num_gifs', type=int, default=10, help='Number of sample GIFs to save')\n",
        "    parser.add_argument('--alpha', type=float, default=0.3, help='Transparency factor for segmentation overlay')\n",
        "    # New argument for segmentation type\n",
        "    parser.add_argument('--segmentation_type', type=int, default=5, choices=range(0, 6),\n",
        "                        help='Type of segmentation overlay:\\n'\n",
        "                             '0 = Raw image\\n'\n",
        "                             '1 = Raw + Side1\\n'\n",
        "                             '2 = Raw + Side2\\n'\n",
        "                             '3 = Raw + Side1 + Side2\\n'\n",
        "                             '4 = Raw + Vesicles\\n'\n",
        "                             '5 = Raw + Side1 + Side2 + Vesicles')\n",
        "\n",
        "    args, _ = parser.parse_known_args()\n",
        "    return args\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    # Initialize processor\n",
        "    processor = SimpleVideoProcessor(size=(80, 80))\n",
        "\n",
        "    # List of all bboxes\n",
        "    # bboxes = [f\"bbox{i}\" for i in range(1, 8)]  # bbox1 to bbox7\n",
        "    bboxes =args.bbox_name # bbox1 to bbox7\n",
        "\n",
        "    # Load volumes for all bboxes\n",
        "    vol_data_dict = {}\n",
        "    for bbox_name in args.bbox_name:\n",
        "        print(bbox_name)\n",
        "        raw_vol, seg_vol, add_mask_vol = load_volumes(\n",
        "            bbox_name=bbox_name,\n",
        "            raw_base_dir=args.raw_base_dir,\n",
        "            seg_base_dir=args.seg_base_dir,\n",
        "            add_mask_base_dir=args.add_mask_base_dir\n",
        "        )\n",
        "        # print(raw_vol)\n",
        "        if raw_vol is not None and seg_vol is not None and add_mask_vol is not None:\n",
        "            vol_data_dict[bbox_name] = (raw_vol, seg_vol, add_mask_vol)\n",
        "        else:\n",
        "            print(f\"Skipping {bbox_name} due to missing volumes.\")\n",
        "\n",
        "    # Load synapse data for all bboxes\n",
        "    synapse_dfs = []\n",
        "    for bbox_name in bboxes:\n",
        "        excel_file_path = os.path.join(args.excel_file, f\"{bbox_name}.xlsx\")\n",
        "        if os.path.exists(excel_file_path):\n",
        "            df = pd.read_excel(excel_file_path)\n",
        "            df['bbox_name'] = bbox_name  # Add bbox_name column\n",
        "            synapse_dfs.append(df)\n",
        "        else:\n",
        "            print(f\"Excel file not found for {bbox_name}. Skipping.\")\n",
        "    syn_df = pd.concat(synapse_dfs, ignore_index=True)\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = VideoMAEDataset(\n",
        "        vol_data_dict=vol_data_dict,\n",
        "        synapse_df=syn_df,\n",
        "        processor=processor,\n",
        "        segmentation_type=args.segmentation_type,\n",
        "        subvol_size=args.subvol_size,\n",
        "        num_frames=args.num_frames,\n",
        "        alpha=args.alpha\n",
        "    )\n",
        "\n",
        "    # Process cubes and collect synapse data\n",
        "    cubes = []\n",
        "    syn_info_list = []  # List to collect synapse information\n",
        "\n",
        "    for idx in range(len(dataset)):\n",
        "        pixel_values, syn_info,bbox_name = dataset[idx]\n",
        "        cubes.append(pixel_values)\n",
        "\n",
        "        # Collect synapse info\n",
        "        syn_info_list.append(syn_info)\n",
        "\n",
        "    # Merge all synapse info into a single DataFrame\n",
        "    merged_syn_info = pd.DataFrame(syn_info_list)\n",
        "\n",
        "    print(f\"Processed {len(cubes)} cubes successfully.\")\n",
        "    return cubes, merged_syn_info\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     args = parse_args()\n",
        "#     main(args)\n",
        "\n",
        "\n",
        "args = parse_args()\n",
        "cubes , sys_inf= main(args)\n",
        "print(f\"Processed {len(cubes)} cubes successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import imageio\n",
        "\n",
        "# After running main() and getting the cubes list\n",
        "cube = cubes[0]  # Select the first cube (adjust index as needed)\n",
        "\n",
        "# Define normalization parameters (must match what's used in SimpleVideoProcessor)\n",
        "mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "\n",
        "# Denormalize the tensor\n",
        "denormalized_cube = cube * std + mean\n",
        "\n",
        "# Clamp values to valid [0,1] range and convert to numpy\n",
        "denormalized_cube = torch.clamp(denormalized_cube, 0, 1)\n",
        "frames = denormalized_cube.permute(0, 2, 3, 1).numpy()  # Change to (T, H, W, C)\n",
        "frames = (frames * 255).astype(np.uint8)  # Convert to 0-255\n",
        "\n",
        "imageio.mimsave('synapse_cube.gif', frames, fps=10)\n",
        "\n",
        "print(\"GIF saved successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeqQGyn8B5ZJ",
        "outputId": "85fb469d-ec6e-48f7-d39d-c9beaa8da17b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GIF saved successfully!\n"
          ]
        }
      ]
    }
  ]
}