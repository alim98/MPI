{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alim98/MPI/blob/main/Feature_Comp_Fully_interactve.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download"
      ],
      "metadata": {
        "id": "VLMtbhfvWAUU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04475a92-0227-4a29-993e-ab266c0e72a4",
        "id": "kgz2qneNCAsD"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-12 06:46:43--  https://drive.usercontent.google.com/download?id=1iHPBdBOPEagvPTHZmrN__LD49emXwReY&export=download&authuser=0&confirm=t&uuid=631d60dd-569c-4bb1-a9e8-d681f0ed3d43&at=APvzH3r4me8x_LwP3n8O7lgPo8oK%3A1733988188000\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.139.132, 2607:f8b0:400c:c05::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.139.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1264688649 (1.2G) [application/octet-stream]\n",
            "Saving to: ‘downloaded_file.zip’\n",
            "\n",
            "downloaded_file.zip 100%[===================>]   1.18G   135MB/s    in 13s     \n",
            "\n",
            "2025-01-12 06:46:58 (90.1 MB/s) - ‘downloaded_file.zip’ saved [1264688649/1264688649]\n",
            "\n",
            "replace seg/bbox1/slice_358.tif? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n",
            "--2025-01-12 06:51:23--  https://drive.usercontent.google.com/download?id=1rOpINPMO5yqXycFFbJsqURRECwMiHUmP&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.139.132, 2607:f8b0:400c:c05::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.139.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4443174 (4.2M) [application/octet-stream]\n",
            "Saving to: ‘ViT_all_features_merged.csv’\n",
            "\n",
            "ViT_all_features_me 100%[===================>]   4.24M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-01-12 06:51:27 (211 MB/s) - ‘ViT_all_features_merged.csv’ saved [4443174/4443174]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O downloaded_file.zip \"https://drive.usercontent.google.com/download?id=1iHPBdBOPEagvPTHZmrN__LD49emXwReY&export=download&authuser=0&confirm=t&uuid=631d60dd-569c-4bb1-a9e8-d681f0ed3d43&at=APvzH3r4me8x_LwP3n8O7lgPo8oK%3A1733988188000\"\n",
        "!pip -q install umap-learn\n",
        "!pip -q install dash\n",
        "!unzip -q downloaded_file.zip\n",
        "!wget -O ViT_all_features_merged.csv \"https://drive.usercontent.google.com/download?id=1rOpINPMO5yqXycFFbJsqURRECwMiHUmP&export=download\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# InterActive"
      ],
      "metadata": {
        "id": "RFklfBlXUbXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tifffile as tiff\n",
        "import imageio.v2 as imageio\n",
        "from PIL import Image, ImageDraw, ImageFont  # For overlaying titles\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import tempfile\n",
        "import plotly.express as px\n",
        "from dash import Dash, dcc, html, Input, Output, State\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import umap\n",
        "import glob\n",
        "from matplotlib.patches import Patch  # Add this import\n",
        "import dash\n",
        "import imageio\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "from google.colab import files\n",
        "\n",
        "def extract_patch(slice_data, x_center, y_center, cube_size):\n",
        "    half_size = cube_size // 2\n",
        "\n",
        "    x_start = x_center - half_size\n",
        "    y_start = y_center - half_size\n",
        "    x_end = x_start + cube_size\n",
        "    y_end = y_start + cube_size\n",
        "\n",
        "    # Extract the patch; this might go out of bounds\n",
        "    patch = slice_data[y_start:y_end, x_start:x_end]\n",
        "\n",
        "    # Calculate padding if the patch goes out of image boundaries\n",
        "    pad_left = max(0, -x_start)\n",
        "    pad_top = max(0, -y_start)\n",
        "    pad_right = max(0, x_end - slice_data.shape[1])\n",
        "    pad_bottom = max(0, y_end - slice_data.shape[0])\n",
        "\n",
        "    if pad_left or pad_top or pad_right or pad_bottom:\n",
        "        patch = np.pad(\n",
        "            patch,\n",
        "            ((pad_top, pad_bottom), (pad_left, pad_right)),\n",
        "            mode='constant',\n",
        "            constant_values=0\n",
        "        )\n",
        "\n",
        "    # Ensure the patch is exactly (cube_size, cube_size)\n",
        "    patch = patch[:cube_size, :cube_size]\n",
        "\n",
        "    # In case the patch is still smaller due to extreme boundary conditions\n",
        "    if patch.shape[0] < cube_size or patch.shape[1] < cube_size:\n",
        "        patch = np.pad(\n",
        "            patch,\n",
        "            (\n",
        "                (0, cube_size - patch.shape[0]),\n",
        "                (0, cube_size - patch.shape[1])\n",
        "            ),\n",
        "            mode='constant',\n",
        "            constant_values=0\n",
        "        )\n",
        "        patch = patch[:cube_size, :cube_size]\n",
        "\n",
        "    # Final assertion to ensure patch size\n",
        "    assert patch.shape == (cube_size, cube_size), f\"Patch shape {patch.shape} is not ({cube_size}, {cube_size})\"\n",
        "\n",
        "    return patch\n",
        "\n",
        "# -----------------------------------------------------------------------------------\n",
        "# Function: Extract Cube from Slices\n",
        "# -----------------------------------------------------------------------------------\n",
        "def extract_cube(folder_path, central_coords, cube_size=80):\n",
        "    x_center, y_center, z_center = central_coords\n",
        "    half_size = cube_size // 2\n",
        "    cube = []\n",
        "\n",
        "    # Load the central slice first to get image dimensions\n",
        "    central_slice_path = os.path.join(folder_path, f\"slice_{z_center}.tif\")\n",
        "    if not os.path.exists(central_slice_path):\n",
        "        raise FileNotFoundError(f\"Central slice {central_slice_path} does not exist.\")\n",
        "\n",
        "    central_slice = tiff.imread(central_slice_path)\n",
        "    img_height, img_width = central_slice.shape\n",
        "    print(f\"Central slice dimensions: {central_slice.shape}\")\n",
        "\n",
        "    for z in range(z_center - half_size, z_center + half_size):\n",
        "        slice_path = os.path.join(folder_path, f\"slice_{z}.tif\")\n",
        "        if os.path.exists(slice_path):\n",
        "            slice_data = tiff.imread(slice_path)\n",
        "            # Verify slice dimensions\n",
        "            if slice_data.shape != (img_height, img_width):\n",
        "                raise ValueError(f\"Slice {z} has inconsistent dimensions: {slice_data.shape}\")\n",
        "            # Extract the patch with proper padding\n",
        "            try:\n",
        "                patch = extract_patch(slice_data, x_center, y_center, cube_size)\n",
        "                cube.append(patch)\n",
        "                print(f\"Extracted patch from slice {z}\")\n",
        "            except AssertionError as ae:\n",
        "                print(f\"Assertion Error for slice {z}: {ae}\")\n",
        "                # Append a zero-filled patch to maintain consistency\n",
        "                cube.append(np.zeros((cube_size, cube_size)))\n",
        "        else:\n",
        "            # If the slice doesn't exist, append a zero-filled patch\n",
        "            print(f\"Slice {z} does not exist. Appending zero-filled patch.\")\n",
        "            cube.append(np.zeros((cube_size, cube_size)))\n",
        "\n",
        "    # Verify that all patches have the same shape\n",
        "    patch_shapes = [patch.shape for patch in cube]\n",
        "    if not all(shape == (cube_size, cube_size) for shape in patch_shapes):\n",
        "        raise ValueError(\"Not all patches have the shape (cube_size, cube_size).\")\n",
        "    else:\n",
        "        print(\"All patches have consistent shapes.\")\n",
        "\n",
        "    return np.stack(cube)\n",
        "\n",
        "def _generate_gif( sub_raw, sub_mask_1, sub_mask_2, syn_info, z_start):\n",
        "    fig, ax = plt.subplots(figsize=(5, 5))\n",
        "    im = ax.imshow(sub_raw[0], cmap='gray', interpolation='nearest')\n",
        "    ax.axis('off')\n",
        "\n",
        "    legend_patches = [\n",
        "        Patch(facecolor='red', edgecolor='red', label='Side 1'),\n",
        "        Patch(facecolor='blue', edgecolor='blue', label='Side 2')\n",
        "    ]\n",
        "    ax.legend(handles=legend_patches, loc='upper right', frameon=True)\n",
        "\n",
        "    def update(frame):\n",
        "        overlay = np.stack([sub_raw[frame]] * 3, axis=-1).astype(float)\n",
        "\n",
        "        overlay[sub_mask_1[frame]] = overlay[sub_mask_1[frame]] * 0.7 + np.array([255, 0, 0]) * 0.3\n",
        "        overlay[sub_mask_2[frame]] = overlay[sub_mask_2[frame]] * 0.7 + np.array([0, 0, 255]) * 0.3\n",
        "\n",
        "        overlay = np.clip(overlay, 0, 255).astype(np.uint8)\n",
        "        ax.set_title(f\"{syn_info['Var1']} - Z: {z_start + frame}\")\n",
        "        im.set_data(overlay)\n",
        "        return [im]\n",
        "\n",
        "    ani = animation.FuncAnimation(fig, update, frames=sub_raw.shape[0], interval=100, blit=True)\n",
        "\n",
        "    gif_filename = f\"{syn_info['Var1']}.gif\"\n",
        "    ani.save(gif_filename, writer='pillow', fps=10)\n",
        "    plt.close(fig)\n",
        "    print(f\"Saved: {gif_filename}\")\n",
        "def create_segment_masks(side1_coord, side2_coord,seg_vol):\n",
        "    x1, y1, z1 = map(int, side1_coord)\n",
        "    x2, y2, z2 = map(int, side2_coord)\n",
        "\n",
        "    seg_id_1 = seg_vol[z1, y1, x1]\n",
        "    seg_id_2 = seg_vol[z2, y2, x2]\n",
        "\n",
        "    mask_1 = (seg_vol == seg_id_1) if seg_id_1 != 0 else np.zeros_like(seg_vol, dtype=bool)\n",
        "    mask_2 = (seg_vol == seg_id_2) if seg_id_2 != 0 else np.zeros_like(seg_vol, dtype=bool)\n",
        "    return mask_1, mask_2\n",
        "# -----------------------------------------------------------------------------------\n",
        "# Function: Create GIF with Titles Using Pillow for Consistent Frame Sizes\n",
        "# -----------------------------------------------------------------------------------\n",
        "def create_gif_with_titles(cube, output_path, var1, bbox_name, z_center, side1_coord, side2_coord ):\n",
        "    frames = []\n",
        "    # Define fixed figure size and DPI\n",
        "    fig_size = (4, 4)  # inches\n",
        "    fig_dpi = 100       # dots per inch\n",
        "\n",
        "    # Define font for text overlay using Pillow\n",
        "    try:\n",
        "        font = ImageFont.truetype(\"arial.ttf\", 15)  # Adjust the font path and size as needed\n",
        "    except IOError:\n",
        "        font = ImageFont.load_default()\n",
        "\n",
        "    for i, slice_data in enumerate(cube):\n",
        "        fig, ax = plt.subplots(figsize=fig_size, dpi=fig_dpi)\n",
        "        ax.imshow(slice_data, cmap='gray')\n",
        "        ax.axis('off')\n",
        "\n",
        "        # Save figure to a bytes buffer without title\n",
        "        buf = io.BytesIO()\n",
        "        plt.savefig(buf, format='png', bbox_inches='tight', pad_inches=0)\n",
        "        buf.seek(0)\n",
        "        frame = Image.open(buf).convert(\"RGBA\")\n",
        "        plt.close(fig)\n",
        "\n",
        "        # Prepare the title\n",
        "        slice_number = z_center - (len(cube) // 2) + i\n",
        "        title = f\"Var1: {var1}, BBox: {bbox_name}, Slice: {slice_number}\"\n",
        "\n",
        "        # Overlay the title on the image using Pillow\n",
        "        draw = ImageDraw.Draw(frame)\n",
        "        text_position = (10, 10)  # Top-left corner; adjust as needed\n",
        "        draw.text(text_position, title, font=font, fill=(255, 255, 255, 255))  # White text\n",
        "\n",
        "        # Convert back to numpy array\n",
        "        frame_np = np.array(frame)\n",
        "        frames.append(frame_np)\n",
        "\n",
        "    # Verify all frames have the same shape\n",
        "    frame_shapes = [frame.shape for frame in frames]\n",
        "    first_shape = frame_shapes[0]\n",
        "    inconsistent = False\n",
        "    for idx, shape in enumerate(frame_shapes):\n",
        "        if shape != first_shape:\n",
        "            print(f\"Frame {idx} has shape {shape}, expected {first_shape}.\")\n",
        "            inconsistent = True\n",
        "    if inconsistent:\n",
        "        raise ValueError(\"Not all frames have the same shape.\")\n",
        "    else:\n",
        "        print(\"All frames have consistent shapes.\")\n",
        "\n",
        "    # Save the GIF using the updated imageio import\n",
        "    imageio.mimsave(output_path, frames, fps=5)\n",
        "    print(f\"GIF saved to {output_path}\")\n",
        "\n",
        "# -----------------------------------------------------------------------------------\n",
        "# Function: Process Features and Apply UMAP\n",
        "# -----------------------------------------------------------------------------------\n",
        "def process_features(df):\n",
        "    feature_cols = [c for c in df.columns if c.startswith('feat_')]\n",
        "    features = df[feature_cols].values\n",
        "    features_scaled = StandardScaler().fit_transform(features)\n",
        "    return features_scaled\n",
        "\n",
        "def reduce_to_umap(features_scaled):\n",
        "    umap_direct = umap.UMAP(n_components=2, random_state=42)\n",
        "    return umap_direct.fit_transform(features_scaled)\n",
        "\n",
        "app = Dash(__name__)\n",
        "\n",
        "file_path = 'ViT_all_features_merged.csv'  # Ensure this is the correct path\n",
        "if not os.path.exists(file_path):\n",
        "    raise FileNotFoundError(f\"CSV file {file_path} does not exist.\")\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Add 'id' column for unique identification\n",
        "df.reset_index(inplace=True)\n",
        "df.rename(columns={'index': 'id'}, inplace=True)\n",
        "\n",
        "# Process Features and Apply UMAP\n",
        "features_scaled = process_features(df)\n",
        "umap_direct_result = reduce_to_umap(features_scaled)\n",
        "\n",
        "df['umap_1'] = umap_direct_result[:, 0]\n",
        "df['umap_2'] = umap_direct_result[:, 1]\n",
        "\n",
        "# -----------------------------------------------------------------------------------\n",
        "# Dash Layout\n",
        "# -----------------------------------------------------------------------------------\n",
        "app.layout = html.Div([\n",
        "    dcc.Graph(\n",
        "        id='umap-plot',\n",
        "        figure=px.scatter(\n",
        "            df, x='umap_1', y='umap_2', color='bbox_name',\n",
        "            hover_data=['id', 'Var1', 'central_coord_1', 'central_coord_2', 'central_coord_3', 'bbox_name','side_1_coord_1','side_1_coord_2','side_1_coord_3','side_2_coord_1','side_2_coord_2','side_2_coord_3'],\n",
        "            title=\"ViT - Direct UMAP\"\n",
        "        ).update_layout(\n",
        "            xaxis=dict(scaleanchor=\"y\",\n",
        "                       scaleratio=1),\n",
        "            yaxis=dict(scaleanchor=\"x\",\n",
        "                       scaleratio=1),\n",
        "            height=1300,\n",
        "            width=1500)\n",
        "    ),\n",
        "    html.Button(\"Done\", id='done-button', n_clicks=0, style={'marginTop': 20}),\n",
        "    dcc.Download(id='download-gif'),\n",
        "    dcc.Download(id='download-log'),\n",
        "    html.Div(id='gif-output', style={'marginTop': 20}),\n",
        "    dcc.Store(id='log-store', data=[])  # Store to keep track of clicked data points\n",
        "])\n",
        "\n",
        "# -----------------------------------------------------------------------------------\n",
        "# Callback to Generate GIF and Log Clicked Data\n",
        "# -----------------------------------------------------------------------------------\n",
        "@app.callback(\n",
        "    [\n",
        "        Output('gif-output', 'children'),\n",
        "        Output('download-gif', 'data'),\n",
        "        Output('log-store', 'data')\n",
        "    ],\n",
        "    [\n",
        "        Input('umap-plot', 'clickData')\n",
        "    ],\n",
        "    [\n",
        "        State('log-store', 'data')\n",
        "    ]\n",
        ")\n",
        "def generate_gif(click_data, existing_log):\n",
        "    if click_data:\n",
        "        try:\n",
        "            # Extract clicked sample details\n",
        "            point = click_data['points'][0]\n",
        "            customdata = point['customdata']\n",
        "            id_ = customdata[0]\n",
        "            var1 = customdata[1]\n",
        "            central_x = int(customdata[2])\n",
        "            central_y = int(customdata[3])\n",
        "            central_z = int(customdata[4])\n",
        "            bbox_name = customdata[5]\n",
        "            side1_coord = (int(customdata[6]), int(customdata[7]), int(customdata[8]))\n",
        "            side2_coord = (int(customdata[9]), int(customdata[10]), int(customdata[11]))\n",
        "\n",
        "            central_coords = (central_x, central_y, central_z)\n",
        "\n",
        "            # Load segmentation volume\n",
        "            seg_base_dir = \"seg\"\n",
        "            seg_dir = os.path.join(seg_base_dir, bbox_name)\n",
        "            seg_files = sorted(glob.glob(os.path.join(seg_dir, 'slice_*.tif')))\n",
        "            seg_vol = np.stack([imageio.imread(f).astype(np.uint32) for f in seg_files], axis=0)\n",
        "\n",
        "            # Crop segmentation volume to match the raw subvolume\n",
        "            cube_size = 80\n",
        "            half_size = cube_size // 2\n",
        "\n",
        "            z_start = max(central_z - half_size, 0)\n",
        "            z_end = min(central_z + half_size, seg_vol.shape[0])\n",
        "            y_start = max(central_y - half_size, 0)\n",
        "            y_end = min(central_y + half_size, seg_vol.shape[1])\n",
        "            x_start = max(central_x - half_size, 0)\n",
        "            x_end = min(central_x + half_size, seg_vol.shape[2])\n",
        "\n",
        "            seg_vol_cropped = seg_vol[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "\n",
        "            # Create masks for the cropped segmentation volume\n",
        "            mask_1_full, mask_2_full = create_segment_masks(side1_coord, side2_coord, seg_vol)\n",
        "            mask_1_cropped = mask_1_full[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "            mask_2_cropped = mask_2_full[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "\n",
        "            # Load raw cube\n",
        "            folder_path = os.path.join('raw', bbox_name)\n",
        "            if not os.path.exists(folder_path):\n",
        "                return f\"Folder {folder_path} does not exist.\", None, existing_log\n",
        "\n",
        "            sub_raw = extract_cube(folder_path, central_coords)\n",
        "\n",
        "            # Generate GIF\n",
        "            syn_info = {'Var1': var1}\n",
        "            gif_filename = f\"{var1}.gif\"\n",
        "\n",
        "            # Update the log\n",
        "            log_entry = {\n",
        "                'id': id_,\n",
        "                'synapse_name': var1,\n",
        "                'central_coord_1': central_x,\n",
        "                'central_coord_2': central_y,\n",
        "                'central_coord_3': central_z,\n",
        "                'bbox_name': bbox_name,\n",
        "                'center_slice': central_z\n",
        "            }\n",
        "            updated_log = existing_log.copy()\n",
        "            updated_log.append(log_entry)\n",
        "\n",
        "            # Provide GIF for download\n",
        "            return f\"GIF generated: {gif_filename}\", dcc.send_file(gif_filename), updated_log\n",
        "        except Exception as e:\n",
        "            return f\"Error generating GIF: {str(e)}\", None, existing_log\n",
        "\n",
        "    return \"Click on a point to generate a GIF.\", None, existing_log\n",
        "\n",
        "\n",
        "@app.callback(\n",
        "    Output('download-log', 'data'),\n",
        "    [\n",
        "        Input('done-button', 'n_clicks')\n",
        "    ],\n",
        "    [\n",
        "        State('log-store', 'data')\n",
        "    ],\n",
        "    prevent_initial_call=True\n",
        ")\n",
        "def export_log(n_clicks, log_data):\n",
        "    if n_clicks > 0 and log_data:\n",
        "        log_df = pd.DataFrame(log_data)\n",
        "\n",
        "        csv_filename = \"clicked_data_log.csv\"\n",
        "\n",
        "        return dcc.send_data_frame(log_df.to_csv, csv_filename, index=False)\n",
        "\n",
        "    return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run_server(debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "id": "aHOIHEt7UMsm",
        "outputId": "1a52f49f-d8c6-4c4f-b25c-bad05b100d02"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
            "\n",
            "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/umap/umap_.py:1952: UserWarning:\n",
            "\n",
            "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "    const iframe = document.createElement('iframe');\n",
              "    iframe.src = new URL(path, url).toString();\n",
              "    iframe.height = height;\n",
              "    iframe.width = width;\n",
              "    iframe.style.border = 0;\n",
              "    iframe.allow = [\n",
              "        'accelerometer',\n",
              "        'autoplay',\n",
              "        'camera',\n",
              "        'clipboard-read',\n",
              "        'clipboard-write',\n",
              "        'gyroscope',\n",
              "        'magnetometer',\n",
              "        'microphone',\n",
              "        'serial',\n",
              "        'usb',\n",
              "        'xr-spatial-tracking',\n",
              "    ].join('; ');\n",
              "    element.appendChild(iframe);\n",
              "  })(8050, \"/\", \"100%\", 650, false, window.element)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Bp-X5vz9IpKeOPJmE7YjP97rB5kaFZ55",
      "authorship_tag": "ABX9TyMdI7Fqoj9Vzxowrz+HpHGQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}