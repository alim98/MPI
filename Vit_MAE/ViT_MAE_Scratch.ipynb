{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNmzKhXGYX2jXW7+SPwiNfm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alim98/MPI/blob/main/Vit_MAE/ViT_MAE_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O downloaded_file.zip \"https://drive.usercontent.google.com/download?id=1iHPBdBOPEagvPTHZmrN__LD49emXwReY&export=download&authuser=0&confirm=t&uuid=631d60dd-569c-4bb1-a9e8-d681f0ed3d43&at=APvzH3r4me8x_LwP3n8O7lgPo8oK%3A1733988188000\"\n",
        "\n",
        "!pip -q install transformers scikit-learn matplotlib seaborn torch torchvision umap-learn openpyxl imageio\n",
        "\n",
        "!unzip -q downloaded_file.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDenC5yq3xLW",
        "outputId": "9c54ec6d-7739-4b58-8c87-e3d814355e28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-14 17:40:14--  https://drive.usercontent.google.com/download?id=1iHPBdBOPEagvPTHZmrN__LD49emXwReY&export=download&authuser=0&confirm=t&uuid=631d60dd-569c-4bb1-a9e8-d681f0ed3d43&at=APvzH3r4me8x_LwP3n8O7lgPo8oK%3A1733988188000\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.101.132, 2607:f8b0:4023:c06::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.101.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1264688649 (1.2G) [application/octet-stream]\n",
            "Saving to: ‘downloaded_file.zip’\n",
            "\n",
            "downloaded_file.zip 100%[===================>]   1.18G  70.1MB/s    in 20s     \n",
            "\n",
            "2025-01-14 17:40:36 (61.0 MB/s) - ‘downloaded_file.zip’ saved [1264688649/1264688649]\n",
            "\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjMP5mOs1pDv",
        "outputId": "40c03e3b-8aa2-4046-b62c-7a1b1e77c7c5"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Total synapses loaded: 220\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/100: 100%|██████████| 176/176 [17:41<00:00,  6.03s/it, Loss=0.175]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/100], Loss: 0.1574\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/100: 100%|██████████| 176/176 [17:50<00:00,  6.08s/it, Loss=0.0844]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/100], Loss: 0.1271\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/100:  38%|███▊      | 67/176 [06:47<10:57,  6.03s/it, Loss=0.127]"
          ]
        }
      ],
      "source": [
        "# mae3d_reconstruction.py\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Import Necessary Libraries\n",
        "# -----------------------------\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import imageio.v2 as imageio\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Data Loading Functions\n",
        "# -----------------------------\n",
        "\n",
        "def load_volumes(bbox_name, raw_base_dir, seg_base_dir):\n",
        "    \"\"\"\n",
        "    Load raw volume and segmentation volume for a bounding box.\n",
        "\n",
        "    Args:\n",
        "        bbox_name (str): Name of the bounding box directory.\n",
        "        raw_base_dir (str): Base directory for raw data.\n",
        "        seg_base_dir (str): Base directory for segmentation data.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (raw_vol, seg_vol) each as np.ndarray or (None, None) if loading fails.\n",
        "    \"\"\"\n",
        "    raw_dir = os.path.join(raw_base_dir, bbox_name)\n",
        "    seg_dir = os.path.join(seg_base_dir, bbox_name)\n",
        "\n",
        "    raw_tif_files = sorted(glob.glob(os.path.join(raw_dir, 'slice_*.tif')))\n",
        "    seg_tif_files = sorted(glob.glob(os.path.join(seg_dir, 'slice_*.tif')))\n",
        "\n",
        "    if len(raw_tif_files) == 0:\n",
        "        print(f\"No raw files found for {bbox_name} in {raw_dir}\")\n",
        "        return None, None\n",
        "\n",
        "    if len(seg_tif_files) == 0:\n",
        "        print(f\"No segmentation files found for {bbox_name} in {seg_dir}\")\n",
        "        return None, None\n",
        "\n",
        "    if len(raw_tif_files) != len(seg_tif_files):\n",
        "        print(f\"Mismatch in number of raw vs seg slices for {bbox_name}. Skipping.\")\n",
        "        return None, None\n",
        "\n",
        "    try:\n",
        "        raw_vol = np.stack([imageio.imread(f) for f in raw_tif_files], axis=0)  # shape: (Z, Y, X)\n",
        "        seg_vol = np.stack([imageio.imread(f).astype(np.uint32) for f in seg_tif_files], axis=0)\n",
        "        return raw_vol, seg_vol\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading volumes for {bbox_name}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def create_segmented_cube(\n",
        "    raw_vol,\n",
        "    seg_vol,\n",
        "    central_coord,\n",
        "    side1_coord,\n",
        "    side2_coord,\n",
        "    subvolume_size=80,\n",
        "    alpha=0.3\n",
        "):\n",
        "    \"\"\"\n",
        "    Constructs an 80x80x80 segmented 3D cube around the specified synapse coordinates\n",
        "    and overlays both segmentation masks (side1_coord, side2_coord) on the raw data\n",
        "    with specified transparency for each slice.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Overlaid cube of shape (3, 80, 80, 80).\n",
        "                    Channels: [Mask1, Raw, Mask2]\n",
        "    \"\"\"\n",
        "\n",
        "    def create_segment_masks(segmentation_volume, s1_coord, s2_coord):\n",
        "        x1, y1, z1 = s1_coord\n",
        "        x2, y2, z2 = s2_coord\n",
        "        # Validate within volume\n",
        "        if not (0 <= z1 < segmentation_volume.shape[0] and\n",
        "                0 <= y1 < segmentation_volume.shape[1] and\n",
        "                0 <= x1 < segmentation_volume.shape[2]):\n",
        "            raise ValueError(\"Side1 coordinates are out of bounds.\")\n",
        "\n",
        "        if not (0 <= z2 < segmentation_volume.shape[0] and\n",
        "                0 <= y2 < segmentation_volume.shape[1] and\n",
        "                0 <= x2 < segmentation_volume.shape[2]):\n",
        "            raise ValueError(\"Side2 coordinates are out of bounds.\")\n",
        "\n",
        "        seg_id_1 = segmentation_volume[z1, y1, x1]\n",
        "        seg_id_2 = segmentation_volume[z2, y2, x2]\n",
        "\n",
        "        # If seg_id == 0, it means no segment at that voxel\n",
        "        if seg_id_1 == 0:\n",
        "            mask_1 = np.zeros_like(segmentation_volume, dtype=bool)\n",
        "        else:\n",
        "            mask_1 = (segmentation_volume == seg_id_1)\n",
        "\n",
        "        if seg_id_2 == 0:\n",
        "            mask_2 = np.zeros_like(segmentation_volume, dtype=bool)\n",
        "        else:\n",
        "            mask_2 = (segmentation_volume == seg_id_2)\n",
        "\n",
        "        return mask_1, mask_2\n",
        "\n",
        "    # Build masks\n",
        "    mask_1_full, mask_2_full = create_segment_masks(seg_vol, side1_coord, side2_coord)\n",
        "\n",
        "    # Define subvolume bounds\n",
        "    half_size = subvolume_size // 2\n",
        "    cx, cy, cz = central_coord\n",
        "\n",
        "    x_start, x_end = max(cx - half_size, 0), min(cx + half_size, raw_vol.shape[2])\n",
        "    y_start, y_end = max(cy - half_size, 0), min(cy + half_size, raw_vol.shape[1])\n",
        "    z_start, z_end = max(cz - half_size, 0), min(cz + half_size, raw_vol.shape[0])\n",
        "\n",
        "    # Extract subvolumes\n",
        "    sub_raw = raw_vol[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "    sub_mask_1 = mask_1_full[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "    sub_mask_2 = mask_2_full[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "\n",
        "    # Pad if smaller than subvolume_size\n",
        "    pad_z = subvolume_size - sub_raw.shape[0]\n",
        "    pad_y = subvolume_size - sub_raw.shape[1]\n",
        "    pad_x = subvolume_size - sub_raw.shape[2]\n",
        "\n",
        "    if pad_z > 0 or pad_y > 0 or pad_x > 0:\n",
        "        sub_raw = np.pad(sub_raw, ((0, pad_z), (0, pad_y), (0, pad_x)),\n",
        "                         mode='constant', constant_values=0)\n",
        "        sub_mask_1 = np.pad(sub_mask_1, ((0, pad_z), (0, pad_y), (0, pad_x)),\n",
        "                            mode='constant', constant_values=False)\n",
        "        sub_mask_2 = np.pad(sub_mask_2, ((0, pad_z), (0, pad_y), (0, pad_x)),\n",
        "                            mode='constant', constant_values=False)\n",
        "\n",
        "    # Slice to exact shape\n",
        "    sub_raw = sub_raw[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "    sub_mask_1 = sub_mask_1[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "    sub_mask_2 = sub_mask_2[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "\n",
        "    # Initialize cube for segmented channels\n",
        "    # Channels: [Mask1, Raw, Mask2]\n",
        "    overlaid_cube = np.zeros((3, subvolume_size, subvolume_size, subvolume_size), dtype=np.float32)\n",
        "\n",
        "    # Normalize raw channel to [0, 1]\n",
        "    raw_normalized = sub_raw.astype(np.float32)\n",
        "    raw_min, raw_max = raw_normalized.min(), raw_normalized.max()\n",
        "    if raw_max != raw_min:\n",
        "        raw_normalized = (raw_normalized - raw_min) / (raw_max - raw_min)\n",
        "    else:\n",
        "        raw_normalized = raw_normalized - raw_min  # All zeros\n",
        "\n",
        "    # Assign channels\n",
        "    overlaid_cube[0] = sub_mask_1.astype(np.float32)  # Mask1\n",
        "    overlaid_cube[1] = raw_normalized                # Raw\n",
        "    overlaid_cube[2] = sub_mask_2.astype(np.float32)  # Mask2\n",
        "\n",
        "    return overlaid_cube  # Shape: (3, 80, 80, 80)\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Dataset Definition\n",
        "# -----------------------------\n",
        "\n",
        "class ElectronMicroscopyDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for loading 3D Electron Microscopy data using custom data loader.\n",
        "    Handles multiple Excel files, each corresponding to a different bounding box.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, bbox_names, raw_base_dir, seg_base_dir, excel_dir, subvolume_size=80, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            bbox_names (list): List of bounding box names to include in the dataset.\n",
        "            raw_base_dir (str): Base directory containing raw data.\n",
        "            seg_base_dir (str): Base directory containing segmentation data.\n",
        "            excel_dir (str): Directory containing Excel files for each bounding box.\n",
        "                             Each Excel file should be named as '<bbox_name>.xlsx'.\n",
        "            subvolume_size (int): Size of the subvolume to extract (default: 80).\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.bbox_names = bbox_names\n",
        "        self.raw_base_dir = raw_base_dir\n",
        "        self.seg_base_dir = seg_base_dir\n",
        "        self.excel_dir = excel_dir\n",
        "        self.subvolume_size = subvolume_size\n",
        "        self.transform = transform\n",
        "\n",
        "        # Initialize an empty list to hold dataframes\n",
        "        self.synapse_data_list = []\n",
        "\n",
        "        # Load synapse data from each Excel file corresponding to the bbox_names\n",
        "        for bbox_name in self.bbox_names:\n",
        "            excel_file = os.path.join(self.excel_dir, f\"{bbox_name}.xlsx\")\n",
        "            if not os.path.exists(excel_file):\n",
        "                print(f\"Excel file {excel_file} not found for {bbox_name}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                syn_df = pd.read_excel(excel_file)\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading Excel file '{excel_file}': {e}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Ensure required columns exist\n",
        "            required_columns = [\n",
        "                'central_coord_1', 'central_coord_2', 'central_coord_3',\n",
        "                'side_1_coord_1', 'side_1_coord_2', 'side_1_coord_3',\n",
        "                'side_2_coord_1', 'side_2_coord_2', 'side_2_coord_3'\n",
        "            ]\n",
        "            for col in required_columns:\n",
        "                if col not in syn_df.columns:\n",
        "                    print(f\"Missing required column '{col}' in Excel file '{excel_file}'. Skipping.\")\n",
        "                    syn_df = None\n",
        "                    break\n",
        "            if syn_df is not None:\n",
        "                # Add a column for bbox_name to keep track\n",
        "                syn_df['bbox_name'] = bbox_name\n",
        "                self.synapse_data_list.append(syn_df)\n",
        "\n",
        "        if not self.synapse_data_list:\n",
        "            raise ValueError(\"No valid synapse data loaded. Please check your Excel files and directories.\")\n",
        "\n",
        "        # Concatenate all synapse data into a single dataframe\n",
        "        self.synapse_data = pd.concat(self.synapse_data_list, ignore_index=True)\n",
        "        print(f\"Total synapses loaded: {len(self.synapse_data)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.synapse_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        syn_info = self.synapse_data.iloc[idx]\n",
        "        bbox_name = syn_info['bbox_name']\n",
        "\n",
        "        # Load volumes\n",
        "        raw_vol, seg_vol = load_volumes(bbox_name, self.raw_base_dir, self.seg_base_dir)\n",
        "        # if raw_vol is None or seg_vol is None:\n",
        "        #     # Return dummy data if volumes not found\n",
        "        #     dummy = torch.zeros((3, self.subvolume_size, self.subvolume_size, self.subvolume_size), dtype=torch.float32)\n",
        "        #     return dummy, dummy\n",
        "\n",
        "        # Extract coordinates\n",
        "        try:\n",
        "            central_coord = (\n",
        "                int(syn_info['central_coord_1']),\n",
        "                int(syn_info['central_coord_2']),\n",
        "                int(syn_info['central_coord_3'])\n",
        "            )\n",
        "            side1_coord = (\n",
        "                int(syn_info['side_1_coord_1']),\n",
        "                int(syn_info['side_1_coord_2']),\n",
        "                int(syn_info['side_1_coord_3'])\n",
        "            )\n",
        "            side2_coord = (\n",
        "                int(syn_info['side_2_coord_1']),\n",
        "                int(syn_info['side_2_coord_2']),\n",
        "                int(syn_info['side_2_coord_3'])\n",
        "            )\n",
        "        except KeyError as e:\n",
        "            raise KeyError(f\"Missing coordinate column: {e}\")\n",
        "        except ValueError as e:\n",
        "            raise ValueError(f\"Invalid coordinate value: {e}\")\n",
        "\n",
        "        # Create the overlaid segmented cube\n",
        "        overlaid_cube = create_segmented_cube(\n",
        "            raw_vol=raw_vol,\n",
        "            seg_vol=seg_vol,\n",
        "            central_coord=central_coord,\n",
        "            side1_coord=side1_coord,\n",
        "            side2_coord=side2_coord,\n",
        "            subvolume_size=self.subvolume_size,\n",
        "            alpha=0.3\n",
        "        )  # shape: (3, 80, 80, 80)\n",
        "\n",
        "        # Apply optional transforms\n",
        "        if self.transform:\n",
        "            overlaid_cube = self.transform(overlaid_cube)\n",
        "\n",
        "        # Convert to torch tensor\n",
        "        overlaid_cube = torch.from_numpy(overlaid_cube).float()  # Shape: (3, 80, 80, 80)\n",
        "\n",
        "        # For MAE, the target is the same as input\n",
        "        return overlaid_cube, overlaid_cube\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Model Components\n",
        "# -----------------------------\n",
        "\n",
        "class PatchEmbed3D(nn.Module):\n",
        "    \"\"\"\n",
        "    3D Patch Embedding Layer\n",
        "\n",
        "    Splits the input into patches and embeds them.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=3, patch_size=4, embed_dim=768):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            in_channels (int): Number of input channels (default: 3).\n",
        "            patch_size (int): Size of each 3D patch (default: 4).\n",
        "            embed_dim (int): Dimension of the embedding (default: 768).\n",
        "        \"\"\"\n",
        "        super(PatchEmbed3D, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.proj = nn.Conv3d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (B, C, D, H, W)\n",
        "        Returns:\n",
        "            torch.Tensor: Embedded patches of shape (B, N, E)\n",
        "        \"\"\"\n",
        "        x = self.proj(x)  # (B, E, D/P, H/P, W/P)\n",
        "        x = x.flatten(2)  # (B, E, N)\n",
        "        x = x.transpose(1, 2)  # (B, N, E)\n",
        "        return x\n",
        "\n",
        "class PositionalEncoding3D(nn.Module):\n",
        "    \"\"\"\n",
        "    3D Positional Encoding using learnable embeddings\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_patches):\n",
        "        super(PositionalEncoding3D, self).__init__()\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pos_embed\n",
        "\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Single Transformer Encoder Block\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, drop=0.0, attn_drop=0.0):\n",
        "        super(TransformerEncoderBlock, self).__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=attn_drop)\n",
        "        self.drop_path = nn.Identity()  # Can implement stochastic depth if needed\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, int(embed_dim * mlp_ratio)),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(int(embed_dim * mlp_ratio), embed_dim),\n",
        "            nn.Dropout(drop)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Self-Attention\n",
        "        x2 = self.norm1(x)\n",
        "        attn_output, _ = self.attn(x2.transpose(0,1), x2.transpose(0,1), x2.transpose(0,1))\n",
        "        attn_output = attn_output.transpose(0,1)\n",
        "        x = x + self.drop_path(attn_output)\n",
        "        # MLP\n",
        "        x2 = self.norm2(x)\n",
        "        x = x + self.drop_path(self.mlp(x2))\n",
        "        return x\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer Encoder consisting of multiple Encoder Blocks\n",
        "    \"\"\"\n",
        "    def __init__(self, depth, embed_dim, num_heads, mlp_ratio=4.0, drop=0.0, attn_drop=0.0):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerEncoderBlock(embed_dim, num_heads, mlp_ratio, drop, attn_drop)\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "class MaskToken(nn.Module):\n",
        "    \"\"\"\n",
        "    Mask Token to be added to the masked positions\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim):\n",
        "        super(MaskToken, self).__init__()\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        nn.init.trunc_normal_(self.mask_token, std=0.02)\n",
        "\n",
        "    def forward(self, B, num_mask, embed_dim):\n",
        "        \"\"\"\n",
        "        Generates mask tokens for each sample in the batch.\n",
        "\n",
        "        Args:\n",
        "            B (int): Batch size.\n",
        "            num_mask (int): Number of masked patches per sample.\n",
        "            embed_dim (int): Embedding dimension.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Mask tokens of shape (B, num_mask, embed_dim).\n",
        "        \"\"\"\n",
        "        # Expand the mask token to (B, num_mask, embed_dim)\n",
        "        return self.mask_token.expand(B, num_mask, embed_dim).clone()\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Single Decoder Block\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, drop=0.0, attn_drop=0.0):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=attn_drop)\n",
        "        self.drop_path = nn.Identity()\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, int(embed_dim * mlp_ratio)),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(int(embed_dim * mlp_ratio), embed_dim),\n",
        "            nn.Dropout(drop)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Self-Attention\n",
        "        x2 = self.norm1(x)\n",
        "        attn_output, _ = self.attn(x2.transpose(0,1), x2.transpose(0,1), x2.transpose(0,1))\n",
        "        attn_output = attn_output.transpose(0,1)\n",
        "        x = x + self.drop_path(attn_output)\n",
        "        # MLP\n",
        "        x2 = self.norm2(x)\n",
        "        x = x + self.drop_path(self.mlp(x2))\n",
        "        return x\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer Decoder consisting of multiple Decoder Blocks\n",
        "    \"\"\"\n",
        "    def __init__(self, depth, embed_dim, num_heads, mlp_ratio=4.0, drop=0.0, attn_drop=0.0):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderBlock(embed_dim, num_heads, mlp_ratio, drop, attn_drop)\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "class MAE3D(nn.Module):\n",
        "    \"\"\"\n",
        "    3D Masked AutoEncoder with Vision Transformer backbone\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels=3,          # [Mask1, Raw, Mask2]\n",
        "        patch_size=4,\n",
        "        embed_dim=768,\n",
        "        encoder_depth=12,\n",
        "        encoder_num_heads=12,\n",
        "        decoder_depth=8,\n",
        "        decoder_num_heads=12,\n",
        "        mlp_ratio=4.0,\n",
        "        mask_ratio=0.75\n",
        "    ):\n",
        "        super(MAE3D, self).__init__()\n",
        "        self.patch_embed = PatchEmbed3D(in_channels, patch_size, embed_dim)\n",
        "        num_patches = (80 // patch_size) ** 3\n",
        "        self.encoder_pos_embed = PositionalEncoding3D(embed_dim, num_patches)\n",
        "\n",
        "        self.encoder = TransformerEncoder(\n",
        "            depth=encoder_depth,\n",
        "            embed_dim=embed_dim,\n",
        "            num_heads=encoder_num_heads,\n",
        "            mlp_ratio=mlp_ratio\n",
        "        )\n",
        "\n",
        "        self.mask_ratio = mask_ratio\n",
        "        self.num_patches = num_patches\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        # Initialize Mask Token\n",
        "        self.mask_token = MaskToken(embed_dim)\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder_embed = nn.Linear(embed_dim, embed_dim, bias=True)\n",
        "        self.decoder_pos_embed = PositionalEncoding3D(embed_dim, num_patches)\n",
        "\n",
        "        self.decoder = TransformerDecoder(\n",
        "            depth=decoder_depth,\n",
        "            embed_dim=embed_dim,\n",
        "            num_heads=decoder_num_heads,\n",
        "            mlp_ratio=mlp_ratio\n",
        "        )\n",
        "\n",
        "        # Output projection\n",
        "        self.decoder_pred = nn.Linear(embed_dim, in_channels * patch_size ** 3, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (B, C, D, H, W)\n",
        "        Returns:\n",
        "            tuple: (pred_masked, mask)\n",
        "                - pred_masked (torch.Tensor): Reconstructed masked patches (B, num_mask, C, P, P, P)\n",
        "                - mask (torch.Tensor): Boolean mask indicating masked patches (B, N)\n",
        "        \"\"\"\n",
        "        B = x.shape[0]\n",
        "        # Patch Embedding\n",
        "        x = self.patch_embed(x)  # (B, N, E)\n",
        "\n",
        "        # Apply positional encoding\n",
        "        x = self.encoder_pos_embed(x)\n",
        "\n",
        "        # Masking: exactly mask 'num_mask' patches per sample\n",
        "        N = x.shape[1]\n",
        "        num_mask = int(self.mask_ratio * N)\n",
        "\n",
        "        # Generate deterministic mask with exactly 'num_mask' patches masked\n",
        "        rand = torch.rand(B, N, device=x.device)\n",
        "        _, topk_indices = rand.topk(num_mask, dim=1)\n",
        "        # Initialize mask tensor\n",
        "        mask = torch.zeros(B, N, dtype=torch.bool, device=x.device)\n",
        "        # Scatter True values at masked indices\n",
        "        mask.scatter_(1, topk_indices, True)  # (B, N)\n",
        "\n",
        "        # Select visible and masked patches\n",
        "        visible_idx = ~mask  # (B, N)\n",
        "        masked_idx = mask      # (B, N)\n",
        "\n",
        "        # Ensure that the number of visible and masked patches is consistent\n",
        "        x_visible = x[visible_idx].view(B, N - num_mask, -1)  # (B, N - num_mask, E)\n",
        "        x_masked = x[masked_idx].view(B, num_mask, -1)        # (B, num_mask, E)\n",
        "\n",
        "        # Encoder\n",
        "        encoded = self.encoder(x_visible)  # (B, N - num_mask, E)\n",
        "\n",
        "        # Prepare tokens for decoder\n",
        "        # Concatenate encoded visible tokens with mask tokens\n",
        "        mask_tokens = self.mask_token(B, num_mask, encoded.shape[-1]).to(x.device)  # (B, num_mask, E)\n",
        "        x_decoder = torch.cat([encoded, mask_tokens], dim=1)  # (B, N, E)\n",
        "\n",
        "        # Add positional encoding\n",
        "        x_decoder = self.decoder_pos_embed(x_decoder)\n",
        "\n",
        "        # Decoder\n",
        "        decoded = self.decoder(x_decoder)  # (B, N, E)\n",
        "\n",
        "        # Predictor\n",
        "        pred = self.decoder_pred(decoded)  # (B, N, C*P^3)\n",
        "\n",
        "        # Reconstruct the masked patches\n",
        "        # Only compute loss on masked patches\n",
        "        pred_masked = pred[masked_idx].view(B, num_mask, 3, self.patch_size, self.patch_size, self.patch_size)  # (B, num_mask, C, P, P, P)\n",
        "\n",
        "        return pred_masked, mask\n",
        "\n",
        "# -----------------------------\n",
        "# 5. Utility Functions\n",
        "# -----------------------------\n",
        "\n",
        "def patchify(x, patch_size):\n",
        "    \"\"\"\n",
        "    Divides the input tensor into patches.\n",
        "\n",
        "    Args:\n",
        "        x (torch.Tensor): Input tensor of shape (B, C, D, H, W)\n",
        "        patch_size (int): Size of each patch\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Patches of shape (B, N, C, P, P, P)\n",
        "    \"\"\"\n",
        "    B, C, D, H, W = x.shape\n",
        "    x = x.unfold(2, patch_size, patch_size)\n",
        "    x = x.unfold(3, patch_size, patch_size)\n",
        "    x = x.unfold(4, patch_size, patch_size)\n",
        "    x = x.contiguous().view(B, C, -1, patch_size, patch_size, patch_size)\n",
        "    x = x.permute(0, 2, 1, 3, 4, 5)  # (B, N, C, P, P, P)\n",
        "    return x\n",
        "\n",
        "def initialize_dataloaders(\n",
        "    raw_base_dir,\n",
        "    seg_base_dir,\n",
        "    excel_dir,\n",
        "    bbox_names,\n",
        "    batch_size=2,\n",
        "    subvolume_size=80,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    num_workers=2  # Reduced to 2 to avoid warnings\n",
        "):\n",
        "    \"\"\"\n",
        "    Initializes training and validation dataloaders.\n",
        "\n",
        "    Args:\n",
        "        raw_base_dir (str): Base directory containing raw data.\n",
        "        seg_base_dir (str): Base directory containing segmentation data.\n",
        "        excel_dir (str): Directory containing Excel files for each bounding box.\n",
        "        bbox_names (list): List of bounding box names.\n",
        "        batch_size (int): Batch size.\n",
        "        subvolume_size (int): Size of the subvolume.\n",
        "        test_size (float): Fraction of data to use for validation.\n",
        "        random_state (int): Random seed for reproducibility.\n",
        "        num_workers (int): Number of subprocesses for data loading.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (train_loader, val_loader)\n",
        "    \"\"\"\n",
        "    # Initialize the full dataset\n",
        "    full_dataset = ElectronMicroscopyDataset(\n",
        "        bbox_names=bbox_names,\n",
        "        raw_base_dir=raw_base_dir,\n",
        "        seg_base_dir=seg_base_dir,\n",
        "        excel_dir=excel_dir,\n",
        "        subvolume_size=subvolume_size\n",
        "    )\n",
        "\n",
        "    # Split into training and validation\n",
        "    train_indices, val_indices = train_test_split(\n",
        "        list(range(len(full_dataset))),\n",
        "        test_size=test_size,\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "    # Subset samplers\n",
        "    train_dataset = Subset(full_dataset, train_indices)\n",
        "    val_dataset = Subset(full_dataset, val_indices)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "# -----------------------------\n",
        "# 6. Training Loop\n",
        "# -----------------------------\n",
        "\n",
        "def train_mae3d(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    device,\n",
        "    num_epochs=100,\n",
        "    scheduler=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Trains the MAE3D model.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The MAE3D model.\n",
        "        train_loader (DataLoader): Training dataloader.\n",
        "        val_loader (DataLoader): Validation dataloader.\n",
        "        optimizer (torch.optim.Optimizer): Optimizer.\n",
        "        criterion (nn.Module): Loss function.\n",
        "        device (torch.device): Device to train on.\n",
        "        num_epochs (int): Number of training epochs.\n",
        "        scheduler (torch.optim.lr_scheduler, optional): Learning rate scheduler.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0.0\n",
        "        progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        for batch_idx, (inputs, _) in enumerate(progress):\n",
        "            inputs = inputs.to(device)   # (B, 3, 80, 80, 80)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            preds, mask = model(inputs)  # preds: (B, num_mask, 3, P, P, P), mask: (B, N)\n",
        "\n",
        "            B, N = mask.shape\n",
        "            P = model.patch_size\n",
        "            C = 3  # Number of channels\n",
        "\n",
        "            # Patchify the inputs to get target patches\n",
        "            target_patches = patchify(inputs, P)  # (B, N, C, P, P, P)\n",
        "\n",
        "            # Select only the masked patches\n",
        "            mask_flat = mask.view(B, N)\n",
        "            target_masked = target_patches[mask_flat]  # (B*num_mask, C, P, P, P)\n",
        "\n",
        "            # Reshape preds to match target_masked\n",
        "            preds = preds.view(B * preds.shape[1], C, P, P, P)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(preds, target_masked)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            progress.set_postfix({'Loss': loss.item()})\n",
        "\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "\n",
        "        # Optional: Implement validation loop here\n",
        "\n",
        "    print(\"Training completed.\")\n",
        "\n",
        "# -----------------------------\n",
        "# 7. Main Function\n",
        "# -----------------------------\n",
        "\n",
        "def main():\n",
        "\n",
        "    # Configuration\n",
        "    raw_base_dir = \"raw\"\n",
        "    seg_base_dir = \"seg\"\n",
        "    excel_dir = \"\"\n",
        "    bbox_names = [\"bbox1\", \"bbox2\", \"bbox3\"]\n",
        "    batch_size = 1\n",
        "    num_epochs = 100\n",
        "    learning_rate = 1e-4\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    subvolume_size = 80\n",
        "    mask_ratio = 0.75\n",
        "    num_workers = 2\n",
        "    print(device)\n",
        "\n",
        "\n",
        "    # Initialize DataLoaders\n",
        "    train_loader, val_loader = initialize_dataloaders(\n",
        "        raw_base_dir=raw_base_dir,\n",
        "        seg_base_dir=seg_base_dir,\n",
        "        excel_dir=excel_dir,\n",
        "        bbox_names=bbox_names,\n",
        "        batch_size=batch_size,\n",
        "        subvolume_size=subvolume_size,\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    # Initialize Model\n",
        "    model = MAE3D(\n",
        "        in_channels=3,         # 3 channels: Mask1, Raw, Mask2\n",
        "        patch_size=4,\n",
        "        embed_dim=768,\n",
        "        encoder_depth=4,\n",
        "        encoder_num_heads=4,\n",
        "        decoder_depth=4,\n",
        "        decoder_num_heads=4,\n",
        "        mlp_ratio=4.0,\n",
        "        mask_ratio=mask_ratio\n",
        "    ).to(device)\n",
        "\n",
        "    # Define Loss and Optimizer\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.05)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "    # Start Training\n",
        "    train_mae3d(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        optimizer=optimizer,\n",
        "        criterion=criterion,\n",
        "        device=device,\n",
        "        num_epochs=num_epochs,\n",
        "        scheduler=scheduler\n",
        "    )\n",
        "\n",
        "    # Save the trained model\n",
        "    torch.save(model.state_dict(), \"mae3d_model.pth\")\n",
        "    print(\"Model saved as mae3d_model.pth\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}