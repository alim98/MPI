{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alim98/MPI/blob/main/Chromatin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQXSBhrjiYS0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daa58lj6b7qc"
      },
      "source": [
        "# Essential downloads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIIIxarwTVT0"
      },
      "outputs": [],
      "source": [
        "!wget -q -O downloaded_file.zip \"https://drive.usercontent.google.com/download?id=1KriedE029fLEmQP3Jy_b6H-j0eRa_Icj&export=download&authuser=0&confirm=t&uuid=631d60dd-569c-4bb1-a9e8-d681f0ed3d43&at=APvzH3r4me8x_LwP3n8O7lgPo8oK%3A1733988188000\"\n",
        "\n",
        "!unzip -q downloaded_file.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZwyJMBQTXV3",
        "outputId": "4df32488-98b8-4045-e866-0b8dfa59f9f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m109.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for funlib.learn.torch (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip -q install transformers scikit-learn matplotlib seaborn torch torchvision umap-learn git+https://github.com/funkelab/funlib.learn.torch.git\n",
        "!pip -q install openpyxl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRS_MN6YTekw"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import glob\n",
        "import imageio.v2 as iio\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Patch, Rectangle\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import ViTImageProcessor, ViTModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns\n",
        "from umap import UMAP\n",
        "import torch.nn.functional as F\n",
        "from funlib.learn.torch.models import Vgg3D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6pwE9leiPFQ"
      },
      "source": [
        "# VGG no mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VBdY_XUk2SP"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/fine_tuned_vgg3d.pth /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZI2Atx1PL_9-",
        "outputId": "f6bdfaff-b354-4a90-b279-94ddd52fd5ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VGG3D checkpoint already exists.\n",
            "Loading model from fine_tuned_vgg3d.pth...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6715e4964020>:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location=device)\n",
            "<ipython-input-2-6715e4964020>:135: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  transforms.Lambda(lambda x: torch.tensor(x, dtype=torch.float32).unsqueeze(0))  # Adds a channel dimension\n",
            "<ipython-input-2-6715e4964020>:276: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  targets = torch.tensor(targets, dtype=torch.long).to(device)  # Ensure the targets are long integers for cross-entropy\n",
            "<ipython-input-2-6715e4964020>:313: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  targets = torch.tensor(targets, dtype=torch.long).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/30] Train Loss: 2.3133, Train Accuracy: 33.13% | Test Loss: 2.5097, Test Accuracy: 20.33%\n",
            "Epoch [2/30] Train Loss: 1.8245, Train Accuracy: 45.60% | Test Loss: 1.8523, Test Accuracy: 33.33%\n",
            "Epoch [3/30] Train Loss: 1.6187, Train Accuracy: 47.03% | Test Loss: 1.7777, Test Accuracy: 39.84%\n",
            "Epoch [4/30] Train Loss: 1.4858, Train Accuracy: 47.44% | Test Loss: 1.8598, Test Accuracy: 30.89%\n",
            "Epoch [5/30] Train Loss: 1.4666, Train Accuracy: 51.12% | Test Loss: 1.5743, Test Accuracy: 47.97%\n",
            "Epoch [6/30] Train Loss: 1.4252, Train Accuracy: 49.08% | Test Loss: 1.6842, Test Accuracy: 45.53%\n",
            "Epoch [7/30] Train Loss: 1.3606, Train Accuracy: 52.97% | Test Loss: 2.1418, Test Accuracy: 29.27%\n",
            "Epoch [8/30] Train Loss: 1.3439, Train Accuracy: 57.06% | Test Loss: 1.6154, Test Accuracy: 49.59%\n",
            "Epoch [9/30] Train Loss: 1.3992, Train Accuracy: 53.37% | Test Loss: 1.7601, Test Accuracy: 43.09%\n",
            "Epoch [10/30] Train Loss: 1.2939, Train Accuracy: 53.99% | Test Loss: 1.7471, Test Accuracy: 41.46%\n",
            "Epoch [11/30] Train Loss: 1.2369, Train Accuracy: 56.65% | Test Loss: 1.5613, Test Accuracy: 48.78%\n",
            "Epoch [12/30] Train Loss: 1.2154, Train Accuracy: 59.10% | Test Loss: 1.6035, Test Accuracy: 53.66%\n",
            "Epoch [13/30] Train Loss: 1.1898, Train Accuracy: 57.26% | Test Loss: 1.3660, Test Accuracy: 56.91%\n",
            "Epoch [14/30] Train Loss: 1.1371, Train Accuracy: 59.30% | Test Loss: 1.4161, Test Accuracy: 56.91%\n",
            "Epoch [15/30] Train Loss: 1.1199, Train Accuracy: 63.39% | Test Loss: 1.5794, Test Accuracy: 46.34%\n",
            "Epoch [16/30] Train Loss: 1.1067, Train Accuracy: 60.12% | Test Loss: 1.4396, Test Accuracy: 50.41%\n",
            "Epoch [17/30] Train Loss: 1.1062, Train Accuracy: 59.30% | Test Loss: 1.4506, Test Accuracy: 50.41%\n",
            "Epoch [18/30] Train Loss: 1.0707, Train Accuracy: 61.55% | Test Loss: 1.9394, Test Accuracy: 38.21%\n",
            "Epoch [19/30] Train Loss: 1.0229, Train Accuracy: 62.78% | Test Loss: 1.7335, Test Accuracy: 43.09%\n",
            "Epoch [20/30] Train Loss: 0.9629, Train Accuracy: 67.48% | Test Loss: 1.3725, Test Accuracy: 49.59%\n",
            "Epoch [21/30] Train Loss: 0.9536, Train Accuracy: 65.44% | Test Loss: 1.3381, Test Accuracy: 56.91%\n",
            "Epoch [22/30] Train Loss: 0.9747, Train Accuracy: 63.39% | Test Loss: 1.4435, Test Accuracy: 53.66%\n",
            "Epoch [23/30] Train Loss: 0.9547, Train Accuracy: 66.46% | Test Loss: 1.4267, Test Accuracy: 56.91%\n",
            "Epoch [24/30] Train Loss: 0.9270, Train Accuracy: 67.28% | Test Loss: 1.5846, Test Accuracy: 49.59%\n",
            "Epoch [25/30] Train Loss: 0.8328, Train Accuracy: 67.89% | Test Loss: 1.3888, Test Accuracy: 57.72%\n",
            "Epoch [26/30] Train Loss: 0.8226, Train Accuracy: 68.92% | Test Loss: 1.5648, Test Accuracy: 50.41%\n",
            "Epoch [27/30] Train Loss: 0.8212, Train Accuracy: 69.33% | Test Loss: 1.3026, Test Accuracy: 53.66%\n",
            "Epoch [28/30] Train Loss: 0.7832, Train Accuracy: 71.17% | Test Loss: 1.5468, Test Accuracy: 45.53%\n",
            "Epoch [29/30] Train Loss: 0.8133, Train Accuracy: 68.10% | Test Loss: 1.6410, Test Accuracy: 49.59%\n",
            "Epoch [30/30] Train Loss: 0.7177, Train Accuracy: 74.64% | Test Loss: 1.6759, Test Accuracy: 49.59%\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"7f7b5eae-e123-49ab-b94e-bfe8d2178301\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"7f7b5eae-e123-49ab-b94e-bfe8d2178301\")) {                    Plotly.newPlot(                        \"7f7b5eae-e123-49ab-b94e-bfe8d2178301\",                        [{\"mode\":\"lines\",\"name\":\"Train Loss\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29],\"y\":[2.313278429210186,1.8245369270443916,1.6186586022377014,1.485830582678318,1.4665740728378296,1.4252225197851658,1.3606177419424057,1.3438727036118507,1.3992275036871433,1.293932680040598,1.2369089685380459,1.215380646288395,1.189828097820282,1.1371098831295967,1.1199350133538246,1.1067401617765427,1.106227569282055,1.0707379058003426,1.0229397527873516,0.9628706984221935,0.9536081328988075,0.9746713899075985,0.9547126069664955,0.9270053952932358,0.832766454666853,0.822586040943861,0.8212172333151102,0.783176451921463,0.8132773637771606,0.7176584154367447],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Test Loss\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29],\"y\":[2.5097203850746155,1.8522851765155792,1.777675062417984,1.859763741493225,1.5742839574813843,1.6841546893119812,2.14184233546257,1.615374207496643,1.7601351141929626,1.7471421360969543,1.561286300420761,1.6035016179084778,1.3660238981246948,1.4160862565040588,1.579388052225113,1.4396089613437653,1.4505833089351654,1.939381867647171,1.7334840297698975,1.372497409582138,1.3381094932556152,1.4434653520584106,1.4267475605010986,1.5846063494682312,1.3887540698051453,1.5648205876350403,1.3026321828365326,1.5468192100524902,1.6409831047058105,1.6759259402751923],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Loss over Epochs\"},\"xaxis\":{\"title\":{\"text\":\"Epoch\"}},\"yaxis\":{\"title\":{\"text\":\"Loss\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('7f7b5eae-e123-49ab-b94e-bfe8d2178301');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"3bb38a56-0bf3-4ef6-917d-7acc0f55a79c\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"3bb38a56-0bf3-4ef6-917d-7acc0f55a79c\")) {                    Plotly.newPlot(                        \"3bb38a56-0bf3-4ef6-917d-7acc0f55a79c\",                        [{\"mode\":\"lines\",\"name\":\"Train Accuracy\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29],\"y\":[33.12883435582822,45.603271983640084,47.034764826175866,47.443762781186095,51.124744376278116,49.079754601226995,52.965235173824134,57.05521472392638,53.374233128834355,53.987730061349694,56.646216768916155,59.100204498977504,57.259713701431494,59.304703476482615,63.394683026584865,60.122699386503065,59.304703476482615,61.554192229038854,62.78118609406953,67.48466257668711,65.439672801636,63.394683026584865,66.46216768916156,67.280163599182,67.89366053169734,68.9161554192229,69.32515337423312,71.16564417177914,68.09815950920246,74.64212678936606],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Test Accuracy\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29],\"y\":[20.32520325203252,33.333333333333336,39.83739837398374,30.89430894308943,47.96747967479675,45.52845528455285,29.26829268292683,49.59349593495935,43.08943089430894,41.46341463414634,48.78048780487805,53.65853658536585,56.91056910569106,56.91056910569106,46.34146341463415,50.40650406504065,50.40650406504065,38.21138211382114,43.08943089430894,49.59349593495935,56.91056910569106,53.65853658536585,56.91056910569106,49.59349593495935,57.72357723577236,50.40650406504065,53.65853658536585,45.52845528455285,49.59349593495935,49.59349593495935],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Accuracy over Epochs\"},\"xaxis\":{\"title\":{\"text\":\"Epoch\"}},\"yaxis\":{\"title\":{\"text\":\"Accuracy (%)\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('3bb38a56-0bf3-4ef6-917d-7acc0f55a79c');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6715e4964020>:135: UserWarning:\n",
            "\n",
            "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "\n",
            "<ipython-input-2-6715e4964020>:354: UserWarning:\n",
            "\n",
            "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.5203\n",
            "Classification Report:\n",
            "                           precision    recall  f1-score   support\n",
            "\n",
            "                     Glia       0.00      0.00      0.00         0\n",
            "                 L-shaped       0.60      0.78      0.68        23\n",
            "        Line detector OGL       0.55      0.55      0.55        11\n",
            "       Medium first order       0.27      0.67      0.38         6\n",
            "Medulla projecting MP IGL       0.00      0.00      0.00         1\n",
            "   Medulla projecting OGL       0.00      0.00      0.00         1\n",
            "           Tangential IGL       1.00      0.25      0.40         4\n",
            "            Three-pronged       0.00      0.00      0.00         5\n",
            "                 U-shaped       0.00      0.00      0.00         4\n",
            " Unipolar plex-porjecting       0.43      0.43      0.43         7\n",
            "              centrifugal       0.29      0.67      0.40         3\n",
            "        large first order       0.67      0.50      0.57         4\n",
            "           large palisade       0.00      0.00      0.00         0\n",
            "           multipolar IGL       0.60      0.60      0.60         5\n",
            "                 palisade       0.75      0.50      0.60         6\n",
            "       palisade with feet       0.00      0.00      0.00         2\n",
            "           small U-shaped       0.00      0.00      0.00         8\n",
            "        small first order       0.67      0.67      0.67        33\n",
            "\n",
            "                 accuracy                           0.52       123\n",
            "                macro avg       0.32      0.31      0.29       123\n",
            "             weighted avg       0.50      0.52      0.49       123\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Custom dataset class\n",
        "class ChromatinDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, target_size=(75, 75, 75)):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.target_size = target_size\n",
        "        self.cell_types = sorted(os.listdir(root_dir))  # Get all cell types\n",
        "\n",
        "        # Exclude '.DS_Store'\n",
        "        self.cell_types = [cell_type for cell_type in self.cell_types if cell_type != '.DS_Store']\n",
        "\n",
        "        self.cell_type_to_idx = {cell_type: idx for idx, cell_type in enumerate(self.cell_types)}  # Map cell types to integer labels\n",
        "        self.samples = self._find_samples()\n",
        "\n",
        "    def _find_samples(self):\n",
        "        samples = []\n",
        "        for cell_type in self.cell_types:\n",
        "            cell_type_dir = os.path.join(self.root_dir, cell_type)\n",
        "            if os.path.isdir(cell_type_dir):\n",
        "                sample_dirs = sorted(os.listdir(cell_type_dir))\n",
        "                for sample in sample_dirs:\n",
        "                    raw_dir = os.path.join(cell_type_dir, sample, 'raw')\n",
        "                    mask_dir = os.path.join(cell_type_dir, sample, 'mask')\n",
        "\n",
        "                    # Get all .tif files in raw and mask directories\n",
        "                    raw_files = sorted(glob.glob(os.path.join(raw_dir, '*.tif')))\n",
        "                    mask_files = sorted(glob.glob(os.path.join(mask_dir, '*.tif')))\n",
        "\n",
        "                    if raw_files and mask_files:\n",
        "                        samples.append((cell_type, sample, raw_files, mask_files))\n",
        "\n",
        "        return samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def _resize_image(self, img, target_size):\n",
        "        \"\"\"Resizes the image to the target size.\"\"\"\n",
        "        return img.resize(target_size, Image.Resampling.LANCZOS)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        cell_type, sample, raw_files, mask_files = self.samples[idx]\n",
        "\n",
        "        # Load raw data and mask data as 3D volumes (stacking each slice)\n",
        "        raw_data = np.stack([np.array(self._resize_image(Image.open(f), self.target_size[:2])) for f in raw_files], axis=-1)\n",
        "        mask_data = np.stack([np.array(self._resize_image(Image.open(f), self.target_size[:2])) for f in mask_files], axis=-1)\n",
        "\n",
        "        # Ensure the depth is the same for raw and mask\n",
        "        if raw_data.shape[-1] != mask_data.shape[-1]:\n",
        "            raise ValueError(f\"Number of slices in raw and mask do not match for sample {sample}\")\n",
        "\n",
        "        # If depth is larger than target_size[2], crop; otherwise, pad\n",
        "        if raw_data.shape[-1] < self.target_size[2]:\n",
        "            raw_data = np.pad(raw_data, ((0, 0), (0, 0), (0, self.target_size[2] - raw_data.shape[2])), mode='constant')\n",
        "            mask_data = np.pad(mask_data, ((0, 0), (0, 0), (0, self.target_size[2] - mask_data.shape[2])), mode='constant')\n",
        "        else:\n",
        "            raw_data = raw_data[:, :, :self.target_size[2]]  # Crop to target depth\n",
        "            mask_data = mask_data[:, :, :self.target_size[2]]  # Crop to target depth\n",
        "\n",
        "        # Convert to torch tensors\n",
        "        raw_data = torch.tensor(raw_data, dtype=torch.float32)\n",
        "        mask_data = torch.tensor(mask_data, dtype=torch.float32)\n",
        "\n",
        "        # Get the integer index for the cell_type\n",
        "        target = self.cell_type_to_idx[cell_type]\n",
        "\n",
        "        sample_data = {'cell_type': cell_type, 'sample': sample, 'raw': raw_data, 'mask': mask_data, 'target': target}\n",
        "\n",
        "        if self.transform:\n",
        "            sample_data['raw'] = self.transform(sample_data['raw'])\n",
        "            sample_data['mask'] = self.transform(sample_data['mask'])\n",
        "\n",
        "        return sample_data\n",
        "import os\n",
        "import torch\n",
        "\n",
        "def load_model_from_checkpoint(model, fine_tuned_path):\n",
        "    # Check if fine_tuned_vgg3d.pth exists\n",
        "    if os.path.exists(fine_tuned_path):\n",
        "        print(\"Loading model from fine_tuned_vgg3d.pth...\")\n",
        "        checkpoint_path = fine_tuned_path\n",
        "    else:\n",
        "        # Paths and directories\n",
        "        checkpoint_url = \"https://dl.dropboxusercontent.com/scl/fo/mfejaomhu43aa6oqs6zsf/AKMAAgT7OrUtruR0AQXZBy0/hemibrain_production.checkpoint.20220225?rlkey=6cmwxdvehy4ylztvsbgkfnrfc&dl=0\"\n",
        "        checkpoint_path = 'hemibrain_production.checkpoint'\n",
        "\n",
        "        # Download the fallback checkpoint if not found\n",
        "        os.system(f\"wget -O {checkpoint_path} '{checkpoint_url}'\")\n",
        "        print(\"fine_tuned_vgg3d.pth not found, loading from hemibrain_production.checkpoint...\")\n",
        "\n",
        "    # Load the checkpoint\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "    # Update the model with the state dict from the checkpoint\n",
        "    model_dict = model.state_dict()\n",
        "\n",
        "    # The checkpoint contains the full model's state_dict, so we update the model's state_dict\n",
        "    # If the checkpoint doesn't have a classifier key, load the model directly.\n",
        "    if 'model_state_dict' in checkpoint:\n",
        "        feature_extraction_weights = {k: v for k, v in checkpoint['model_state_dict'].items() if 'classifier' not in k}\n",
        "    else:\n",
        "        # If no 'model_state_dict', assume the checkpoint contains all weights\n",
        "        feature_extraction_weights = {k: v for k, v in checkpoint.items() if 'classifier' not in k}\n",
        "\n",
        "    # Update the model's feature extraction layers with the pre-trained weights\n",
        "    model_dict.update(feature_extraction_weights)\n",
        "    model.load_state_dict(model_dict)\n",
        "\n",
        "    # Return the model with the loaded weights\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "# Data Augmentation for balancing classes\n",
        "augmentation_transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(30),\n",
        "    # transforms.RandomResizedCrop(size=(75, 75, 75), scale=(0.8, 1.2)),\n",
        "    transforms.Lambda(lambda x: torch.tensor(x, dtype=torch.float32).unsqueeze(0))  # Adds a channel dimension\n",
        "])\n",
        "\n",
        "# Vgg3D model class remains the same\n",
        "class Vgg3D(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=(80, 80, 80),\n",
        "        fmaps=24,\n",
        "        downsample_factors=[(2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2)],\n",
        "        fmap_inc=(2, 2, 2, 2),\n",
        "        n_convolutions=(4, 2, 2, 2),\n",
        "        output_classes=7,\n",
        "        input_fmaps=1,\n",
        "    ):\n",
        "        super(Vgg3D, self).__init__()\n",
        "\n",
        "        # Validate input parameters\n",
        "        if len(downsample_factors) != len(fmap_inc):\n",
        "            raise ValueError(\"fmap_inc needs to have same length as downsample factors\")\n",
        "        if len(n_convolutions) != len(fmap_inc):\n",
        "            raise ValueError(\"n_convolutions needs to have the same length as downsample factors\")\n",
        "        if np.any(np.array(n_convolutions) < 1):\n",
        "            raise ValueError(\"Each layer must have at least one convolution\")\n",
        "\n",
        "        current_fmaps = input_fmaps\n",
        "        current_size = np.array(input_size)\n",
        "\n",
        "        # Feature extraction layers\n",
        "        layers = []\n",
        "        for i, (df, nc) in enumerate(zip(downsample_factors, n_convolutions)):\n",
        "            layers += [\n",
        "                nn.Conv3d(current_fmaps, fmaps, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm3d(fmaps),\n",
        "                nn.ReLU(inplace=True)\n",
        "            ]\n",
        "\n",
        "            for _ in range(nc - 1):\n",
        "                layers += [\n",
        "                    nn.Conv3d(fmaps, fmaps, kernel_size=3, padding=1),\n",
        "                    nn.BatchNorm3d(fmaps),\n",
        "                    nn.ReLU(inplace=True)\n",
        "                ]\n",
        "\n",
        "            layers.append(nn.MaxPool3d(df))\n",
        "\n",
        "            current_fmaps = fmaps\n",
        "            fmaps *= fmap_inc[i]\n",
        "\n",
        "            current_size = np.floor(current_size / np.array(df))\n",
        "\n",
        "        self.features = nn.Sequential(*layers)\n",
        "\n",
        "        # Classifier layer (will be reinitialized later)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(int(np.prod(current_size)) * current_fmaps, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, output_classes),  # This will be adapted to your dataset\n",
        "        )\n",
        "\n",
        "    def forward(self, x, return_features=False):\n",
        "        x = self.features(x)\n",
        "        if return_features:\n",
        "            return x\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Lambda(lambda x: torch.tensor(x, dtype=torch.float32).unsqueeze(0)),  # Adds a channel dimension\n",
        "])\n",
        "\n",
        "dataset = ChromatinDataset(root_dir='/content/chromatin_task', transform=augmentation_transforms)\n",
        "\n",
        "# Train-test split (80% train, 20% test)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "output_classes = len(dataset.cell_types)\n",
        "\n",
        "# dataset = ChromatinDataset(root_dir='/content/chromatin_task', transform=transform)\n",
        "# dataloader = DataLoader(dataset, batch_size=512, shuffle=True)\n",
        "\n",
        "output_classes = len(dataset.cell_types)\n",
        "\n",
        "# Initialize the model\n",
        "output_classes = len(dataset.cell_types)\n",
        "model = Vgg3D(input_size=(75, 75, 75), fmaps=24, output_classes=output_classes, input_fmaps=1)\n",
        "\n",
        "# Paths and directories\n",
        "checkpoint_url = \"https://dl.dropboxusercontent.com/scl/fo/mfejaomhu43aa6oqs6zsf/AKMAAgT7OrUtruR0AQXZBy0/hemibrain_production.checkpoint.20220225?rlkey=6cmwxdvehy4ylztvsbgkfnrfc&dl=0\"\n",
        "checkpoint_path = 'hemibrain_production.checkpoint'\n",
        "\n",
        "# Download the checkpoint if it doesn't exist\n",
        "if not os.path.exists(checkpoint_path):\n",
        "    os.system(f\"wget -O {checkpoint_path} '{checkpoint_url}'\")\n",
        "    print(\"Downloaded VGG3D checkpoint.\")\n",
        "else:\n",
        "    print(\"VGG3D checkpoint already exists.\")\n",
        "\n",
        "# Define checkpoint path\n",
        "fine_tuned_path = 'fine_tuned_vgg3d.pth'\n",
        "model = load_model_from_checkpoint(model, fine_tuned_path)\n",
        "\n",
        "# # Check if the checkpoint exists\n",
        "# if os.path.exists(checkpoint_path):\n",
        "#     print(\"Checkpoint found, resuming training...\")\n",
        "#     model = load_model_from_checkpoint(model, fine_tuned_path)\n",
        "# else:\n",
        "#     print(\"No checkpoint found, starting from scratch.\")\n",
        "\n",
        "model.to(device)  # Move model to GPU if available\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Training loop with CUDA\n",
        "num_epochs = 30\n",
        "train_losses, test_losses, train_accuracies, test_accuracies = [], [], [], []\n",
        "from tqdm import tqdm\n",
        "for epoch in (range(num_epochs)):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for batch_idx, batch in (enumerate(train_dataloader)):\n",
        "        inputs, targets = batch['raw'].to(device), batch['target'].to(device)  # Move inputs and targets to GPU\n",
        "\n",
        "        # Ensure inputs are of shape [batch_size, 1, depth, height, width]\n",
        "        inputs = inputs.squeeze(2)  # Remove extra dimension at index 2 (if present)\n",
        "\n",
        "        targets = torch.tensor(targets, dtype=torch.long).to(device)  # Ensure the targets are long integers for cross-entropy\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total_train += targets.size(0)\n",
        "        correct_train += (predicted == targets).sum().item()\n",
        "\n",
        "    train_losses.append(running_loss / len(train_dataloader))\n",
        "    train_accuracies.append(100 * correct_train / total_train)\n",
        "\n",
        "    # Test phase\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct_test = 0\n",
        "    total_test = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(test_dataloader):\n",
        "            inputs, targets = batch['raw'].to(device), batch['target'].to(device)\n",
        "\n",
        "            inputs = inputs.squeeze(2)\n",
        "\n",
        "            targets = torch.tensor(targets, dtype=torch.long).to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs, targets)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_test += targets.size(0)\n",
        "            correct_test += (predicted == targets).sum().item()\n",
        "\n",
        "    test_losses.append(running_loss / len(test_dataloader))\n",
        "    test_accuracies.append(100 * correct_test / total_test)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_losses[-1]:.4f}, Train Accuracy: {train_accuracies[-1]:.2f}% | \"\n",
        "          f\"Test Loss: {test_losses[-1]:.4f}, Test Accuracy: {test_accuracies[-1]:.2f}%\")\n",
        "\n",
        "# Plotting loss and accuracy using Plotly\n",
        "fig_loss = go.Figure()\n",
        "fig_loss.add_trace(go.Scatter(x=list(range(num_epochs)), y=train_losses, mode='lines', name='Train Loss'))\n",
        "fig_loss.add_trace(go.Scatter(x=list(range(num_epochs)), y=test_losses, mode='lines', name='Test Loss'))\n",
        "fig_loss.update_layout(title=\"Loss over Epochs\", xaxis_title=\"Epoch\", yaxis_title=\"Loss\")\n",
        "\n",
        "fig_acc = go.Figure()\n",
        "fig_acc.add_trace(go.Scatter(x=list(range(num_epochs)), y=train_accuracies, mode='lines', name='Train Accuracy'))\n",
        "fig_acc.add_trace(go.Scatter(x=list(range(num_epochs)), y=test_accuracies, mode='lines', name='Test Accuracy'))\n",
        "fig_acc.update_layout(title=\"Accuracy over Epochs\", xaxis_title=\"Epoch\", yaxis_title=\"Accuracy (%)\")\n",
        "\n",
        "fig_loss.show()\n",
        "fig_acc.show()\n",
        "# Evaluation on the test set\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        inputs, targets = batch['raw'].to(device), batch['target'].to(device)\n",
        "        inputs = inputs.squeeze(2)\n",
        "        targets = torch.tensor(targets, dtype=torch.long).to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        y_true.extend(targets.cpu().numpy())\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Calculate metrics\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "# Get all possible labels from the original dataset\n",
        "labels = range(len(dataset.cell_types))\n",
        "report = classification_report(y_true, y_pred, target_names=dataset.cell_types, labels=labels, zero_division=0)\n",
        "\n",
        "print(f\"Test Accuracy: {acc:.4f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n",
        "\n",
        "# Save the fine-tuned model\n",
        "torch.save(model.state_dict(), \"fine_tuned_vgg3d.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pBwtOItgOs6"
      },
      "outputs": [],
      "source": [
        "!cp /content/fine_tuned_vgg3d.pth /content/drive/MyDrive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIzW9Deu5pWb"
      },
      "source": [
        "# with mask loss functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "3PI-CFI05oq-",
        "outputId": "3e0cabb7-8693-4a06-f20a-2c246265bafd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VGG3D checkpoint already exists.\n",
            "fine_tuned_vgg3d.pth not found, loading from hemibrain_production.checkpoint...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-1-593f3c9b38fa>:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location=device)\n",
            "<ipython-input-1-593f3c9b38fa>:135: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  transforms.Lambda(lambda x: torch.tensor(x, dtype=torch.float32).unsqueeze(0))  # Adds a channel dimension\n",
            "<ipython-input-1-593f3c9b38fa>:274: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  targets = torch.tensor(targets, dtype=torch.long).to(device)  # Ensure the targets are long integers for cross-entropy\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (32) must match the size of tensor b (75) at non-singleton dimension 1",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-593f3c9b38fa>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;31m# Apply the mask to the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0mmasked_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;31m# Backward pass and optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (75) at non-singleton dimension 1"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "au9Uwu3HilcQ",
        "outputId": "8cb62672-0784-47ba-94e2-65a6a3bfb92c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VGG3D checkpoint already exists.\n",
            "fine_tuned_vgg3d.pth not found, loading from hemibrain_production.checkpoint...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-1-f52aa054fc68>:155: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location=device)\n",
            "<ipython-input-1-f52aa054fc68>:333: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  targets = torch.tensor(targets, dtype=torch.long).to(device)  # Ensure the targets are long integers for cross-entropy\n",
            "<ipython-input-1-f52aa054fc68>:371: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  targets = torch.tensor(targets, dtype=torch.long).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/30] Train Loss: 2.9880, Train Accuracy: 22.90% | Test Loss: 2.7204, Test Accuracy: 18.70%\n",
            "Epoch [2/30] Train Loss: 2.5460, Train Accuracy: 22.90% | Test Loss: 2.4870, Test Accuracy: 26.02%\n",
            "Epoch [3/30] Train Loss: 2.3802, Train Accuracy: 24.95% | Test Loss: 2.3995, Test Accuracy: 26.02%\n",
            "Epoch [4/30] Train Loss: 2.3283, Train Accuracy: 27.20% | Test Loss: 2.3361, Test Accuracy: 26.83%\n",
            "Epoch [5/30] Train Loss: 2.2931, Train Accuracy: 30.67% | Test Loss: 2.3419, Test Accuracy: 26.02%\n",
            "Epoch [6/30] Train Loss: 2.1863, Train Accuracy: 31.49% | Test Loss: 2.3431, Test Accuracy: 28.46%\n",
            "Epoch [7/30] Train Loss: 2.0889, Train Accuracy: 37.01% | Test Loss: 2.0906, Test Accuracy: 37.40%\n",
            "Epoch [8/30] Train Loss: 2.0315, Train Accuracy: 38.45% | Test Loss: 1.9949, Test Accuracy: 39.84%\n",
            "Epoch [9/30] Train Loss: 1.8294, Train Accuracy: 44.79% | Test Loss: 2.1565, Test Accuracy: 35.77%\n",
            "Epoch [10/30] Train Loss: 1.7552, Train Accuracy: 45.19% | Test Loss: 1.7404, Test Accuracy: 45.53%\n",
            "Epoch [11/30] Train Loss: 1.7144, Train Accuracy: 46.01% | Test Loss: 1.6893, Test Accuracy: 42.28%\n",
            "Epoch [12/30] Train Loss: 1.6388, Train Accuracy: 47.85% | Test Loss: 1.7789, Test Accuracy: 39.02%\n",
            "Epoch [13/30] Train Loss: 1.6351, Train Accuracy: 44.79% | Test Loss: 1.7431, Test Accuracy: 43.09%\n",
            "Epoch [14/30] Train Loss: 1.5708, Train Accuracy: 50.51% | Test Loss: 1.6598, Test Accuracy: 49.59%\n",
            "Epoch [15/30] Train Loss: 1.5834, Train Accuracy: 48.06% | Test Loss: 1.6937, Test Accuracy: 47.97%\n",
            "Epoch [16/30] Train Loss: 1.5805, Train Accuracy: 49.69% | Test Loss: 1.6188, Test Accuracy: 50.41%\n",
            "Epoch [17/30] Train Loss: 1.5536, Train Accuracy: 48.26% | Test Loss: 1.6034, Test Accuracy: 49.59%\n",
            "Epoch [18/30] Train Loss: 1.4726, Train Accuracy: 49.28% | Test Loss: 1.5994, Test Accuracy: 49.59%\n",
            "Epoch [19/30] Train Loss: 1.4900, Train Accuracy: 50.31% | Test Loss: 1.4932, Test Accuracy: 46.34%\n",
            "Epoch [20/30] Train Loss: 1.4749, Train Accuracy: 49.28% | Test Loss: 1.5323, Test Accuracy: 52.03%\n",
            "Epoch [21/30] Train Loss: 1.4357, Train Accuracy: 48.47% | Test Loss: 1.4331, Test Accuracy: 52.85%\n",
            "Epoch [22/30] Train Loss: 1.3303, Train Accuracy: 51.74% | Test Loss: 1.6543, Test Accuracy: 46.34%\n",
            "Epoch [23/30] Train Loss: 1.4054, Train Accuracy: 53.37% | Test Loss: 1.4547, Test Accuracy: 47.15%\n",
            "Epoch [24/30] Train Loss: 1.3715, Train Accuracy: 52.56% | Test Loss: 1.4708, Test Accuracy: 52.85%\n",
            "Epoch [25/30] Train Loss: 1.3246, Train Accuracy: 53.58% | Test Loss: 1.3693, Test Accuracy: 48.78%\n",
            "Epoch [26/30] Train Loss: 1.3253, Train Accuracy: 57.06% | Test Loss: 1.4285, Test Accuracy: 48.78%\n",
            "Epoch [27/30] Train Loss: 1.2679, Train Accuracy: 52.97% | Test Loss: 1.3705, Test Accuracy: 47.97%\n",
            "Epoch [28/30] Train Loss: 1.3302, Train Accuracy: 56.44% | Test Loss: 1.4677, Test Accuracy: 46.34%\n",
            "Epoch [29/30] Train Loss: 1.2517, Train Accuracy: 57.67% | Test Loss: 1.4255, Test Accuracy: 51.22%\n",
            "Epoch [30/30] Train Loss: 1.2656, Train Accuracy: 54.60% | Test Loss: 1.4781, Test Accuracy: 52.85%\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"0e96744e-5094-45ae-b76c-498ca7979dce\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"0e96744e-5094-45ae-b76c-498ca7979dce\")) {                    Plotly.newPlot(                        \"0e96744e-5094-45ae-b76c-498ca7979dce\",                        [{\"mode\":\"lines\",\"name\":\"Train Loss\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29],\"y\":[2.9880422949790955,2.5460327863693237,2.38024665415287,2.3283097445964813,2.2930987030267715,2.186335377395153,2.088888630270958,2.0315015614032745,1.8294475302100182,1.7551884800195694,1.714428298175335,1.638752318918705,1.6351298317313194,1.5708061382174492,1.5834006518125534,1.58053769916296,1.5535971894860268,1.4726029708981514,1.4899580776691437,1.4748622849583626,1.4356976300477982,1.3303119204938412,1.4053520895540714,1.3715416267514229,1.3246234320104122,1.3252668604254723,1.2678636759519577,1.330166831612587,1.2516757063567638,1.2655881941318512],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Test Loss\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29],\"y\":[2.720435321331024,2.48702335357666,2.3994733691215515,2.336147129535675,2.3418873846530914,2.3431312441825867,2.0906263291835785,1.9949193894863129,2.1564717292785645,1.7404235005378723,1.6892988979816437,1.7788927853107452,1.74312824010849,1.6598030626773834,1.6936825811862946,1.6188427209854126,1.6033626198768616,1.5993723571300507,1.493157833814621,1.5322845876216888,1.4331427216529846,1.6543009579181671,1.454666644334793,1.4708003103733063,1.3692902326583862,1.4285019040107727,1.370453029870987,1.46774023771286,1.425534427165985,1.4781038463115692],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Loss over Epochs\"},\"xaxis\":{\"title\":{\"text\":\"Epoch\"}},\"yaxis\":{\"title\":{\"text\":\"Loss\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('0e96744e-5094-45ae-b76c-498ca7979dce');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"9fe0b76a-2856-49e8-8a8d-f4f82014b33e\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"9fe0b76a-2856-49e8-8a8d-f4f82014b33e\")) {                    Plotly.newPlot(                        \"9fe0b76a-2856-49e8-8a8d-f4f82014b33e\",                        [{\"mode\":\"lines\",\"name\":\"Train Accuracy\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29],\"y\":[22.903885480572598,22.903885480572598,24.948875255623722,27.198364008179958,30.67484662576687,31.49284253578732,37.01431492842536,38.445807770961146,44.785276073619634,45.194274028629856,46.012269938650306,47.852760736196316,44.785276073619634,50.511247443762784,48.057259713701434,49.693251533742334,48.261758691206545,49.284253578732105,50.306748466257666,49.284253578732105,48.466257668711656,51.738241308793455,53.374233128834355,52.556237218813905,53.578732106339466,57.05521472392638,52.965235173824134,56.441717791411044,57.668711656441715,54.60122699386503],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Test Accuracy\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29],\"y\":[18.69918699186992,26.016260162601625,26.016260162601625,26.829268292682926,26.016260162601625,28.45528455284553,37.39837398373984,39.83739837398374,35.77235772357724,45.52845528455285,42.27642276422764,39.02439024390244,43.08943089430894,49.59349593495935,47.96747967479675,50.40650406504065,49.59349593495935,49.59349593495935,46.34146341463415,52.03252032520325,52.84552845528455,46.34146341463415,47.15447154471545,52.84552845528455,48.78048780487805,48.78048780487805,47.96747967479675,46.34146341463415,51.21951219512195,52.84552845528455],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Accuracy over Epochs\"},\"xaxis\":{\"title\":{\"text\":\"Epoch\"}},\"yaxis\":{\"title\":{\"text\":\"Accuracy (%)\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('9fe0b76a-2856-49e8-8a8d-f4f82014b33e');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyError",
          "evalue": "'raw'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f52aa054fc68>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'raw'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'raw'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Custom dataset class\n",
        "class ChromatinDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, target_size=(75, 75, 75),transparency=0.5):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.target_size = target_size\n",
        "        self.cell_types = sorted(os.listdir(root_dir))  # Get all cell types\n",
        "        # self.mask_policy = mask_policy  # 'masked' or 'unmasked'\n",
        "        self.transparency = transparency  # Transparency value\n",
        "\n",
        "        # Exclude '.DS_Store'\n",
        "        self.cell_types = [cell_type for cell_type in self.cell_types if cell_type != '.DS_Store']\n",
        "\n",
        "        self.cell_type_to_idx = {cell_type: idx for idx, cell_type in enumerate(self.cell_types)}  # Map cell types to integer labels\n",
        "        self.samples = self._find_samples()\n",
        "\n",
        "    def _find_samples(self):\n",
        "        samples = []\n",
        "        for cell_type in self.cell_types:\n",
        "            cell_type_dir = os.path.join(self.root_dir, cell_type)\n",
        "            if os.path.isdir(cell_type_dir):\n",
        "                sample_dirs = sorted(os.listdir(cell_type_dir))\n",
        "                for sample in sample_dirs:\n",
        "                    raw_dir = os.path.join(cell_type_dir, sample, 'raw')\n",
        "                    mask_dir = os.path.join(cell_type_dir, sample, 'mask')\n",
        "\n",
        "                    # Get all .tif files in raw and mask directories\n",
        "                    raw_files = sorted(glob.glob(os.path.join(raw_dir, '*.tif')))\n",
        "                    mask_files = sorted(glob.glob(os.path.join(mask_dir, '*.tif')))\n",
        "\n",
        "                    if raw_files and mask_files:\n",
        "                        samples.append((cell_type, sample, raw_files, mask_files))\n",
        "\n",
        "        return samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def _resize_image(self, img, target_size):\n",
        "        \"\"\"Resizes the image to the target size.\"\"\"\n",
        "        return img.resize(target_size, Image.Resampling.LANCZOS)\n",
        "\n",
        "    def overlay_mask_on_raw(self, raw_image, mask_image):\n",
        "        \"\"\"\n",
        "        Overlay the mask image on the raw image with given transparency.\n",
        "        - 'masked': Masked regions stay unchanged, unmasked regions are blackened with transparency.\n",
        "        - 'unmasked': Unmasked regions stay unchanged, masked regions are blackened with transparency.\n",
        "        \"\"\"\n",
        "        raw_image = np.array(raw_image.convert(\"RGBA\"))\n",
        "        mask_image = np.array(mask_image.convert(\"L\"))  # Convert to grayscale (L) for binary mask\n",
        "\n",
        "        black_image = np.zeros_like(raw_image)\n",
        "        black_image[:, :, 0:3] = 0  # RGB channels set to black (0)\n",
        "\n",
        "        # if self.mask_policy == \"masked\":\n",
        "        #     mask_alpha = (mask_image > 0) * self.transparency\n",
        "        # elif self.mask_policy == \"unmasked\":\n",
        "        mask_alpha = (mask_image == 0) * self.transparency\n",
        "\n",
        "        # Blend the images: keep raw where the mask is non-zero, else use black with transparency\n",
        "        blended_image = (1 - mask_alpha[:, :, None]) * raw_image + mask_alpha[:, :, None] * black_image\n",
        "\n",
        "        # Convert back to PIL image (removing alpha channel)\n",
        "        blended_pil_image = Image.fromarray(np.uint8(blended_image[:, :, :3]))  # Remove alpha channel for final image\n",
        "        return blended_pil_image\n",
        "    def __getitem__(self, idx):\n",
        "        cell_type, sample, raw_files, mask_files = self.samples[idx]\n",
        "\n",
        "        # Load raw data and mask data as 3D volumes (stacking each slice)\n",
        "        raw_data = np.stack([np.array(self._resize_image(Image.open(f), self.target_size[:2])) for f in raw_files], axis=-1)\n",
        "        mask_data = np.stack([np.array(self._resize_image(Image.open(f), self.target_size[:2])) for f in mask_files], axis=-1)\n",
        "\n",
        "        # Ensure the depth is the same for raw and mask\n",
        "        if raw_data.shape[-1] != mask_data.shape[-1]:\n",
        "            raise ValueError(f\"Number of slices in raw and mask do not match for sample {sample}\")\n",
        "\n",
        "        # If depth is larger than target_size[2], crop; otherwise, pad\n",
        "        if raw_data.shape[-1] < self.target_size[2]:\n",
        "            raw_data = np.pad(raw_data, ((0, 0), (0, 0), (0, self.target_size[2] - raw_data.shape[2])), mode='constant')\n",
        "            mask_data = np.pad(mask_data, ((0, 0), (0, 0), (0, self.target_size[2] - mask_data.shape[2])), mode='constant')\n",
        "        else:\n",
        "            raw_data = raw_data[:, :, :self.target_size[2]]  # Crop to target depth\n",
        "            mask_data = mask_data[:, :, :self.target_size[2]]  # Crop to target depth\n",
        "\n",
        "        # Convert to torch tensors and add channel dimension (since the model expects [batch_size, 1, depth, height, width])\n",
        "        raw_data = torch.tensor(raw_data, dtype=torch.float32).unsqueeze(0)  # Add channel dimension (1 channel)\n",
        "        mask_data = torch.tensor(mask_data, dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
        "\n",
        "        target = self.cell_type_to_idx[cell_type]\n",
        "\n",
        "        # Overlay mask on raw data for the whole cube (depth slices)\n",
        "        # overlaid_cube = np.zeros((raw_data.shape[0], raw_data.shape[1], raw_data.shape[2], 3), dtype=np.uint8)  # RGB cube\n",
        "\n",
        "        # Initialize overlaid_cube with correct dimensions (H, W, D, 3)\n",
        "        overlaid_cube = np.zeros((self.target_size[0], self.target_size[1], self.target_size[2], 3), dtype=np.uint8)\n",
        "\n",
        "        for i in range(raw_data.shape[-1]):  # Iterate over depth\n",
        "            raw_slice = raw_data[:, :, i].numpy().squeeze()  # Remove channel dimension (1, H, W) -> (H, W)\n",
        "            mask_slice = mask_data[:, :, i].numpy().squeeze()\n",
        "\n",
        "            raw_image = Image.fromarray(raw_slice.astype(np.uint8))\n",
        "            mask_image = Image.fromarray(mask_slice.astype(np.uint8))\n",
        "\n",
        "            overlaid_image = self.overlay_mask_on_raw(raw_image, mask_image)\n",
        "            overlaid_cube[:, :, i, :] = np.array(overlaid_image)  # Correct shape assignment\n",
        "\n",
        "        # Convert to tensor and adjust dimensions\n",
        "        overlaid_cube = torch.tensor(overlaid_cube, dtype=torch.float32).permute(3, 0, 1, 2)  # (C, H, W, D)\n",
        "\n",
        "        sample_data = {\n",
        "            'cell_type': cell_type,\n",
        "            'sample': sample,\n",
        "            'overlaid_cube': overlaid_cube,\n",
        "            'target': target\n",
        "        }\n",
        "\n",
        "        if self.transform:\n",
        "            sample_data['overlaid_cube'] = self.transform(sample_data['overlaid_cube'])\n",
        "\n",
        "        return sample_data\n",
        "\n",
        "import os\n",
        "import torch\n",
        "\n",
        "def load_model_from_checkpoint(model, fine_tuned_path):\n",
        "    # Check if fine_tuned_vgg3d.pth exists\n",
        "    if os.path.exists(fine_tuned_path):\n",
        "        print(\"Loading model from fine_tuned_vgg3d.pth...\")\n",
        "        checkpoint_path = fine_tuned_path\n",
        "    else:\n",
        "        # Paths and directories\n",
        "        checkpoint_url = \"https://dl.dropboxusercontent.com/scl/fo/mfejaomhu43aa6oqs6zsf/AKMAAgT7OrUtruR0AQXZBy0/hemibrain_production.checkpoint.20220225?rlkey=6cmwxdvehy4ylztvsbgkfnrfc&dl=0\"\n",
        "        checkpoint_path = 'hemibrain_production.checkpoint'\n",
        "\n",
        "        # Download the fallback checkpoint if not found\n",
        "        os.system(f\"wget -O {checkpoint_path} '{checkpoint_url}'\")\n",
        "        print(\"fine_tuned_vgg3d.pth not found, loading from hemibrain_production.checkpoint...\")\n",
        "\n",
        "    # Load the checkpoint\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "    # Update the model with the state dict from the checkpoint\n",
        "    model_dict = model.state_dict()\n",
        "\n",
        "    # The checkpoint contains the full model's state_dict, so we update the model's state_dict\n",
        "    # If the checkpoint doesn't have a classifier key, load the model directly.\n",
        "    if 'model_state_dict' in checkpoint:\n",
        "        feature_extraction_weights = {k: v for k, v in checkpoint['model_state_dict'].items() if 'classifier' not in k}\n",
        "    else:\n",
        "        # If no 'model_state_dict', assume the checkpoint contains all weights\n",
        "        feature_extraction_weights = {k: v for k, v in checkpoint.items() if 'classifier' not in k}\n",
        "\n",
        "    # Update the model's feature extraction layers with the pre-trained weights\n",
        "    model_dict.update(feature_extraction_weights)\n",
        "    model.load_state_dict(model_dict)\n",
        "\n",
        "    # Return the model with the loaded weights\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "# Data Augmentation for balancing classes\n",
        "augmentation_transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(30),\n",
        "    # transforms.RandomResizedCrop(size=(75, 75, 75), scale=(0.8, 1.2)),\n",
        "    # transforms.ToTensor(),  # Converts the PIL Image or NumPy array to a torch tensor\n",
        "])\n",
        "\n",
        "# Vgg3D model class remains the same\n",
        "class Vgg3D(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=(80, 80, 80),\n",
        "        fmaps=24,\n",
        "        downsample_factors=[(2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2)],\n",
        "        fmap_inc=(2, 2, 2, 2),\n",
        "        n_convolutions=(4, 2, 2, 2),\n",
        "        output_classes=7,\n",
        "        input_fmaps=1,\n",
        "    ):\n",
        "        super(Vgg3D, self).__init__()\n",
        "\n",
        "        # Validate input parameters\n",
        "        if len(downsample_factors) != len(fmap_inc):\n",
        "            raise ValueError(\"fmap_inc needs to have same length as downsample factors\")\n",
        "        if len(n_convolutions) != len(fmap_inc):\n",
        "            raise ValueError(\"n_convolutions needs to have the same length as downsample factors\")\n",
        "        if np.any(np.array(n_convolutions) < 1):\n",
        "            raise ValueError(\"Each layer must have at least one convolution\")\n",
        "\n",
        "        current_fmaps = input_fmaps\n",
        "        current_size = np.array(input_size)\n",
        "\n",
        "        # Feature extraction layers\n",
        "        layers = []\n",
        "        for i, (df, nc) in enumerate(zip(downsample_factors, n_convolutions)):\n",
        "            layers += [\n",
        "                nn.Conv3d(current_fmaps, fmaps, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm3d(fmaps),\n",
        "                nn.ReLU(inplace=True)\n",
        "            ]\n",
        "\n",
        "            for _ in range(nc - 1):\n",
        "                layers += [\n",
        "                    nn.Conv3d(fmaps, fmaps, kernel_size=3, padding=1),\n",
        "                    nn.BatchNorm3d(fmaps),\n",
        "                    nn.ReLU(inplace=True)\n",
        "                ]\n",
        "\n",
        "            layers.append(nn.MaxPool3d(df))\n",
        "\n",
        "            current_fmaps = fmaps\n",
        "            fmaps *= fmap_inc[i]\n",
        "\n",
        "            current_size = np.floor(current_size / np.array(df))\n",
        "\n",
        "        self.features = nn.Sequential(*layers)\n",
        "\n",
        "        # Classifier layer (will be reinitialized later)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(int(np.prod(current_size)) * current_fmaps, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, output_classes),  # This will be adapted to your dataset\n",
        "        )\n",
        "\n",
        "    def forward(self, x, return_features=False):\n",
        "        x = self.features(x)\n",
        "        if return_features:\n",
        "            return x\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Lambda(lambda x: torch.tensor(x, dtype=torch.float32).unsqueeze(0)),  # Adds a channel dimension\n",
        "])\n",
        "\n",
        "dataset = ChromatinDataset(root_dir='/content/chromatin_task', transform=augmentation_transforms)\n",
        "\n",
        "# Train-test split (80% train, 20% test)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "output_classes = len(dataset.cell_types)\n",
        "\n",
        "# dataset = ChromatinDataset(root_dir='/content/chromatin_task', transform=transform)\n",
        "# dataloader = DataLoader(dataset, batch_size=512, shuffle=True)\n",
        "\n",
        "output_classes = len(dataset.cell_types)\n",
        "\n",
        "# Initialize the model\n",
        "output_classes = len(dataset.cell_types)\n",
        "model = Vgg3D(input_size=(75, 75, 75), fmaps=24, output_classes=output_classes, input_fmaps=1)\n",
        "\n",
        "# Paths and directories\n",
        "checkpoint_url = \"https://dl.dropboxusercontent.com/scl/fo/mfejaomhu43aa6oqs6zsf/AKMAAgT7OrUtruR0AQXZBy0/hemibrain_production.checkpoint.20220225?rlkey=6cmwxdvehy4ylztvsbgkfnrfc&dl=0\"\n",
        "checkpoint_path = 'hemibrain_production.checkpoint'\n",
        "\n",
        "# Download the checkpoint if it doesn't exist\n",
        "if not os.path.exists(checkpoint_path):\n",
        "    os.system(f\"wget -O {checkpoint_path} '{checkpoint_url}'\")\n",
        "    print(\"Downloaded VGG3D checkpoint.\")\n",
        "else:\n",
        "    print(\"VGG3D checkpoint already exists.\")\n",
        "\n",
        "# Define checkpoint path\n",
        "fine_tuned_path = 'fine_tuned_vgg3d.pth'\n",
        "model = load_model_from_checkpoint(model, fine_tuned_path)\n",
        "\n",
        "# # Check if the checkpoint exists\n",
        "# if os.path.exists(checkpoint_path):\n",
        "#     print(\"Checkpoint found, resuming training...\")\n",
        "#     model = load_model_from_checkpoint(model, fine_tuned_path)\n",
        "# else:\n",
        "#     print(\"No checkpoint found, starting from scratch.\")\n",
        "\n",
        "model.to(device)  # Move model to GPU if available\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Training loop with CUDA\n",
        "num_epochs = 30\n",
        "train_losses, test_losses, train_accuracies, test_accuracies = [], [], [], []\n",
        "from tqdm import tqdm\n",
        "for epoch in (range(num_epochs)):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    def rgb_to_grayscale(tensor):\n",
        "        # Convert RGB to grayscale using the formula: Y = 0.2989 * R + 0.5870 * G + 0.1140 * B\n",
        "        r, g, b = tensor[:, 0, :, :, :], tensor[:, 1, :, :, :], tensor[:, 2, :, :, :]\n",
        "        grayscale = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
        "        return grayscale.unsqueeze(1)  # Add channel dimension\n",
        "\n",
        "    # Modify the training loop\n",
        "    for batch_idx, batch in enumerate(train_dataloader):\n",
        "        inputs, targets = batch['overlaid_cube'].to(device), batch['target'].to(device)\n",
        "\n",
        "        # Convert RGB to grayscale\n",
        "        inputs = rgb_to_grayscale(inputs)\n",
        "\n",
        "        # Ensure inputs are of shape [batch_size, 1, depth, height, width]\n",
        "        inputs = inputs.squeeze(2)  # Remove extra dimension at index 2 (if present)\n",
        "\n",
        "        targets = torch.tensor(targets, dtype=torch.long).to(device)  # Ensure the targets are long integers for cross-entropy\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total_train += targets.size(0)\n",
        "        correct_train += (predicted == targets).sum().item()\n",
        "\n",
        "    train_losses.append(running_loss / len(train_dataloader))\n",
        "    train_accuracies.append(100 * correct_train / total_train)\n",
        "\n",
        "    # Test phase\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct_test = 0\n",
        "    total_test = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(test_dataloader):\n",
        "            inputs, targets = batch['overlaid_cube'].to(device), batch['target'].to(device)\n",
        "            inputs = rgb_to_grayscale(inputs)\n",
        "\n",
        "            inputs = inputs.squeeze(2)\n",
        "\n",
        "            targets = torch.tensor(targets, dtype=torch.long).to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs, targets)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_test += targets.size(0)\n",
        "            correct_test += (predicted == targets).sum().item()\n",
        "\n",
        "    test_losses.append(running_loss / len(test_dataloader))\n",
        "    test_accuracies.append(100 * correct_test / total_test)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_losses[-1]:.4f}, Train Accuracy: {train_accuracies[-1]:.2f}% | \"\n",
        "          f\"Test Loss: {test_losses[-1]:.4f}, Test Accuracy: {test_accuracies[-1]:.2f}%\")\n",
        "\n",
        "# Plotting loss and accuracy using Plotly\n",
        "fig_loss = go.Figure()\n",
        "fig_loss.add_trace(go.Scatter(x=list(range(num_epochs)), y=train_losses, mode='lines', name='Train Loss'))\n",
        "fig_loss.add_trace(go.Scatter(x=list(range(num_epochs)), y=test_losses, mode='lines', name='Test Loss'))\n",
        "fig_loss.update_layout(title=\"Loss over Epochs\", xaxis_title=\"Epoch\", yaxis_title=\"Loss\")\n",
        "\n",
        "fig_acc = go.Figure()\n",
        "fig_acc.add_trace(go.Scatter(x=list(range(num_epochs)), y=train_accuracies, mode='lines', name='Train Accuracy'))\n",
        "fig_acc.add_trace(go.Scatter(x=list(range(num_epochs)), y=test_accuracies, mode='lines', name='Test Accuracy'))\n",
        "fig_acc.update_layout(title=\"Accuracy over Epochs\", xaxis_title=\"Epoch\", yaxis_title=\"Accuracy (%)\")\n",
        "\n",
        "fig_loss.show()\n",
        "fig_acc.show()\n",
        "# Evaluation on the test set\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        inputs, targets = batch['overlaid_cube'].to(device), batch['target'].to(device)\n",
        "        inputs = rgb_to_grayscale(inputs)\n",
        "\n",
        "        inputs = inputs.squeeze(2)\n",
        "        targets = torch.tensor(targets, dtype=torch.long).to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        y_true.extend(targets.cpu().numpy())\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Calculate metrics\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "# Get all possible labels from the original dataset\n",
        "labels = range(len(dataset.cell_types))\n",
        "report = classification_report(y_true, y_pred, target_names=dataset.cell_types, labels=labels, zero_division=0)\n",
        "\n",
        "print(f\"Test Accuracy: {acc:.4f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n",
        "\n",
        "torch.save(model.state_dict(), \"fine_tuned_vgg3d_with_mask_overlaid.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qr1dKfd1HdgZ",
        "outputId": "4effe8bc-2e48-41c8-eee8-45b10a42df85"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-4-21580692216a>:7: UserWarning:\n",
            "\n",
            "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.5041\n",
            "Classification Report:\n",
            "                           precision    recall  f1-score   support\n",
            "\n",
            "                     Glia       0.00      0.00      0.00         1\n",
            "                 L-shaped       0.51      1.00      0.67        32\n",
            "        Line detector OGL       0.57      0.22      0.32        18\n",
            "       Medium first order       0.67      0.55      0.60        11\n",
            "Medulla projecting MP IGL       0.00      0.00      0.00         0\n",
            "   Medulla projecting OGL       0.00      0.00      0.00         0\n",
            "           Tangential IGL       0.00      0.00      0.00         2\n",
            "            Three-pronged       0.00      0.00      0.00         2\n",
            "                 U-shaped       0.00      0.00      0.00         5\n",
            " Unipolar plex-porjecting       0.00      0.00      0.00         5\n",
            "              centrifugal       0.29      0.50      0.36         4\n",
            "        large first order       1.00      0.33      0.50         3\n",
            "           large palisade       0.50      0.33      0.40         3\n",
            "           multipolar IGL       0.00      0.00      0.00         1\n",
            "                 palisade       0.00      0.00      0.00         4\n",
            "       palisade with feet       0.00      0.00      0.00         0\n",
            "           small U-shaped       0.00      0.00      0.00        12\n",
            "        small first order       0.64      0.80      0.71        20\n",
            "\n",
            "                 accuracy                           0.50       123\n",
            "                macro avg       0.23      0.21      0.20       123\n",
            "             weighted avg       0.43      0.50      0.43       123\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        inputs, targets = batch['overlaid_cube'].to(device), batch['target'].to(device)\n",
        "        inputs = rgb_to_grayscale(inputs)\n",
        "\n",
        "        inputs = inputs.squeeze(2)\n",
        "        targets = torch.tensor(targets, dtype=torch.long).to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        y_true.extend(targets.cpu().numpy())\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Calculate metrics\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "# Get all possible labels from the original dataset\n",
        "labels = range(len(dataset.cell_types))\n",
        "report = classification_report(y_true, y_pred, target_names=dataset.cell_types, labels=labels, zero_division=0)\n",
        "\n",
        "print(f\"Test Accuracy: {acc:.4f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n",
        "\n",
        "torch.save(model.state_dict(), \"fine_tuned_vgg3d_with_mask_overlaid.pth\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAMbMzcmohrW"
      },
      "source": [
        "#transparency 80"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpvcMseyittG",
        "outputId": "984eecdc-68e2-43fa-99f9-4283f412f524"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VGG3D checkpoint already exists.\n",
            "fine_tuned_vgg3d.pth not found, loading from hemibrain_production.checkpoint...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-1-3177934390a5>:155: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location=device)\n",
            "<ipython-input-1-3177934390a5>:333: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  targets = torch.tensor(targets, dtype=torch.long).to(device)  # Ensure the targets are long integers for cross-entropy\n",
            "<ipython-input-1-3177934390a5>:371: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  targets = torch.tensor(targets, dtype=torch.long).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/30] Train Loss: 2.6686, Train Accuracy: 23.93% | Test Loss: 2.6345, Test Accuracy: 26.83%\n",
            "Epoch [2/30] Train Loss: 2.3572, Train Accuracy: 26.99% | Test Loss: 2.2836, Test Accuracy: 26.83%\n",
            "Epoch [3/30] Train Loss: 2.3520, Train Accuracy: 25.97% | Test Loss: 2.2202, Test Accuracy: 30.08%\n",
            "Epoch [4/30] Train Loss: 2.2323, Train Accuracy: 29.45% | Test Loss: 2.1827, Test Accuracy: 26.83%\n",
            "Epoch [5/30] Train Loss: 2.1640, Train Accuracy: 30.88% | Test Loss: 2.0366, Test Accuracy: 39.84%\n",
            "Epoch [6/30] Train Loss: 2.0266, Train Accuracy: 35.99% | Test Loss: 1.8913, Test Accuracy: 43.09%\n",
            "Epoch [7/30] Train Loss: 1.9187, Train Accuracy: 39.67% | Test Loss: 1.7897, Test Accuracy: 48.78%\n",
            "Epoch [8/30] Train Loss: 1.7823, Train Accuracy: 42.94% | Test Loss: 1.7034, Test Accuracy: 43.90%\n",
            "Epoch [9/30] Train Loss: 1.7805, Train Accuracy: 44.38% | Test Loss: 1.6921, Test Accuracy: 43.90%\n",
            "Epoch [10/30] Train Loss: 1.7153, Train Accuracy: 44.99% | Test Loss: 1.6680, Test Accuracy: 47.97%\n",
            "Epoch [11/30] Train Loss: 1.6403, Train Accuracy: 43.56% | Test Loss: 1.6700, Test Accuracy: 43.90%\n",
            "Epoch [12/30] Train Loss: 1.6493, Train Accuracy: 45.81% | Test Loss: 1.6742, Test Accuracy: 45.53%\n",
            "Epoch [13/30] Train Loss: 1.5380, Train Accuracy: 46.63% | Test Loss: 1.6357, Test Accuracy: 47.15%\n",
            "Epoch [14/30] Train Loss: 1.5789, Train Accuracy: 47.44% | Test Loss: 1.7445, Test Accuracy: 45.53%\n",
            "Epoch [15/30] Train Loss: 1.5924, Train Accuracy: 42.74% | Test Loss: 1.7121, Test Accuracy: 48.78%\n",
            "Epoch [16/30] Train Loss: 1.5841, Train Accuracy: 47.44% | Test Loss: 1.6132, Test Accuracy: 50.41%\n",
            "Epoch [17/30] Train Loss: 1.4934, Train Accuracy: 49.90% | Test Loss: 1.6458, Test Accuracy: 46.34%\n",
            "Epoch [18/30] Train Loss: 1.5235, Train Accuracy: 50.72% | Test Loss: 1.6378, Test Accuracy: 44.72%\n",
            "Epoch [19/30] Train Loss: 1.4589, Train Accuracy: 50.51% | Test Loss: 1.5835, Test Accuracy: 47.97%\n",
            "Epoch [20/30] Train Loss: 1.4556, Train Accuracy: 49.08% | Test Loss: 1.6354, Test Accuracy: 45.53%\n",
            "Epoch [21/30] Train Loss: 1.4791, Train Accuracy: 51.12% | Test Loss: 1.5753, Test Accuracy: 45.53%\n",
            "Epoch [22/30] Train Loss: 1.4227, Train Accuracy: 52.97% | Test Loss: 1.6153, Test Accuracy: 49.59%\n",
            "Epoch [23/30] Train Loss: 1.3993, Train Accuracy: 51.94% | Test Loss: 1.4922, Test Accuracy: 52.85%\n",
            "Epoch [24/30] Train Loss: 1.3949, Train Accuracy: 53.17% | Test Loss: 1.4649, Test Accuracy: 48.78%\n",
            "Epoch [25/30] Train Loss: 1.3437, Train Accuracy: 53.78% | Test Loss: 1.4535, Test Accuracy: 51.22%\n",
            "Epoch [26/30] Train Loss: 1.2516, Train Accuracy: 54.60% | Test Loss: 1.5483, Test Accuracy: 47.15%\n",
            "Epoch [27/30] Train Loss: 1.2953, Train Accuracy: 53.58% | Test Loss: 1.5303, Test Accuracy: 50.41%\n",
            "Epoch [28/30] Train Loss: 1.3374, Train Accuracy: 53.78% | Test Loss: 1.5016, Test Accuracy: 50.41%\n",
            "Epoch [29/30] Train Loss: 1.2294, Train Accuracy: 54.60% | Test Loss: 1.4478, Test Accuracy: 50.41%\n",
            "Epoch [30/30] Train Loss: 1.2326, Train Accuracy: 56.65% | Test Loss: 1.4548, Test Accuracy: 52.85%\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"5715f9e9-64bb-4ccf-952d-f31f571da51a\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"5715f9e9-64bb-4ccf-952d-f31f571da51a\")) {                    Plotly.newPlot(                        \"5715f9e9-64bb-4ccf-952d-f31f571da51a\",                        [{\"mode\":\"lines\",\"name\":\"Train Loss\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29],\"y\":[2.6686258763074875,2.3572216629981995,2.3519582748413086,2.2323363348841667,2.163990631699562,2.0265571922063828,1.9186943024396896,1.782250039279461,1.780501052737236,1.7152837365865707,1.6402898542582989,1.649322859942913,1.5380028262734413,1.5788987204432487,1.5923641473054886,1.584113322198391,1.493384599685669,1.5234949886798859,1.4589407593011856,1.4555728435516357,1.4791130349040031,1.4226737394928932,1.3993419110774994,1.394892767071724,1.3436549082398415,1.251588448882103,1.295347809791565,1.3373998627066612,1.2293825149536133,1.232639778405428],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Test Loss\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29],\"y\":[2.634452223777771,2.2835970520973206,2.2202035188674927,2.18272864818573,2.0365510880947113,1.8913418650627136,1.7897046506404877,1.703440636396408,1.6921047568321228,1.6679697334766388,1.6699887812137604,1.674194186925888,1.6356857419013977,1.7444942891597748,1.7120973467826843,1.613210678100586,1.6458026468753815,1.6377596855163574,1.5834904909133911,1.6354073286056519,1.575272560119629,1.615261435508728,1.4922265708446503,1.4648824632167816,1.4535045623779297,1.5483039319515228,1.530286431312561,1.5016421675682068,1.4477500021457672,1.454756498336792],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Loss over Epochs\"},\"xaxis\":{\"title\":{\"text\":\"Epoch\"}},\"yaxis\":{\"title\":{\"text\":\"Loss\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('5715f9e9-64bb-4ccf-952d-f31f571da51a');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"97dfdca7-c74f-454f-8d86-0123c8102507\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"97dfdca7-c74f-454f-8d86-0123c8102507\")) {                    Plotly.newPlot(                        \"97dfdca7-c74f-454f-8d86-0123c8102507\",                        [{\"mode\":\"lines\",\"name\":\"Train Accuracy\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29],\"y\":[23.926380368098158,26.993865030674847,25.971370143149283,29.447852760736197,30.879345603271982,35.991820040899796,39.67280163599182,42.94478527607362,44.376278118609406,44.989775051124745,43.558282208588956,45.807770961145195,46.625766871165645,47.443762781186095,42.740286298568506,47.443762781186095,49.897750511247445,50.715746421267895,50.511247443762784,49.079754601226995,51.124744376278116,52.965235173824134,51.942740286298566,53.169734151329244,53.78323108384458,54.60122699386503,53.578732106339466,53.78323108384458,54.60122699386503,56.646216768916155],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Test Accuracy\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29],\"y\":[26.829268292682926,26.829268292682926,30.08130081300813,26.829268292682926,39.83739837398374,43.08943089430894,48.78048780487805,43.90243902439025,43.90243902439025,47.96747967479675,43.90243902439025,45.52845528455285,47.15447154471545,45.52845528455285,48.78048780487805,50.40650406504065,46.34146341463415,44.71544715447155,47.96747967479675,45.52845528455285,45.52845528455285,49.59349593495935,52.84552845528455,48.78048780487805,51.21951219512195,47.15447154471545,50.40650406504065,50.40650406504065,50.40650406504065,52.84552845528455],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Accuracy over Epochs\"},\"xaxis\":{\"title\":{\"text\":\"Epoch\"}},\"yaxis\":{\"title\":{\"text\":\"Accuracy (%)\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('97dfdca7-c74f-454f-8d86-0123c8102507');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-1-3177934390a5>:414: UserWarning:\n",
            "\n",
            "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.4715\n",
            "Classification Report:\n",
            "                           precision    recall  f1-score   support\n",
            "\n",
            "                     Glia       0.00      0.00      0.00         0\n",
            "                 L-shaped       0.65      0.97      0.78        33\n",
            "        Line detector OGL       0.06      0.17      0.09         6\n",
            "       Medium first order       0.40      0.20      0.27        10\n",
            "Medulla projecting MP IGL       0.00      0.00      0.00         1\n",
            "   Medulla projecting OGL       0.00      0.00      0.00         0\n",
            "           Tangential IGL       0.00      0.00      0.00         3\n",
            "            Three-pronged       0.00      0.00      0.00         3\n",
            "                 U-shaped       0.00      0.00      0.00         8\n",
            " Unipolar plex-porjecting       0.00      0.00      0.00         4\n",
            "              centrifugal       0.44      0.78      0.56         9\n",
            "        large first order       0.00      0.00      0.00         2\n",
            "           large palisade       0.33      0.50      0.40         2\n",
            "           multipolar IGL       0.00      0.00      0.00         4\n",
            "                 palisade       0.75      0.43      0.55         7\n",
            "       palisade with feet       0.00      0.00      0.00         1\n",
            "           small U-shaped       0.00      0.00      0.00         6\n",
            "        small first order       0.52      0.50      0.51        24\n",
            "\n",
            "                 accuracy                           0.47       123\n",
            "                macro avg       0.18      0.20      0.18       123\n",
            "             weighted avg       0.39      0.47      0.41       123\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Custom dataset class\n",
        "class ChromatinDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, target_size=(75, 75, 75),transparency=0.8):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.target_size = target_size\n",
        "        self.cell_types = sorted(os.listdir(root_dir))  # Get all cell types\n",
        "        # self.mask_policy = mask_policy  # 'masked' or 'unmasked'\n",
        "        self.transparency = transparency  # Transparency value\n",
        "\n",
        "        # Exclude '.DS_Store'\n",
        "        self.cell_types = [cell_type for cell_type in self.cell_types if cell_type != '.DS_Store']\n",
        "\n",
        "        self.cell_type_to_idx = {cell_type: idx for idx, cell_type in enumerate(self.cell_types)}  # Map cell types to integer labels\n",
        "        self.samples = self._find_samples()\n",
        "\n",
        "    def _find_samples(self):\n",
        "        samples = []\n",
        "        for cell_type in self.cell_types:\n",
        "            cell_type_dir = os.path.join(self.root_dir, cell_type)\n",
        "            if os.path.isdir(cell_type_dir):\n",
        "                sample_dirs = sorted(os.listdir(cell_type_dir))\n",
        "                for sample in sample_dirs:\n",
        "                    raw_dir = os.path.join(cell_type_dir, sample, 'raw')\n",
        "                    mask_dir = os.path.join(cell_type_dir, sample, 'mask')\n",
        "\n",
        "                    # Get all .tif files in raw and mask directories\n",
        "                    raw_files = sorted(glob.glob(os.path.join(raw_dir, '*.tif')))\n",
        "                    mask_files = sorted(glob.glob(os.path.join(mask_dir, '*.tif')))\n",
        "\n",
        "                    if raw_files and mask_files:\n",
        "                        samples.append((cell_type, sample, raw_files, mask_files))\n",
        "\n",
        "        return samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def _resize_image(self, img, target_size):\n",
        "        \"\"\"Resizes the image to the target size.\"\"\"\n",
        "        return img.resize(target_size, Image.Resampling.LANCZOS)\n",
        "\n",
        "    def overlay_mask_on_raw(self, raw_image, mask_image):\n",
        "        \"\"\"\n",
        "        Overlay the mask image on the raw image with given transparency.\n",
        "        - 'masked': Masked regions stay unchanged, unmasked regions are blackened with transparency.\n",
        "        - 'unmasked': Unmasked regions stay unchanged, masked regions are blackened with transparency.\n",
        "        \"\"\"\n",
        "        raw_image = np.array(raw_image.convert(\"RGBA\"))\n",
        "        mask_image = np.array(mask_image.convert(\"L\"))  # Convert to grayscale (L) for binary mask\n",
        "\n",
        "        black_image = np.zeros_like(raw_image)\n",
        "        black_image[:, :, 0:3] = 0  # RGB channels set to black (0)\n",
        "\n",
        "        # if self.mask_policy == \"masked\":\n",
        "        #     mask_alpha = (mask_image > 0) * self.transparency\n",
        "        # elif self.mask_policy == \"unmasked\":\n",
        "        mask_alpha = (mask_image == 0) * self.transparency\n",
        "\n",
        "        # Blend the images: keep raw where the mask is non-zero, else use black with transparency\n",
        "        blended_image = (1 - mask_alpha[:, :, None]) * raw_image + mask_alpha[:, :, None] * black_image\n",
        "\n",
        "        # Convert back to PIL image (removing alpha channel)\n",
        "        blended_pil_image = Image.fromarray(np.uint8(blended_image[:, :, :3]))  # Remove alpha channel for final image\n",
        "        return blended_pil_image\n",
        "    def __getitem__(self, idx):\n",
        "        cell_type, sample, raw_files, mask_files = self.samples[idx]\n",
        "\n",
        "        # Load raw data and mask data as 3D volumes (stacking each slice)\n",
        "        raw_data = np.stack([np.array(self._resize_image(Image.open(f), self.target_size[:2])) for f in raw_files], axis=-1)\n",
        "        mask_data = np.stack([np.array(self._resize_image(Image.open(f), self.target_size[:2])) for f in mask_files], axis=-1)\n",
        "\n",
        "        # Ensure the depth is the same for raw and mask\n",
        "        if raw_data.shape[-1] != mask_data.shape[-1]:\n",
        "            raise ValueError(f\"Number of slices in raw and mask do not match for sample {sample}\")\n",
        "\n",
        "        # If depth is larger than target_size[2], crop; otherwise, pad\n",
        "        if raw_data.shape[-1] < self.target_size[2]:\n",
        "            raw_data = np.pad(raw_data, ((0, 0), (0, 0), (0, self.target_size[2] - raw_data.shape[2])), mode='constant')\n",
        "            mask_data = np.pad(mask_data, ((0, 0), (0, 0), (0, self.target_size[2] - mask_data.shape[2])), mode='constant')\n",
        "        else:\n",
        "            raw_data = raw_data[:, :, :self.target_size[2]]  # Crop to target depth\n",
        "            mask_data = mask_data[:, :, :self.target_size[2]]  # Crop to target depth\n",
        "\n",
        "        # Convert to torch tensors and add channel dimension (since the model expects [batch_size, 1, depth, height, width])\n",
        "        raw_data = torch.tensor(raw_data, dtype=torch.float32).unsqueeze(0)  # Add channel dimension (1 channel)\n",
        "        mask_data = torch.tensor(mask_data, dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
        "\n",
        "        target = self.cell_type_to_idx[cell_type]\n",
        "\n",
        "        # Overlay mask on raw data for the whole cube (depth slices)\n",
        "        # overlaid_cube = np.zeros((raw_data.shape[0], raw_data.shape[1], raw_data.shape[2], 3), dtype=np.uint8)  # RGB cube\n",
        "\n",
        "        # Initialize overlaid_cube with correct dimensions (H, W, D, 3)\n",
        "        overlaid_cube = np.zeros((self.target_size[0], self.target_size[1], self.target_size[2], 3), dtype=np.uint8)\n",
        "\n",
        "        for i in range(raw_data.shape[-1]):  # Iterate over depth\n",
        "            raw_slice = raw_data[:, :, i].numpy().squeeze()  # Remove channel dimension (1, H, W) -> (H, W)\n",
        "            mask_slice = mask_data[:, :, i].numpy().squeeze()\n",
        "\n",
        "            raw_image = Image.fromarray(raw_slice.astype(np.uint8))\n",
        "            mask_image = Image.fromarray(mask_slice.astype(np.uint8))\n",
        "\n",
        "            overlaid_image = self.overlay_mask_on_raw(raw_image, mask_image)\n",
        "            overlaid_cube[:, :, i, :] = np.array(overlaid_image)  # Correct shape assignment\n",
        "\n",
        "        # Convert to tensor and adjust dimensions\n",
        "        overlaid_cube = torch.tensor(overlaid_cube, dtype=torch.float32).permute(3, 0, 1, 2)  # (C, H, W, D)\n",
        "\n",
        "        sample_data = {\n",
        "            'cell_type': cell_type,\n",
        "            'sample': sample,\n",
        "            'overlaid_cube': overlaid_cube,\n",
        "            'target': target\n",
        "        }\n",
        "\n",
        "        if self.transform:\n",
        "            sample_data['overlaid_cube'] = self.transform(sample_data['overlaid_cube'])\n",
        "\n",
        "        return sample_data\n",
        "\n",
        "import os\n",
        "import torch\n",
        "\n",
        "def load_model_from_checkpoint(model, fine_tuned_path):\n",
        "    # Check if fine_tuned_vgg3d.pth exists\n",
        "    if os.path.exists(fine_tuned_path):\n",
        "        print(\"Loading model from fine_tuned_vgg3d.pth...\")\n",
        "        checkpoint_path = fine_tuned_path\n",
        "    else:\n",
        "        # Paths and directories\n",
        "        checkpoint_url = \"https://dl.dropboxusercontent.com/scl/fo/mfejaomhu43aa6oqs6zsf/AKMAAgT7OrUtruR0AQXZBy0/hemibrain_production.checkpoint.20220225?rlkey=6cmwxdvehy4ylztvsbgkfnrfc&dl=0\"\n",
        "        checkpoint_path = 'hemibrain_production.checkpoint'\n",
        "\n",
        "        # Download the fallback checkpoint if not found\n",
        "        os.system(f\"wget -O {checkpoint_path} '{checkpoint_url}'\")\n",
        "        print(\"fine_tuned_vgg3d.pth not found, loading from hemibrain_production.checkpoint...\")\n",
        "\n",
        "    # Load the checkpoint\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "    # Update the model with the state dict from the checkpoint\n",
        "    model_dict = model.state_dict()\n",
        "\n",
        "    # The checkpoint contains the full model's state_dict, so we update the model's state_dict\n",
        "    # If the checkpoint doesn't have a classifier key, load the model directly.\n",
        "    if 'model_state_dict' in checkpoint:\n",
        "        feature_extraction_weights = {k: v for k, v in checkpoint['model_state_dict'].items() if 'classifier' not in k}\n",
        "    else:\n",
        "        # If no 'model_state_dict', assume the checkpoint contains all weights\n",
        "        feature_extraction_weights = {k: v for k, v in checkpoint.items() if 'classifier' not in k}\n",
        "\n",
        "    # Update the model's feature extraction layers with the pre-trained weights\n",
        "    model_dict.update(feature_extraction_weights)\n",
        "    model.load_state_dict(model_dict)\n",
        "\n",
        "    # Return the model with the loaded weights\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "# Data Augmentation for balancing classes\n",
        "augmentation_transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(30),\n",
        "    # transforms.RandomResizedCrop(size=(75, 75, 75), scale=(0.8, 1.2)),\n",
        "    # transforms.ToTensor(),  # Converts the PIL Image or NumPy array to a torch tensor\n",
        "])\n",
        "\n",
        "# Vgg3D model class remains the same\n",
        "class Vgg3D(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=(80, 80, 80),\n",
        "        fmaps=24,\n",
        "        downsample_factors=[(2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2)],\n",
        "        fmap_inc=(2, 2, 2, 2),\n",
        "        n_convolutions=(4, 2, 2, 2),\n",
        "        output_classes=7,\n",
        "        input_fmaps=1,\n",
        "    ):\n",
        "        super(Vgg3D, self).__init__()\n",
        "\n",
        "        # Validate input parameters\n",
        "        if len(downsample_factors) != len(fmap_inc):\n",
        "            raise ValueError(\"fmap_inc needs to have same length as downsample factors\")\n",
        "        if len(n_convolutions) != len(fmap_inc):\n",
        "            raise ValueError(\"n_convolutions needs to have the same length as downsample factors\")\n",
        "        if np.any(np.array(n_convolutions) < 1):\n",
        "            raise ValueError(\"Each layer must have at least one convolution\")\n",
        "\n",
        "        current_fmaps = input_fmaps\n",
        "        current_size = np.array(input_size)\n",
        "\n",
        "        # Feature extraction layers\n",
        "        layers = []\n",
        "        for i, (df, nc) in enumerate(zip(downsample_factors, n_convolutions)):\n",
        "            layers += [\n",
        "                nn.Conv3d(current_fmaps, fmaps, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm3d(fmaps),\n",
        "                nn.ReLU(inplace=True)\n",
        "            ]\n",
        "\n",
        "            for _ in range(nc - 1):\n",
        "                layers += [\n",
        "                    nn.Conv3d(fmaps, fmaps, kernel_size=3, padding=1),\n",
        "                    nn.BatchNorm3d(fmaps),\n",
        "                    nn.ReLU(inplace=True)\n",
        "                ]\n",
        "\n",
        "            layers.append(nn.MaxPool3d(df))\n",
        "\n",
        "            current_fmaps = fmaps\n",
        "            fmaps *= fmap_inc[i]\n",
        "\n",
        "            current_size = np.floor(current_size / np.array(df))\n",
        "\n",
        "        self.features = nn.Sequential(*layers)\n",
        "\n",
        "        # Classifier layer (will be reinitialized later)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(int(np.prod(current_size)) * current_fmaps, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, output_classes),  # This will be adapted to your dataset\n",
        "        )\n",
        "\n",
        "    def forward(self, x, return_features=False):\n",
        "        x = self.features(x)\n",
        "        if return_features:\n",
        "            return x\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Lambda(lambda x: torch.tensor(x, dtype=torch.float32).unsqueeze(0)),  # Adds a channel dimension\n",
        "])\n",
        "\n",
        "dataset = ChromatinDataset(root_dir='/content/chromatin_task', transform=augmentation_transforms)\n",
        "\n",
        "# Train-test split (80% train, 20% test)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "output_classes = len(dataset.cell_types)\n",
        "\n",
        "# dataset = ChromatinDataset(root_dir='/content/chromatin_task', transform=transform)\n",
        "# dataloader = DataLoader(dataset, batch_size=512, shuffle=True)\n",
        "\n",
        "output_classes = len(dataset.cell_types)\n",
        "\n",
        "# Initialize the model\n",
        "output_classes = len(dataset.cell_types)\n",
        "model = Vgg3D(input_size=(75, 75, 75), fmaps=24, output_classes=output_classes, input_fmaps=1)\n",
        "\n",
        "# Paths and directories\n",
        "checkpoint_url = \"https://dl.dropboxusercontent.com/scl/fo/mfejaomhu43aa6oqs6zsf/AKMAAgT7OrUtruR0AQXZBy0/hemibrain_production.checkpoint.20220225?rlkey=6cmwxdvehy4ylztvsbgkfnrfc&dl=0\"\n",
        "checkpoint_path = 'hemibrain_production.checkpoint'\n",
        "\n",
        "# Download the checkpoint if it doesn't exist\n",
        "if not os.path.exists(checkpoint_path):\n",
        "    os.system(f\"wget -O {checkpoint_path} '{checkpoint_url}'\")\n",
        "    print(\"Downloaded VGG3D checkpoint.\")\n",
        "else:\n",
        "    print(\"VGG3D checkpoint already exists.\")\n",
        "\n",
        "# Define checkpoint path\n",
        "fine_tuned_path = 'fine_tuned_vgg3d.pth'\n",
        "model = load_model_from_checkpoint(model, fine_tuned_path)\n",
        "\n",
        "# # Check if the checkpoint exists\n",
        "# if os.path.exists(checkpoint_path):\n",
        "#     print(\"Checkpoint found, resuming training...\")\n",
        "#     model = load_model_from_checkpoint(model, fine_tuned_path)\n",
        "# else:\n",
        "#     print(\"No checkpoint found, starting from scratch.\")\n",
        "\n",
        "model.to(device)  # Move model to GPU if available\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Training loop with CUDA\n",
        "num_epochs = 30\n",
        "train_losses, test_losses, train_accuracies, test_accuracies = [], [], [], []\n",
        "from tqdm import tqdm\n",
        "for epoch in (range(num_epochs)):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    def rgb_to_grayscale(tensor):\n",
        "        # Convert RGB to grayscale using the formula: Y = 0.2989 * R + 0.5870 * G + 0.1140 * B\n",
        "        r, g, b = tensor[:, 0, :, :, :], tensor[:, 1, :, :, :], tensor[:, 2, :, :, :]\n",
        "        grayscale = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
        "        return grayscale.unsqueeze(1)  # Add channel dimension\n",
        "\n",
        "    # Modify the training loop\n",
        "    for batch_idx, batch in enumerate(train_dataloader):\n",
        "        inputs, targets = batch['overlaid_cube'].to(device), batch['target'].to(device)\n",
        "\n",
        "        # Convert RGB to grayscale\n",
        "        inputs = rgb_to_grayscale(inputs)\n",
        "\n",
        "        # Ensure inputs are of shape [batch_size, 1, depth, height, width]\n",
        "        inputs = inputs.squeeze(2)  # Remove extra dimension at index 2 (if present)\n",
        "\n",
        "        targets = torch.tensor(targets, dtype=torch.long).to(device)  # Ensure the targets are long integers for cross-entropy\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total_train += targets.size(0)\n",
        "        correct_train += (predicted == targets).sum().item()\n",
        "\n",
        "    train_losses.append(running_loss / len(train_dataloader))\n",
        "    train_accuracies.append(100 * correct_train / total_train)\n",
        "\n",
        "    # Test phase\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct_test = 0\n",
        "    total_test = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(test_dataloader):\n",
        "            inputs, targets = batch['overlaid_cube'].to(device), batch['target'].to(device)\n",
        "            inputs = rgb_to_grayscale(inputs)\n",
        "\n",
        "            inputs = inputs.squeeze(2)\n",
        "\n",
        "            targets = torch.tensor(targets, dtype=torch.long).to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs, targets)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_test += targets.size(0)\n",
        "            correct_test += (predicted == targets).sum().item()\n",
        "\n",
        "    test_losses.append(running_loss / len(test_dataloader))\n",
        "    test_accuracies.append(100 * correct_test / total_test)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_losses[-1]:.4f}, Train Accuracy: {train_accuracies[-1]:.2f}% | \"\n",
        "          f\"Test Loss: {test_losses[-1]:.4f}, Test Accuracy: {test_accuracies[-1]:.2f}%\")\n",
        "\n",
        "# Plotting loss and accuracy using Plotly\n",
        "fig_loss = go.Figure()\n",
        "fig_loss.add_trace(go.Scatter(x=list(range(num_epochs)), y=train_losses, mode='lines', name='Train Loss'))\n",
        "fig_loss.add_trace(go.Scatter(x=list(range(num_epochs)), y=test_losses, mode='lines', name='Test Loss'))\n",
        "fig_loss.update_layout(title=\"Loss over Epochs\", xaxis_title=\"Epoch\", yaxis_title=\"Loss\")\n",
        "\n",
        "fig_acc = go.Figure()\n",
        "fig_acc.add_trace(go.Scatter(x=list(range(num_epochs)), y=train_accuracies, mode='lines', name='Train Accuracy'))\n",
        "fig_acc.add_trace(go.Scatter(x=list(range(num_epochs)), y=test_accuracies, mode='lines', name='Test Accuracy'))\n",
        "fig_acc.update_layout(title=\"Accuracy over Epochs\", xaxis_title=\"Epoch\", yaxis_title=\"Accuracy (%)\")\n",
        "\n",
        "fig_loss.show()\n",
        "fig_acc.show()\n",
        "# Evaluation on the test set\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        inputs, targets = batch['overlaid_cube'].to(device), batch['target'].to(device)\n",
        "        inputs = rgb_to_grayscale(inputs)\n",
        "\n",
        "        inputs = inputs.squeeze(2)\n",
        "        targets = torch.tensor(targets, dtype=torch.long).to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        y_true.extend(targets.cpu().numpy())\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Calculate metrics\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "# Get all possible labels from the original dataset\n",
        "labels = range(len(dataset.cell_types))\n",
        "report = classification_report(y_true, y_pred, target_names=dataset.cell_types, labels=labels, zero_division=0)\n",
        "\n",
        "print(f\"Test Accuracy: {acc:.4f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n",
        "\n",
        "torch.save(model.state_dict(), \"fine_tuned_vgg3d_with_mask_overlaid.pth\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vit"
      ],
      "metadata": {
        "id": "sbyz1JfShZ_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install open_clip_torch\n"
      ],
      "metadata": {
        "id": "HE5XRAzjh9qB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from transformers import TimesformerForVideoClassification, TimesformerConfig, AutoImageProcessor\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Custom dataset class\n",
        "class ChromatinDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, target_size=(75, 75, 75)):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.target_size = target_size\n",
        "        self.cell_types = sorted(os.listdir(root_dir))  # Get all cell types\n",
        "\n",
        "        # Exclude '.DS_Store'\n",
        "        self.cell_types = [cell_type for cell_type in self.cell_types if cell_type != '.DS_Store']\n",
        "\n",
        "        self.cell_type_to_idx = {cell_type: idx for idx, cell_type in enumerate(self.cell_types)}  # Map cell types to integer labels\n",
        "        self.samples = self._find_samples()\n",
        "\n",
        "    def _find_samples(self):\n",
        "        samples = []\n",
        "        for cell_type in self.cell_types:\n",
        "            cell_type_dir = os.path.join(self.root_dir, cell_type)\n",
        "            if os.path.isdir(cell_type_dir):\n",
        "                sample_dirs = sorted(os.listdir(cell_type_dir))\n",
        "                for sample in sample_dirs:\n",
        "                    raw_dir = os.path.join(cell_type_dir, sample, 'raw')\n",
        "                    mask_dir = os.path.join(cell_type_dir, sample, 'mask')\n",
        "\n",
        "                    # Get all .tif files in raw and mask directories\n",
        "                    raw_files = sorted(glob.glob(os.path.join(raw_dir, '*.tif')))\n",
        "                    mask_files = sorted(glob.glob(os.path.join(mask_dir, '*.tif')))\n",
        "\n",
        "                    if raw_files and mask_files:\n",
        "                        samples.append((cell_type, sample, raw_files, mask_files))\n",
        "\n",
        "        return samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def _resize_image(self, img, target_size):\n",
        "        \"\"\"Resizes the image to the target size.\"\"\"\n",
        "        return img.resize(target_size, Image.Resampling.LANCZOS)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        cell_type, sample, raw_files, mask_files = self.samples[idx]\n",
        "\n",
        "        # Load raw data and mask data as 3D volumes (stacking each slice)\n",
        "        raw_data = np.stack([np.array(self._resize_image(Image.open(f), self.target_size[:2])) for f in raw_files], axis=-1)\n",
        "        mask_data = np.stack([np.array(self._resize_image(Image.open(f), self.target_size[:2])) for f in mask_files], axis=-1)\n",
        "\n",
        "        # Ensure the depth is the same for raw and mask\n",
        "        if raw_data.shape[-1] != mask_data.shape[-1]:\n",
        "            raise ValueError(f\"Number of slices in raw and mask do not match for sample {sample}\")\n",
        "\n",
        "        # If depth is larger than target_size[2], crop; otherwise, pad\n",
        "        if raw_data.shape[-1] < self.target_size[2]:\n",
        "            raw_data = np.pad(raw_data, ((0, 0), (0, 0), (0, self.target_size[2] - raw_data.shape[2])), mode='constant')\n",
        "            mask_data = np.pad(mask_data, ((0, 0), (0, 0), (0, self.target_size[2] - mask_data.shape[2])), mode='constant')\n",
        "        else:\n",
        "            raw_data = raw_data[:, :, :self.target_size[2]]  # Crop to target depth\n",
        "            mask_data = mask_data[:, :, :self.target_size[2]]  # Crop to target depth\n",
        "\n",
        "        # Convert to torch tensors\n",
        "        raw_data = torch.tensor(raw_data, dtype=torch.float32)\n",
        "        mask_data = torch.tensor(mask_data, dtype=torch.float32)\n",
        "\n",
        "        # Get the integer index for the cell_type\n",
        "        target = self.cell_type_to_idx[cell_type]\n",
        "\n",
        "        sample_data = {'cell_type': cell_type, 'sample': sample, 'raw': raw_data, 'mask': mask_data, 'target': target}\n",
        "\n",
        "        if self.transform:\n",
        "            sample_data['raw'] = self.transform(sample_data['raw'])\n",
        "            sample_data['mask'] = self.transform(sample_data['mask'])\n",
        "\n",
        "        return sample_data\n",
        "\n",
        "# Transformations for augmenting data (if needed)\n",
        "augmentation_transforms = transforms.Compose([\n",
        "    transforms.Lambda(lambda x: torch.tensor(x, dtype=torch.float32).unsqueeze(0)),  # Adds a channel dimension\n",
        "])\n",
        "\n",
        "# Instantiate the dataset\n",
        "dataset = ChromatinDataset(root_dir='/content/chromatin_task', transform=augmentation_transforms)\n",
        "\n",
        "# Train-test split (80% train, 20% test)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "output_classes = len(dataset.cell_types)\n",
        "# Load pretrained TimeSformer model and processor\n",
        "image_processor = AutoImageProcessor.from_pretrained(\"facebook/timesformer-base-finetuned-k400\")\n",
        "model = TimesformerForVideoClassification.from_pretrained(\n",
        "    \"facebook/timesformer-base-finetuned-k400\",\n",
        "    num_labels=output_classes,\n",
        "    ignore_mismatched_sizes=True  # Ignore mismatched sizes for the classifier layer\n",
        ")\n",
        "\n",
        "# Modify the final classifier to match the number of classes in your dataset\n",
        "model.classifier = torch.nn.Linear(model.config.hidden_size, output_classes)\n",
        "\n",
        "# Setup optimizer and loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Function to convert grayscale to RGB\n",
        "def grayscale_to_rgb(video_clips):\n",
        "    # Repeat the grayscale channel 3 times to mimic RGB\n",
        "    return np.repeat(video_clips[..., np.newaxis], 3, axis=-1)\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "def preprocess_video_frames(video_clips):\n",
        "    # Ensure that video_clips is a numpy array with shape (batch_size, num_frames, height, width, depth, channels)\n",
        "    video_clips = np.array(video_clips)  # Convert to numpy array if it's not already\n",
        "    print(f\"Initial video_clips shape: {video_clips.shape}\")\n",
        "\n",
        "    # If the input has an unnecessary dimension, remove it (e.g., 1 in (batch_size, 1, num_frames, height, width, 3))\n",
        "    if video_clips.shape[1] == 1:\n",
        "        video_clips = np.squeeze(video_clips, axis=1)  # Remove the redundant singleton dimension\n",
        "        print(f\"After squeezing redundant dimension: {video_clips.shape}\")\n",
        "\n",
        "    # If the input is grayscale (depth = 1), convert it to RGB by repeating the channel\n",
        "    if video_clips.shape[-1] != 3:\n",
        "        video_clips = np.repeat(video_clips[..., np.newaxis], 3, axis=-1)\n",
        "        print(f\"Converted video_clips shape (grayscale to RGB): {video_clips.shape}\")\n",
        "\n",
        "    # Ensure the shape is (batch_size, num_frames, height, width, 3)\n",
        "    if len(video_clips.shape) == 5:  # (batch_size, num_frames, height, width, 3)\n",
        "        video_clips = video_clips.astype(np.uint8)  # Convert to uint8 for PIL compatibility\n",
        "        print(f\"Processed video_clips shape: {video_clips.shape}\")\n",
        "\n",
        "    return video_clips\n",
        "\n",
        "# Inside the training loop:\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Prepare video clips and labels\n",
        "        video_clips = batch['raw']\n",
        "        labels = batch['target']\n",
        "\n",
        "        print(f\"Raw video_clips shape: {video_clips.shape}\")\n",
        "\n",
        "        # Convert grayscale to RGB and preprocess\n",
        "        video_clips = preprocess_video_frames(video_clips.numpy())\n",
        "        print(f\"Processed video_clips shape: {video_clips.shape}\")\n",
        "\n",
        "        # Process input and forward pass\n",
        "        inputs = image_processor(list(video_clips), return_tensors=\"pt\")\n",
        "        print(f\"Inputs shape after image processor: {inputs['pixel_values'].shape}\")\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Compute loss and update model\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Statistics\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(logits, 1)\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataloader)\n",
        "    epoch_accuracy = correct_predictions / total_samples\n",
        "    print(f\"Epoch {epoch+1} - Loss: {epoch_loss:.4f} - Accuracy: {epoch_accuracy:.4f}\")\n",
        "\n",
        "model.eval()\n",
        "correct_predictions = 0\n",
        "total_samples = 0\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
        "        video_clips = batch['raw']\n",
        "        labels = batch['target']\n",
        "\n",
        "        # Convert grayscale to RGB\n",
        "        video_clips = grayscale_to_rgb(video_clips.numpy())\n",
        "\n",
        "        # Process input and forward pass\n",
        "        inputs = image_processor(list(video_clips), return_tensors=\"pt\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        _, predicted = torch.max(logits, 1)\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "test_accuracy = correct_predictions / total_samples\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "id": "V6keZqN5ha0l",
        "outputId": "ca956cec-b93b-4758-c083-d887006f4714"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at facebook/timesformer-base-finetuned-k400 and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([18, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([18]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 1/10:   0%|          | 0/16 [00:00<?, ?it/s]<ipython-input-55-4f348342602b>:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  transforms.Lambda(lambda x: torch.tensor(x, dtype=torch.float32).unsqueeze(0)),  # Adds a channel dimension\n",
            "Epoch 1/10:   0%|          | 0/16 [00:03<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw video_clips shape: torch.Size([32, 1, 75, 75, 75])\n",
            "Initial video_clips shape: (32, 1, 75, 75, 75)\n",
            "After squeezing redundant dimension: (32, 75, 75, 75)\n",
            "Converted video_clips shape (grayscale to RGB): (32, 75, 75, 75, 3)\n",
            "Processed video_clips shape: (32, 75, 75, 75, 3)\n",
            "Processed video_clips shape: (32, 75, 75, 75, 3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Cannot handle this data type: (1, 1, 75, 3), |u1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3310\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3311\u001b[0;31m             \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fromarray_typemap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtypekey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3312\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: ((1, 1, 75, 3), '|u1')",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-4f348342602b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;31m# Process input and forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_processor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_clips\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Inputs shape after image processor: {inputs['pixel_values'].shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBatchFeature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;34m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBatchFeature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    852\u001b[0m                 )\n\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mvalid_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/videomae/image_processing_videomae.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(self, videos, do_resize, size, resample, do_center_crop, crop_size, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_tensors, data_format, input_data_format)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mvideos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_batched\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         videos = [\n\u001b[0m\u001b[1;32m    323\u001b[0m             [\n\u001b[1;32m    324\u001b[0m                 self._preprocess_image(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/videomae/image_processing_videomae.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         videos = [\n\u001b[0;32m--> 323\u001b[0;31m             [\n\u001b[0m\u001b[1;32m    324\u001b[0m                 self._preprocess_image(\n\u001b[1;32m    325\u001b[0m                     \u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/videomae/image_processing_videomae.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    322\u001b[0m         videos = [\n\u001b[1;32m    323\u001b[0m             [\n\u001b[0;32m--> 324\u001b[0;31m                 self._preprocess_image(\n\u001b[0m\u001b[1;32m    325\u001b[0m                     \u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                     \u001b[0mdo_resize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdo_resize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/videomae/image_processing_videomae.py\u001b[0m in \u001b[0;36m_preprocess_image\u001b[0;34m(self, image, do_resize, size, resample, do_center_crop, crop_size, do_rescale, rescale_factor, do_normalize, image_mean, image_std, data_format, input_data_format)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_resize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_data_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_center_crop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/videomae/image_processing_videomae.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, image, size, resample, data_format, input_data_format, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Size must have 'height' and 'width' or 'shortest_edge' as keys. Got {size.keys()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         return resize(\n\u001b[0m\u001b[1;32m    170\u001b[0m             \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/image_transforms.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(image, size, resample, reducing_gap, data_format, return_numpy, input_data_format)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0mdo_rescale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_rescale_for_pil_conversion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_rescale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdo_rescale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_data_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m     \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0;31m# PIL images are in the format (width, height)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/image_transforms.py\u001b[0m in \u001b[0;36mto_pil_image\u001b[0;34m(image, do_rescale, image_mode, input_data_format)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3313\u001b[0m             \u001b[0mtypekey_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypestr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtypekey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3314\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Cannot handle this data type: {typekey_shape}, {typestr}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3315\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3316\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3317\u001b[0m         \u001b[0mrawmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Cannot handle this data type: (1, 1, 75, 3), |u1"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1dzf_5j6xcsBDkvhbPiH_6n_qXSL2xnTG",
      "authorship_tag": "ABX9TyM49NHaPIWi4DzD+KC6FekK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}